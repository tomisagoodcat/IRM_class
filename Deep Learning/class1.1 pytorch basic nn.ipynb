{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch 基础\n",
    "\n",
    "本课内容包括\n",
    "1. 基本的pytorch入门\n",
    "2. pytorch中向量、导数、偏导操作，参考之前线性代数以及微积分\n",
    "3. 基于pytorch构建一个线性回归网络\n",
    "4. 基于pytorch实现class1 对图片的分类以及优化\n",
    "\n",
    "---\n",
    "\n",
    "参考资料\n",
    "\n",
    "1. [ws university pytorch introudction](https://courses.cs.washington.edu/courses/cse446/19au/section9.html)\n",
    "2. [li hong yi](https://www.youtube.com/watch?v=kQeezFrNoOg)\n",
    "3. [pytorch homepage](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.安装pytorch\n",
    "\n",
    "与tensorflow相同，pytorch分为cpu版本与gpu版本，官网有相对于安装代码\n",
    "[安装页面](https://pytorch.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参看本地计算机是否支持GPU以及GPU型号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only support cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def pytrochGPU(memoryview=False):\n",
    "    support=torch.cuda.is_available()\n",
    "    if support==True:\n",
    "        print(\"GPU is available\")\n",
    "        print(\"GPU的数量\",torch.cuda.device_count())\n",
    "        print(\"GPU的名字\",torch.cuda.get_device_name(0))\n",
    "        print(\"当前GPU索引\",torch.cuda.current_device())\n",
    "    else:\n",
    "        print(\"only support cpu\")\n",
    "if __name__ == '__main__':\n",
    "    print(pytrochGPU())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于可能本地计算机没有gpu，为了避免出错，首先判断是GPU还是CPU，并将数据在对应设备上进行运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x=torch.randn(2,3)\n",
    "x=x.to(device)\n",
    "print(x.device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "pytorch 中torch的作用类似numpy，包括创建数组，进行线性代数运算等，但torch还可以直接进行求导运算\n",
    "\n",
    "numpy 与 torch之间也可以相互转化\n",
    "\n",
    "例如numpy中创建数组与torch中创建tensor张量对比\n",
    "\n",
    "```python \n",
    "np.array([12,3])\n",
    "torch.tensor([12,3])\n",
    "```\n",
    "\n",
    "1. array与tensor相互转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([2, 3, 2])\n",
      "<class 'numpy.ndarray'>\n",
      "[2 3 2]\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 3)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "if __name__ == '__main__':\n",
    "    x_tensor=torch.tensor([2,3,2])\n",
    "    print(type(x_tensor))\n",
    "    print(x_tensor)\n",
    "    y_array=x_tensor.numpy()#转换tensor到array\n",
    "    print(type(y_array))\n",
    "    print(y_array)\n",
    "\n",
    "    x_arrray=np.array([[2,2,3],[2,2,2]])\n",
    "    print(type(x_arrray))\n",
    "    print(x_arrray.shape)\n",
    "    y_tensor=torch.from_numpy(x_arrray)\n",
    "    print(type(y_tensor))\n",
    "    print(y_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensor 中的线性代数计算\n",
    "tensor中的计算与numpy中基本一致，包括加减，内积,求norm等。\n",
    "\n",
    "例如以下代码模拟了一个神经网络affine层中的输入值与权参相乘与偏参相加的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17., 18.], dtype=torch.float64)\n",
      "tensor(24.7588, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 注意，torch在求norm时，要求数据必须为float数据，故在开始就必须制定向量的数据类型\n",
    "x=torch.tensor([2,3,3],dtype=float)\n",
    "w=torch.tensor([[2,2],[1,2],[3,2]],dtype=float)\n",
    "b=torch.tensor([1,2],dtype=float)\n",
    "z=x@w+b\n",
    "print(z)\n",
    "print(torch.norm(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor中正态分布、随机值等的生成与numpy也一致\n",
    "\n",
    "```python\n",
    "torch.rand(维度1数，维度2数，维度3数)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(2,2,2,2)\n",
    "print(x)\n",
    "x_a=np.random.rand(2,2,2,2,2)\n",
    "print(x_a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 形状改变\n",
    "* Tensor.view-改变形状\n",
    "* Tensor.flatten-降维为1维\n",
    "\n",
    "We can use the Tensor.view() function to reshape tensors similarly to numpy.reshape()\n",
    "\n",
    "It can also automatically calculate the correct dimension if a -1 is passed in. This is useful if we are working with batches, but the batch size is unknown.\n",
    "---\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 28, 28])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([784000])\n"
     ]
    }
   ],
   "source": [
    "batch=1000\n",
    "img_x=28\n",
    "img_y=28\n",
    "#生成一个（1000，28，29）形状的张量，模拟1000张28*28图像\n",
    "x=torch.rand(batch,img_x,img_y)\n",
    "print(x.shape)\n",
    "#将x变更为(1000,784)形状的张量\n",
    "x2=x.view(batch,img_x*img_y)\n",
    "print(x2.shape)\n",
    "#当不确定某维度的大小（batch大小），可以设为-1，torch将自动赋值\n",
    "x3=x.view(batch,-1)\n",
    "print(x3.shape)\n",
    "x3=x.view(-1,784)\n",
    "print(x3.shape)\n",
    "#将张量降维为1维\n",
    "x_fallten=torch.flatten(x)\n",
    "print(x_fallten.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graphs 计算图\n",
    "\n",
    "参考\n",
    "* [computation graph](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "\n",
    "\n",
    "What's special about PyTorch's tensor object is that it implicitly creates a computation graph in the background. A computation graph is a a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself.\n",
    "\n",
    "---\n",
    "\n",
    "pytorch可以自动实现计算图，Backpropagation反向传播\n",
    "\n",
    "<img src=\"figs\\tree-def.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "例如上图对应计算 $e=(a+b) \\times (1+b)$，求$a=1,b=2$时的反向传播\n",
    "\n",
    "为此，在对tensor复制时候，必须设置**requires_grad=True**从而pytroch将会保留计算图\n",
    ">we set requires_grad=True to let PyTorch know to keep the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., grad_fn=<AddBackward0>)\n",
      "tensor(3., grad_fn=<AddBackward0>)\n",
      "tensor(9., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(1.0,requires_grad=True)\n",
    "b=torch.tensor(2.0,requires_grad=True)\n",
    "c=a+b\n",
    "d=1+b\n",
    "e=c*d\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as an auto grad framework\n",
    "Now that we have seen that PyTorch keeps the graph around for us, let's use it to compute some gradients for us.\n",
    "\n",
    "Consider the function  $f(x)=(x−2)^2$\n",
    " .\n",
    "\n",
    "Q: Compute  $\\frac{df(x)}{dx}$\n",
    "  and then compute  $f′(1)$\n",
    " .\n",
    "\n",
    "We make a backward() call on the leaf variable (y) in the computation, computing all the gradients of y at once.\n",
    "\n",
    "---\n",
    "1. pytorch 中，张量具有require_grad属性，该属性为True则将跟踪对此张量的所有计算。\n",
    "2. 完成正向传播计算后，可以对计算结果调用backward（）方法，将自动计算所有梯度，并保存至grad属性中\n",
    "3. 张量的grad_fn属性将指向运算生成该张量的方法。\n",
    "通过pytorch中backward可以求得导函数结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正向传播forward: tensor(1., grad_fn=<PowBackward0>)\n",
      "方向传播结果 tensor(-2.)\n",
      "<PowBackward0 object at 0x000002008549E130>\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "x=torch.tensor(1.0,requires_grad=True)\n",
    "y=f(x)\n",
    "print(\"正向传播forward:\",y)\n",
    "y.backward()\n",
    "print(\"方向传播结果\",x.grad)\n",
    "print(y.grad_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以设置一个嵌套函数，并求得不同层变量的导数\n",
    "设\n",
    "$$y=x^2 \\rightarrow z=y+w \\rightarrow e=z\\times 3+5$$\n",
    "其中$x=2,w=3$求位于链式求导叶子节点的$\\frac{de}{dx},\\frac{de}{dw}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward result: tensor(21., grad_fn=<MulBackward0>)\n",
      "e函数求梯度方法： <MulBackward0 object at 0x000002008750C340>\n",
      "y函数求梯度方法： <PowBackward0 object at 0x000002008750C340>\n",
      "x的梯度 tensor(12.)\n",
      "w的梯度 tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "def y(x):\n",
    "    return x**2\n",
    "def z(y,w):\n",
    "    return y+w\n",
    "def e(z):\n",
    "    return z*3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x=torch.tensor(2.0,requires_grad=True)\n",
    "    w=torch.tensor(3.0,requires_grad=True)\n",
    "    ##forwad\n",
    "    y=y(x)\n",
    "    z=z(y,w)\n",
    "    e=e(z)\n",
    "    print(\"forward result:\",e)\n",
    "    #backward \n",
    "    e.backward()\n",
    "    print(\"e函数求梯度方法：\",e.grad_fn)\n",
    "    print(\"y函数求梯度方法：\",y.grad_fn)\n",
    "    print(\"x的梯度\",x.grad)\n",
    "    print(\"w的梯度\",w.grad)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also find gradients of functions.\n",
    "\n",
    "Let  $w=[w_1,w_2]^T$\n",
    " \n",
    "Consider  $(w)=2w_1w_2+w_2\\cos(w_1)$\n",
    " \n",
    "Q: Compute  $∇wg(w)$\n",
    "  and verify $ ∇wg([π,1])=[2,π−1]^T$\n",
    "\n",
    "---\n",
    "\n",
    "进一步求得偏导数，并构成梯度向量\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwad\n",
      "tensor(5.2832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "backward\n",
      "tensor([2.0000, 5.2832], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2*w[0]*w[1]+w[1]*torch.cos(w[0])\n",
    "if __name__ == '__main__':\n",
    "    w=torch.tensor([torch.pi,1],dtype=float,requires_grad=True)\n",
    "    print(\"forwad\")\n",
    "    g=g(w)\n",
    "    print(g)\n",
    "    print(\"backward\")\n",
    "    g.backward()\n",
    "    print(w.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于以上torch对backward，梯度计算方式，可以结合实现梯度下降法；\n",
    "\n",
    "---\n",
    "Using the gradients\n",
    "Now that we have gradients, we can use our favorite optimization algorithm: gradient descent!\n",
    "\n",
    "Let  $f$\n",
    "  the same function we defined above.\n",
    "\n",
    "Q: What is the value of $ x$\n",
    "  that minimizes $ f$?\n",
    "\n",
    "  ---\n",
    "\n",
    "  注意在此"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5568\\1245778344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "if __name__=='__main__':\n",
    "    iter=100\n",
    "    learning_rate=0.8\n",
    "    x=torch.tensor([100.0],requires_grad=True)\n",
    "    \n",
    "    for i in range(iter):\n",
    "        y=f(x)\n",
    "        y.backward()\n",
    "        #计算时，必须使用x.data\n",
    "        x.data=x.data-(x.grad)*learning_rate\n",
    "        print(x.data)\n",
    "        \n",
    "        x.grad.zero_() \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Pytorch建立线性方程\n",
    "\n",
    "1. 构建基本的线性方程\n",
    "2. 描述方程各个部分\n",
    "\n",
    "---\n",
    "Linear Module \n",
    " \n",
    "The bread and butter of modules is the Linear module which does a linear transformation with a bias. It takes the input and output dimensions as parameters, and creates the weights in the object.\n",
    "\n",
    "Unlike how we initialized our  w manually, the Linear module automatically initializes the weights randomly. For minimizing non convex loss functions (e.g. training neural networks), initialization is important and can affect results. If training isn't working as well as expected, one thing to try is manually initializing the weights to something different from the default. PyTorch implements some common initializations in torch.nn.init.\n",
    "\n",
    "---\n",
    "\n",
    "pytorch 在linear层将会自动初始化权参、偏参的初始值，通过torch.nn.init 实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch as t \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "class LinerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinerModel, self).__init__()\n",
    "        self.linear=nn.Linear(in_features=1,out_features=1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "if  __name__ == '__main__':\n",
    "    \n",
    " \n",
    "\n",
    "    #plt.scatter(x,y)\n",
    "    x_t=torch.tensor([[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]]).T\n",
    "    y_t=torch.tensor([[10,  22,  13,  43,  20,  22,  33,  50,  62, 48,  55,  75,  62,  73,  81,  76,  64,  82,  90,  93]]).T\n",
    "    print(x_t.shape)\n",
    "    linear_module=LinerModel()\n",
    "    y_pred=linear_module.forward(x_t)\n",
    "    print(\"通过一次\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor torch.Size([3, 1])\n",
      "transormed torch.Size([3, 4])\n",
      "\n",
      "We can see that the weights exist in the background\n",
      "\n",
      "W: Parameter containing:\n",
      "tensor([[ 0.7270],\n",
      "        [ 0.7984],\n",
      "        [-0.8338],\n",
      "        [ 0.7386]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([ 0.5252, -0.5931,  0.0407,  0.5955], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "linear_module = nn.Linear(1, d_out)\n",
    "\n",
    "example_tensor = torch.tensor([[1.,2,3]]).T\n",
    "# applys a linear transformation to the data\n",
    "transformed = linear_module(example_tensor)\n",
    "print('example_tensor', example_tensor.shape)\n",
    "print('transormed', transformed.shape)\n",
    "print()\n",
    "print('We can see that the weights exist in the background\\n')\n",
    "print('W:', linear_module.weight)\n",
    "print('b:', linear_module.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4500649f3376875055274a6ea5762786a730155ceb79c8798b9983f2d85af345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
