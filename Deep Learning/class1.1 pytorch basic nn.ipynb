{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch 基础\n",
    "\n",
    "本课内容包括\n",
    "1. 基本的pytorch入门\n",
    "2. pytorch中向量、导数、偏导操作，参考之前线性代数以及微积分\n",
    "3. 基于pytorch构建一个线性回归网络\n",
    "4. 基于pytorch实现class1 对图片的分类以及优化\n",
    "\n",
    "---\n",
    "\n",
    "参考资料\n",
    "\n",
    "1. [ws university pytorch introudction](https://courses.cs.washington.edu/courses/cse446/19au/section9.html)\n",
    "2. [li hong yi](https://www.youtube.com/watch?v=kQeezFrNoOg)\n",
    "3. [pytorch homepage](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.安装pytorch\n",
    "\n",
    "与tensorflow相同，pytorch分为cpu版本与gpu版本，官网有相对于安装代码\n",
    "[安装页面](https://pytorch.org/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参看本地计算机是否支持GPU以及GPU型号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only support cpu\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def pytrochGPU(memoryview=False):\n",
    "    support=torch.cuda.is_available()\n",
    "    if support==True:\n",
    "        print(\"GPU is available\")\n",
    "        print(\"GPU的数量\",torch.cuda.device_count())\n",
    "        print(\"GPU的名字\",torch.cuda.get_device_name(0))\n",
    "        print(\"当前GPU索引\",torch.cuda.current_device())\n",
    "    else:\n",
    "        print(\"only support cpu\")\n",
    "if __name__ == '__main__':\n",
    "    print(pytrochGPU())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于可能本地计算机没有gpu，为了避免出错，首先判断是GPU还是CPU，并将数据在对应设备上进行运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "x=torch.randn(2,3)\n",
    "x=x.to(device)\n",
    "print(x.device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "pytorch 中torch的作用类似numpy，包括创建数组，进行线性代数运算等，但torch还可以直接进行求导运算\n",
    "\n",
    "numpy 与 torch之间也可以相互转化\n",
    "\n",
    "例如numpy中创建数组与torch中创建tensor张量对比\n",
    "\n",
    "```python \n",
    "np.array([12,3])\n",
    "torch.tensor([12,3])\n",
    "```\n",
    "\n",
    "1. array与tensor相互转换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([2, 3, 2])\n",
      "<class 'numpy.ndarray'>\n",
      "[2 3 2]\n",
      "<class 'numpy.ndarray'>\n",
      "(2, 3)\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "if __name__ == '__main__':\n",
    "    x_tensor=torch.tensor([2,3,2])\n",
    "    print(type(x_tensor))\n",
    "    print(x_tensor)\n",
    "    y_array=x_tensor.numpy()#转换tensor到array\n",
    "    print(type(y_array))\n",
    "    print(y_array)\n",
    "\n",
    "    x_arrray=np.array([[2,2,3],[2,2,2]])\n",
    "    print(type(x_arrray))\n",
    "    print(x_arrray.shape)\n",
    "    y_tensor=torch.from_numpy(x_arrray)\n",
    "    print(type(y_tensor))\n",
    "    print(y_tensor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. tensor 中的线性代数计算\n",
    "tensor中的计算与numpy中基本一致，包括加减，内积,求norm等。\n",
    "\n",
    "例如以下代码模拟了一个神经网络affine层中的输入值与权参相乘与偏参相加的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17., 18.], dtype=torch.float64)\n",
      "tensor(24.7588, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# 注意，torch在求norm时，要求数据必须为float数据，故在开始就必须制定向量的数据类型\n",
    "x=torch.tensor([2,3,3],dtype=float)\n",
    "w=torch.tensor([[2,2],[1,2],[3,2]],dtype=float)\n",
    "b=torch.tensor([1,2],dtype=float)\n",
    "z=x@w+b\n",
    "print(z)\n",
    "print(torch.norm(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor中正态分布、随机值等的生成与numpy也一致\n",
    "\n",
    "```python\n",
    "torch.rand(维度1数，维度2数，维度3数)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(2,2,2,2)\n",
    "print(x)\n",
    "x_a=np.random.rand(2,2,2,2,2)\n",
    "print(x_a)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor 形状改变\n",
    "* Tensor.view-改变形状\n",
    "* Tensor.flatten-降维为1维\n",
    "\n",
    "We can use the Tensor.view() function to reshape tensors similarly to numpy.reshape()\n",
    "\n",
    "It can also automatically calculate the correct dimension if a -1 is passed in. This is useful if we are working with batches, but the batch size is unknown.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 28, 28])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([1000, 784])\n",
      "torch.Size([784000])\n"
     ]
    }
   ],
   "source": [
    "batch=1000\n",
    "img_x=28\n",
    "img_y=28\n",
    "#生成一个（1000，28，29）形状的张量，模拟1000张28*28图像\n",
    "x=torch.rand(batch,img_x,img_y)\n",
    "print(x.shape)\n",
    "#将x变更为(1000,784)形状的张量\n",
    "x2=x.view(batch,img_x*img_y)\n",
    "print(x2.shape)\n",
    "#当不确定某维度的大小（batch大小），可以设为-1，torch将自动赋值\n",
    "x3=x.view(batch,-1)\n",
    "print(x3.shape)\n",
    "x3=x.view(-1,784)\n",
    "print(x3.shape)\n",
    "#将张量降维为1维\n",
    "x_fallten=torch.flatten(x)\n",
    "print(x_fallten.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation graphs 计算图\n",
    "\n",
    "参考\n",
    "* [computation graph](https://colah.github.io/posts/2015-08-Backprop/)\n",
    "\n",
    "\n",
    "What's special about PyTorch's tensor object is that it implicitly creates a computation graph in the background. A computation graph is a a way of writing a mathematical expression as a graph. There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself.\n",
    "\n",
    "---\n",
    "\n",
    "pytorch可以自动实现计算图，Backpropagation反向传播\n",
    "\n",
    "<img src=\"figs\\tree-def.png\" height=\"50%\" width=\"50%\">\n",
    "\n",
    "例如上图对应计算 $e=(a+b) \\times (1+b)$，求$a=1,b=2$时的反向传播\n",
    "\n",
    "为此，在对tensor复制时候，必须设置**requires_grad=True**从而pytroch将会保留计算图\n",
    ">we set requires_grad=True to let PyTorch know to keep the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., grad_fn=<AddBackward0>)\n",
      "tensor(3., grad_fn=<AddBackward0>)\n",
      "tensor(9., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a=torch.tensor(1.0,requires_grad=True)\n",
    "b=torch.tensor(2.0,requires_grad=True)\n",
    "c=a+b\n",
    "d=1+b\n",
    "e=c*d\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch as an auto grad framework\n",
    "Now that we have seen that PyTorch keeps the graph around for us, let's use it to compute some gradients for us.\n",
    "\n",
    "Consider the function  $f(x)=(x−2)^2$\n",
    " .\n",
    "\n",
    "Q: Compute  $\\frac{df(x)}{dx}$\n",
    "  and then compute  $f′(1)$\n",
    " .\n",
    "\n",
    "We make a backward() call on the leaf variable (y) in the computation, computing all the gradients of y at once.\n",
    "\n",
    "---\n",
    "1. pytorch 中，张量具有require_grad属性，该属性为True则将跟踪对此张量的所有计算。\n",
    "2. 完成正向传播计算后，可以对计算结果调用backward（）方法，将自动计算所有梯度，并保存至grad属性中\n",
    "3. 张量的grad_fn属性将指向运算生成该张量的方法。\n",
    "通过pytorch中backward可以求得导函数结果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正向传播forward: tensor(1., grad_fn=<PowBackward0>)\n",
      "方向传播结果 tensor(-2.)\n",
      "<PowBackward0 object at 0x000002008549E130>\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "x=torch.tensor(1.0,requires_grad=True)\n",
    "y=f(x)\n",
    "print(\"正向传播forward:\",y)\n",
    "y.backward()\n",
    "print(\"方向传播结果\",x.grad)\n",
    "print(y.grad_fn)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以设置一个嵌套函数，并求得不同层变量的导数\n",
    "设\n",
    "$$y=x^2 \\rightarrow z=y+w \\rightarrow e=z\\times 3+5$$\n",
    "其中$x=2,w=3$求位于链式求导叶子节点的$\\frac{de}{dx},\\frac{de}{dw}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward result: tensor(21., grad_fn=<MulBackward0>)\n",
      "e函数求梯度方法： <MulBackward0 object at 0x000002008750C340>\n",
      "y函数求梯度方法： <PowBackward0 object at 0x000002008750C340>\n",
      "x的梯度 tensor(12.)\n",
      "w的梯度 tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "def y(x):\n",
    "    return x**2\n",
    "def z(y,w):\n",
    "    return y+w\n",
    "def e(z):\n",
    "    return z*3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x=torch.tensor(2.0,requires_grad=True)\n",
    "    w=torch.tensor(3.0,requires_grad=True)\n",
    "    ##forwad\n",
    "    y=y(x)\n",
    "    z=z(y,w)\n",
    "    e=e(z)\n",
    "    print(\"forward result:\",e)\n",
    "    #backward \n",
    "    e.backward()\n",
    "    print(\"e函数求梯度方法：\",e.grad_fn)\n",
    "    print(\"y函数求梯度方法：\",y.grad_fn)\n",
    "    print(\"x的梯度\",x.grad)\n",
    "    print(\"w的梯度\",w.grad)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also find gradients of functions.\n",
    "\n",
    "Let  $w=[w_1,w_2]^T$\n",
    " \n",
    "Consider  $(w)=2w_1w_2+w_2\\cos(w_1)$\n",
    " \n",
    "Q: Compute  $∇wg(w)$\n",
    "  and verify $ ∇wg([π,1])=[2,π−1]^T$\n",
    "\n",
    "---\n",
    "\n",
    "进一步求得偏导数，并构成梯度向量\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwad\n",
      "tensor(5.2832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "backward\n",
      "tensor([2.0000, 5.2832], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "def g(w):\n",
    "    return 2*w[0]*w[1]+w[1]*torch.cos(w[0])\n",
    "if __name__ == '__main__':\n",
    "    w=torch.tensor([torch.pi,1],dtype=float,requires_grad=True)\n",
    "    print(\"forwad\")\n",
    "    g=g(w)\n",
    "    print(g)\n",
    "    print(\"backward\")\n",
    "    g.backward()\n",
    "    print(w.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于以上torch对backward，梯度计算方式，可以结合实现梯度下降法；\n",
    "\n",
    "---\n",
    "Using the gradients\n",
    "Now that we have gradients, we can use our favorite optimization algorithm: gradient descent!\n",
    "\n",
    "Let  $f$\n",
    "  the same function we defined above.\n",
    "\n",
    "Q: What is the value of $ x$\n",
    "  that minimizes $ f$?\n",
    "\n",
    "  ---\n",
    "\n",
    "  注意在此"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5568\\1245778344.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "if __name__=='__main__':\n",
    "    iter=100\n",
    "    learning_rate=0.8\n",
    "    x=torch.tensor([100.0],requires_grad=True)\n",
    "    \n",
    "    for i in range(iter):\n",
    "        y=f(x)\n",
    "        y.backward()\n",
    "        #计算时，必须使用x.data\n",
    "        x.data=x.data-(x.grad)*learning_rate\n",
    "        print(x.data)\n",
    "        \n",
    "        x.grad.zero_() \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于Pytorch建立线性方程\n",
    "\n",
    "1. 构建基本的线性方程\n",
    "2. 描述方程各个部分\n",
    "\n",
    "---\n",
    "Linear Module \n",
    " \n",
    "The bread and butter of modules is the Linear module which does a linear transformation with a bias. It takes the input and output dimensions as parameters, and creates the weights in the object.\n",
    "\n",
    "Unlike how we initialized our  w manually, the Linear module automatically initializes the weights randomly. For minimizing non convex loss functions (e.g. training neural networks), initialization is important and can affect results. If training isn't working as well as expected, one thing to try is manually initializing the weights to something different from the default. PyTorch implements some common initializations in torch.nn.init.\n",
    "\n",
    "---\n",
    "\n",
    "pytorch 在linear层将会自动初始化权参、偏参的初始值，通过torch.nn.init 实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim 优化函数\n",
    "PyTorch implements a number of gradient-based optimization methods in torch.optim, including Gradient Descent.\n",
    "\n",
    " At the minimum, it takes in the model parameters and a learning rate.\n",
    "\n",
    "Optimizers do not compute the gradients for you, so you must call backward() yourself.\n",
    "\n",
    "You also must call the optim.zero_grad() function before calling backward() since by default\n",
    "\n",
    " PyTorch does and inplace add to the .grad member variable rather than overwriting it.\n",
    "\n",
    "This does both the detach_() and zero_() calls on all tensor's grad variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "class LinerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinerModel, self).__init__()\n",
    "        self.linear=nn.Linear(in_features=1,out_features=1)\n",
    "        self.weight=self.linear.weight\n",
    "        self.bias=self.linear.bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    def getParamters(self):\n",
    "        return self.weight,self.bias\n",
    "if  __name__ == '__main__':\n",
    "    \n",
    " \n",
    "\n",
    "    #plt.scatter(x,y)\n",
    "    x_t=t.tensor([[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]]).T\n",
    "    y_t=t.tensor([[10,  22,  13,  43,  20,  22,  33,  50,  62, 48,  55,  75,  62,  73,  81,  76,  64,  82,  90,  93]]).T\n",
    "    x_n=x_t.numpy().T\n",
    "    y_n=y_t.numpy().T\n",
    "    plt.scatter(x_n,y_n)\n",
    "    print(x_t.shape)\n",
    "    linear_module=LinerModel()\n",
    "    y_pred=linear_module.forward(x_t)\n",
    "    print(\"通过一次forwad后得到y predict\")\n",
    "    weight,bias=linear_module.getParamters()\n",
    "    print(\"模型中参数：\",linear_module.parameters())\n",
    "    print(\"初始权参值:\",weight)\n",
    "    print(\"初始偏参值:\",bias)\n",
    "    ########################实例化损失函数，优化函数########################################\n",
    "    loss_fn=nn.MSELoss()#定义均方误差函数作为损失函数\n",
    "    optimizer=t.optim.SGD(linear_module.parameters(),lr=0.001)#使用SGD优化函数，对inear_module.parameters()中权参、偏参进行梯度下降法优化，学习率为0.001\n",
    "    \n",
    "    iter=1000\n",
    "    i=0\n",
    "    for epoch in range(iter):#循环10000次epoch\n",
    "        print(\"第\",epoch,\"次epoch\")\n",
    "        for x,y in zip(x_t,y_t):#对训练集中每一个值都投入模型运算并求得对应梯度，并运行一次梯度下降，完成一个epoch\n",
    "            i+=1\n",
    "            y_predict=linear_module(x_t)\n",
    "            y_predict=y_predict.to(t.float32)\n",
    "            y_t=y_t.to(t.float32)\n",
    "            loss=loss_fn(y_predict, y_t)\n",
    "            optimizer.zero_grad()#清空上次的梯度积累\n",
    "            loss.backward()#损失函数进行backward反向传播\n",
    "            if iter>900:\n",
    "                \n",
    "                print(\"第\",i,\"次梯度下降后\")\n",
    "                print(\"模型中权参梯度：\",linear_module.weight.grad)\n",
    "                print(\"模型中偏参梯度\",linear_module.bias.grad)\n",
    "            optimizer.step()#根据计算得到的梯度开始梯度下降更新即W=W-∇w*learningrate\n",
    "    \n",
    "    ################################输出最终优化的权参和偏参##############################################\n",
    "    print(\"优化后得到权参、偏参分别为：\",list(linear_module.named_parameters()))\n",
    "    y_predict=linear_module(x_t).detach().numpy()#通过模型进行预测.detach()实现返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。\n",
    "    plt.scatter(x_n,y_n)\n",
    "    plt.plot(x_t,y_predict)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example_tensor torch.Size([3, 1])\n",
      "transormed torch.Size([3, 4])\n",
      "\n",
      "We can see that the weights exist in the background\n",
      "\n",
      "W: Parameter containing:\n",
      "tensor([[ 0.7270],\n",
      "        [ 0.7984],\n",
      "        [-0.8338],\n",
      "        [ 0.7386]], requires_grad=True)\n",
      "b: Parameter containing:\n",
      "tensor([ 0.5252, -0.5931,  0.0407,  0.5955], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "linear_module = nn.Linear(1, d_out)\n",
    "\n",
    "example_tensor = torch.tensor([[1.,2,3]]).T\n",
    "# applys a linear transformation to the data\n",
    "transformed = linear_module(example_tensor)\n",
    "print('example_tensor', example_tensor.shape)\n",
    "print('transormed', transformed.shape)\n",
    "print()\n",
    "print('We can see that the weights exist in the background\\n')\n",
    "print('W:', linear_module.weight)\n",
    "print('b:', linear_module.bias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的改进\n",
    "\n",
    "以上模型没有加入激活函数，只是一个简单的一层线性全连接网络。\n",
    "\n",
    "以下改进模型，加入激活层，使用relu激活函数：\n",
    "\n",
    "---\n",
    "\n",
    "Activation functions\n",
    "PyTorch implements a number of activation functions including but not limited to ReLU, Tanh, and Sigmoid. Since they are modules, they need to be instantiated.\n",
    "\n",
    "---\n",
    "\n",
    "pytorch中，激活层的使用与之前numpy构建相似，首先实例化一个激活函数，正向传播全连接层input的数据后，得到激活值，传递给下一个全连接层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4335, 0.6307], dtype=torch.float64)\n",
      "relu 季候函数\n",
      "tensor([1.4335, 0.6307], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "relu_fn=nn.ReLU()#实例化Relu函数\n",
    "x=t.randn(2,dtype=float)\n",
    "print(x)\n",
    "print(\"relu 激活函数很简单，即如果x>0则输出x反之输出0\")\n",
    "y=relu_fn.forward(x)\n",
    "print(y)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现将激活函数加入之前线性层之后，注意输出层也必须为一个线性层，而后在外部使用平均误差函数\n",
    "\n",
    "---\n",
    "出现问题\n",
    "1. 单独将数据放入GPU无法训练，还必须将model放入gpu\n",
    "```python\n",
    "    x_t=x_t.to(device)#将数据移动到gpu\n",
    "    y_t=y_t.to(device)#将数据运动到gpu\n",
    "    linear_module=LinerModel()\n",
    "    linear_module.to(device)#模型也必须移动到GPU\n",
    "```\n",
    "2. 如果想把CUDA tensor格式的数据改成numpy时，需要先将其转换成cpu float-tensor随后再转到numpy格式。 numpy不能读取CUDA tensor 需要将它转化为 CPU tensor\n",
    "```python\n",
    "    x_n=x_t.cpu().numpy().T\n",
    "    y_n=y_t.cpu().numpy().T\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "torch.Size([20, 1])\n",
      "通过一次forwad后得到y predict\n",
      "模型中参数： <generator object Module.parameters at 0x7f13dc35bc80>\n",
      "初始权参值: Parameter containing:\n",
      "tensor([[-0.1672]], device='cuda:0', requires_grad=True)\n",
      "初始偏参值: Parameter containing:\n",
      "tensor([0.2451], device='cuda:0', requires_grad=True)\n",
      "优化后得到权参、偏参分别为： [('weight', Parameter containing:\n",
      "tensor([[-2.3126]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([8.8496], device='cuda:0', requires_grad=True)), ('linear2.weight', Parameter containing:\n",
      "tensor([[-9.1446]], device='cuda:0', requires_grad=True)), ('linear2.bias', Parameter containing:\n",
      "tensor([80.9977], device='cuda:0', requires_grad=True))]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as t \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "class LinerModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinerModel, self).__init__()\n",
    "        self.linear1=nn.Linear(in_features=1,out_features=1)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.linear2=nn.Linear(in_features=1,out_features=1)\n",
    "        self.weight=self.linear1.weight\n",
    "        self.bias=self.linear1.bias\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "    def getParamters(self):\n",
    "        return self.weight,self.bias\n",
    "if  __name__ == '__main__':\n",
    "    \n",
    " \n",
    "\n",
    "    #plt.scatter(x,y)\n",
    "    x_t=t.tensor([[0.50,0.75,1.00,1.25,1.50,1.75,1.75,2.00,2.25,2.50,2.75,3.00,3.25,3.50,4.00,4.25,4.50,4.75,5.00,5.50]]).T\n",
    "    y_t=t.tensor([[10,  22,  13,  43,  20,  22,  33,  50,  62, 48,  55,  75,  62,  73,  81,  76,  64,  82,  90,  93]]).T\n",
    "    \n",
    "    device=\"cuda\" if t.cuda.is_available() else \"cpu\"\n",
    "    x_t=x_t.to(device)#将数据移动到gpu\n",
    "    y_t=y_t.to(device)#将数据运动到gpu\n",
    "    \n",
    "    print(x_t.device)\n",
    "    x_n=x_t.cpu().numpy().T\n",
    "    y_n=y_t.cpu().numpy().T\n",
    "    print(x_t.shape)\n",
    "    linear_module=LinerModel()\n",
    "    linear_module.to(device)#模型也必须移动到GPU\n",
    "    y_pred=linear_module.forward(x_t)\n",
    "    print(\"通过一次forwad后得到y predict\")\n",
    "    weight,bias=linear_module.getParamters()\n",
    "    print(\"模型中参数：\",linear_module.parameters())\n",
    "    print(\"初始权参值:\",weight)\n",
    "    print(\"初始偏参值:\",bias)\n",
    "    ########################实例化损失函数，优化函数########################################\n",
    "    loss_fn=nn.MSELoss()#定义均方误差函数作为损失函数\n",
    "    optimizer=t.optim.SGD(linear_module.parameters(),lr=0.001)#使用SGD优化函数，对inear_module.parameters()中权参、偏参进行梯度下降法优化，学习率为0.001\n",
    "    \n",
    "    iter=1000\n",
    "    i=0\n",
    "    for epoch in range(iter):#循环10000次epoch\n",
    "        #print(\"第\",epoch,\"次epoch\")\n",
    "        for x,y in zip(x_t,y_t):#对训练集中每一个值都投入模型运算并求得对应梯度，并运行一次梯度下降，完成一个epoch\n",
    "            i+=1\n",
    "            y_predict=linear_module(x_t)\n",
    "            y_predict=y_predict.to(t.float32)\n",
    "            y_t=y_t.to(t.float32)\n",
    "            loss=loss_fn(y_predict, y_t)\n",
    "            optimizer.zero_grad()#清空上次的梯度积累\n",
    "            loss.backward()#损失函数进行backward反向传播\n",
    "            if iter>9000:\n",
    "                \n",
    "                print(\"第\",i,\"次梯度下降后\")\n",
    "                print(\"模型中权参梯度：\",linear_module.weight.grad)\n",
    "                print(\"模型中偏参梯度\",linear_module.bias.grad)\n",
    "            optimizer.step()#根据计算得到的梯度开始梯度下降更新即W=W-∇w*learningrate\n",
    "    \n",
    "    ################################输出最终优化的权参和偏参##############################################\n",
    "    print(\"优化后得到权参、偏参分别为：\",list(linear_module.named_parameters()))\n",
    "    y_predict=linear_module(x_t).detach().cpu().numpy()#通过模型进行预测.detach()实现返回一个新的tensor，从当前计算图中分离下来的，但是仍指向原变量的存放位置,不同之处只是requires_grad为false，得到的这个tensor永远不需要计算其梯度，不具有grad。\n",
    "    plt.scatter(x_n,y_n)\n",
    "    plt.plot(x_n,y_predict.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于以上代码，重现华盛顿大学 pytroch课程\n",
    "\n",
    "首先定义相关的待拟合函数\n",
    "$$y=4 \\times sin(\\pi \\times x)\\times cos(6 \\times x^2)$$\n",
    "\n",
    "并展现相关的图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 1]) torch.Size([200, 1])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self,input,output):\n",
    "        super(simpleNet, self).__init__()\n",
    "        self.linear1=nn.Linear(in_features=input, out_features=200)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.linear2=nn.Linear(in_features=200, out_features=300)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.linear3=nn.Linear(in_features=300, out_features=output)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.linear3(x)\n",
    "        return x\n",
    "if __name__=='__main__':\n",
    "    \n",
    "\n",
    "    d = 1\n",
    "    n = 200\n",
    "    X = torch.rand(n,d)\n",
    "    iterm=50000\n",
    "    y = 4 * torch.sin(np.pi * X) * torch.cos(6*np.pi*X**2)\n",
    "    print(X.shape, y.shape)\n",
    " \n",
    "################################################################\n",
    "    net=simpleNet(input=d,output=d)\n",
    "    loss_fn=nn.MSELoss()\n",
    "    opt=torch.optim.Adam(params=net.parameters(),lr=0.01)\n",
    "    for epoch in range(iterm):\n",
    "         \n",
    "            y_pred=net.forward(X)\n",
    "            loss=loss_fn(y_pred,y)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()#更新权参、偏参\n",
    "    \n",
    "    X_grid = torch.from_numpy(np.linspace(0,1,50)).float().view(-1, d)\n",
    "    y_hat = net(X_grid)\n",
    "    plt.scatter(X.numpy(), y.numpy())\n",
    "    plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), 'r')\n",
    "    plt.title('plot of $f(x)$ and $\\hat{f}(x)$')\n",
    "    plt.xlabel('$x$')\n",
    "    plt.ylabel('$y$')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential\n",
    "---\n",
    "除了使用创建modle类，再进行模型训练方法，也可以通过Sequenltal将模型串联，开始运算，但这种方式正规性不强，与tensorflow类似。\n",
    "\n",
    "Many times, we want to compose Modules together. torch.nn.Sequential provides a good interface for composing simple modules."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \n",
    "在以上案例基础上，进一步实现多元回归的神经网络，包括\n",
    "1. 数据获取\n",
    "2. 数据观察\n",
    "3. 对数据的标准化\n",
    "4. 实现mini-batch载入\n",
    "5. 通过train、test数据划分验证数据\n",
    "6. 结合之前知识，实验模型优化\n",
    "总之，实现一个完整的面向数值多元数据的神经网络构建流程\n",
    "\n",
    "参考\n",
    "* [神经网络多元回归](https://blog.csdn.net/qq_39567427/article/details/111935833)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.version' from 'c:\\\\Users\\\\tom\\\\anaconda3\\\\lib\\\\site-packages\\\\torch\\\\version.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import scale\n",
    "from torchsummary import summary\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "torch.version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入数据，进行基本数据观察，数据来源于sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tom\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "载入数据tpye <class 'sklearn.utils.Bunch'>\n",
      "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename', 'data_module'])\n",
      "自变量多元数据：\n",
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "[[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      "  4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]]\n",
      "自变量数据类型： <class 'numpy.ndarray'> 数据形状： (506, 13)\n",
      "因变量数据类型： <class 'numpy.ndarray'> 数据形状： (506, 1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 200]           2,800\n",
      "              ReLU-2                  [-1, 200]               0\n",
      "            Linear-3                  [-1, 500]         100,500\n",
      "              ReLU-4                  [-1, 500]               0\n",
      "            Linear-5                  [-1, 300]         150,300\n",
      "              ReLU-6                  [-1, 300]               0\n",
      "            Linear-7                    [-1, 1]             301\n",
      "================================================================\n",
      "Total params: 253,901\n",
      "Trainable params: 253,901\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.97\n",
      "Estimated Total Size (MB): 0.98\n",
      "----------------------------------------------------------------\n",
      "完成一个epoch，需要读取 15 个batchs\n",
      "epoch： 0\n",
      "epoch： 1\n",
      "epoch： 2\n",
      "epoch： 3\n",
      "epoch： 4\n",
      "epoch： 5\n",
      "epoch： 6\n",
      "epoch： 7\n",
      "epoch： 8\n",
      "epoch： 9\n",
      "epoch： 10\n",
      "epoch： 11\n",
      "epoch： 12\n",
      "epoch： 13\n",
      "epoch： 14\n",
      "epoch： 15\n",
      "epoch： 16\n",
      "epoch： 17\n",
      "epoch： 18\n",
      "epoch： 19\n",
      "epoch： 20\n",
      "epoch： 21\n",
      "epoch： 22\n",
      "epoch： 23\n",
      "epoch： 24\n",
      "epoch： 25\n",
      "epoch： 26\n",
      "epoch： 27\n",
      "epoch： 28\n",
      "epoch： 29\n",
      "epoch： 30\n",
      "epoch： 31\n",
      "epoch： 32\n",
      "epoch： 33\n",
      "epoch： 34\n",
      "epoch： 35\n",
      "epoch： 36\n",
      "epoch： 37\n",
      "epoch： 38\n",
      "epoch： 39\n",
      "epoch： 40\n",
      "epoch： 41\n",
      "epoch： 42\n",
      "epoch： 43\n",
      "epoch： 44\n",
      "epoch： 45\n",
      "epoch： 46\n",
      "epoch： 47\n",
      "epoch： 48\n",
      "epoch： 49\n",
      "epoch： 50\n",
      "epoch： 51\n",
      "epoch： 52\n",
      "epoch： 53\n",
      "epoch： 54\n",
      "epoch： 55\n",
      "epoch： 56\n",
      "epoch： 57\n",
      "epoch： 58\n",
      "epoch： 59\n",
      "epoch： 60\n",
      "epoch： 61\n",
      "epoch： 62\n",
      "epoch： 63\n",
      "epoch： 64\n",
      "epoch： 65\n",
      "epoch： 66\n",
      "epoch： 67\n",
      "epoch： 68\n",
      "epoch： 69\n",
      "epoch： 70\n",
      "epoch： 71\n",
      "epoch： 72\n",
      "epoch： 73\n",
      "epoch： 74\n",
      "epoch： 75\n",
      "epoch： 76\n",
      "epoch： 77\n",
      "epoch： 78\n",
      "epoch： 79\n",
      "epoch： 80\n",
      "epoch： 81\n",
      "epoch： 82\n",
      "epoch： 83\n",
      "epoch： 84\n",
      "epoch： 85\n",
      "epoch： 86\n",
      "epoch： 87\n",
      "epoch： 88\n",
      "epoch： 89\n",
      "epoch： 90\n",
      "epoch： 91\n",
      "epoch： 92\n",
      "epoch： 93\n",
      "epoch： 94\n",
      "epoch： 95\n",
      "epoch： 96\n",
      "epoch： 97\n",
      "epoch： 98\n",
      "epoch： 99\n",
      "epoch： 100\n",
      "epoch： 101\n",
      "epoch： 102\n",
      "epoch： 103\n",
      "epoch： 104\n",
      "epoch： 105\n",
      "epoch： 106\n",
      "epoch： 107\n",
      "epoch： 108\n",
      "epoch： 109\n",
      "epoch： 110\n",
      "epoch： 111\n",
      "epoch： 112\n",
      "epoch： 113\n",
      "epoch： 114\n",
      "epoch： 115\n",
      "epoch： 116\n",
      "epoch： 117\n",
      "epoch： 118\n",
      "epoch： 119\n",
      "epoch： 120\n",
      "epoch： 121\n",
      "epoch： 122\n",
      "epoch： 123\n",
      "epoch： 124\n",
      "epoch： 125\n",
      "epoch： 126\n",
      "epoch： 127\n",
      "epoch： 128\n",
      "epoch： 129\n",
      "epoch： 130\n",
      "epoch： 131\n",
      "epoch： 132\n",
      "epoch： 133\n",
      "epoch： 134\n",
      "epoch： 135\n",
      "epoch： 136\n",
      "epoch： 137\n",
      "epoch： 138\n",
      "epoch： 139\n",
      "epoch： 140\n",
      "epoch： 141\n",
      "epoch： 142\n",
      "epoch： 143\n",
      "epoch： 144\n",
      "epoch： 145\n",
      "epoch： 146\n",
      "epoch： 147\n",
      "epoch： 148\n",
      "epoch： 149\n",
      "epoch： 150\n",
      "epoch： 151\n",
      "epoch： 152\n",
      "epoch： 153\n",
      "epoch： 154\n",
      "epoch： 155\n",
      "epoch： 156\n",
      "epoch： 157\n",
      "epoch： 158\n",
      "epoch： 159\n",
      "epoch： 160\n",
      "epoch： 161\n",
      "epoch： 162\n",
      "epoch： 163\n",
      "epoch： 164\n",
      "epoch： 165\n",
      "epoch： 166\n",
      "epoch： 167\n",
      "epoch： 168\n",
      "epoch： 169\n",
      "epoch： 170\n",
      "epoch： 171\n",
      "epoch： 172\n",
      "epoch： 173\n",
      "epoch： 174\n",
      "epoch： 175\n",
      "epoch： 176\n",
      "epoch： 177\n",
      "epoch： 178\n",
      "epoch： 179\n",
      "epoch： 180\n",
      "epoch： 181\n",
      "epoch： 182\n",
      "epoch： 183\n",
      "epoch： 184\n",
      "epoch： 185\n",
      "epoch： 186\n",
      "epoch： 187\n",
      "epoch： 188\n",
      "epoch： 189\n",
      "epoch： 190\n",
      "epoch： 191\n",
      "epoch： 192\n",
      "epoch： 193\n",
      "epoch： 194\n",
      "epoch： 195\n",
      "epoch： 196\n",
      "epoch： 197\n",
      "epoch： 198\n",
      "epoch： 199\n",
      "epoch： 200\n",
      "epoch： 201\n",
      "epoch： 202\n",
      "epoch： 203\n",
      "epoch： 204\n",
      "epoch： 205\n",
      "epoch： 206\n",
      "epoch： 207\n",
      "epoch： 208\n",
      "epoch： 209\n",
      "epoch： 210\n",
      "epoch： 211\n",
      "epoch： 212\n",
      "epoch： 213\n",
      "epoch： 214\n",
      "epoch： 215\n",
      "epoch： 216\n",
      "epoch： 217\n",
      "epoch： 218\n",
      "epoch： 219\n",
      "epoch： 220\n",
      "epoch： 221\n",
      "epoch： 222\n",
      "epoch： 223\n",
      "epoch： 224\n",
      "epoch： 225\n",
      "epoch： 226\n",
      "epoch： 227\n",
      "epoch： 228\n",
      "epoch： 229\n",
      "epoch： 230\n",
      "epoch： 231\n",
      "epoch： 232\n",
      "epoch： 233\n",
      "epoch： 234\n",
      "epoch： 235\n",
      "epoch： 236\n",
      "epoch： 237\n",
      "epoch： 238\n",
      "epoch： 239\n",
      "epoch： 240\n",
      "epoch： 241\n",
      "epoch： 242\n",
      "epoch： 243\n",
      "epoch： 244\n",
      "epoch： 245\n",
      "epoch： 246\n",
      "epoch： 247\n",
      "epoch： 248\n",
      "epoch： 249\n",
      "epoch： 250\n",
      "epoch： 251\n",
      "epoch： 252\n",
      "epoch： 253\n",
      "epoch： 254\n",
      "epoch： 255\n",
      "epoch： 256\n",
      "epoch： 257\n",
      "epoch： 258\n",
      "epoch： 259\n",
      "epoch： 260\n",
      "epoch： 261\n",
      "epoch： 262\n",
      "epoch： 263\n",
      "epoch： 264\n",
      "epoch： 265\n",
      "epoch： 266\n",
      "epoch： 267\n",
      "epoch： 268\n",
      "epoch： 269\n",
      "epoch： 270\n",
      "epoch： 271\n",
      "epoch： 272\n",
      "epoch： 273\n",
      "epoch： 274\n",
      "epoch： 275\n",
      "epoch： 276\n",
      "epoch： 277\n",
      "epoch： 278\n",
      "epoch： 279\n",
      "epoch： 280\n",
      "epoch： 281\n",
      "epoch： 282\n",
      "epoch： 283\n",
      "epoch： 284\n",
      "epoch： 285\n",
      "epoch： 286\n",
      "epoch： 287\n",
      "epoch： 288\n",
      "epoch： 289\n",
      "epoch： 290\n",
      "epoch： 291\n",
      "epoch： 292\n",
      "epoch： 293\n",
      "epoch： 294\n",
      "epoch： 295\n",
      "epoch： 296\n",
      "epoch： 297\n",
      "epoch： 298\n",
      "epoch： 299\n",
      "epoch： 300\n",
      "epoch： 301\n",
      "epoch： 302\n",
      "epoch： 303\n",
      "epoch： 304\n",
      "epoch： 305\n",
      "epoch： 306\n",
      "epoch： 307\n",
      "epoch： 308\n",
      "epoch： 309\n",
      "epoch： 310\n",
      "epoch： 311\n",
      "epoch： 312\n",
      "epoch： 313\n",
      "epoch： 314\n",
      "epoch： 315\n",
      "epoch： 316\n",
      "epoch： 317\n",
      "epoch： 318\n",
      "epoch： 319\n",
      "epoch： 320\n",
      "epoch： 321\n",
      "epoch： 322\n",
      "epoch： 323\n",
      "epoch： 324\n",
      "epoch： 325\n",
      "epoch： 326\n",
      "epoch： 327\n",
      "epoch： 328\n",
      "epoch： 329\n",
      "epoch： 330\n",
      "epoch： 331\n",
      "epoch： 332\n",
      "epoch： 333\n",
      "epoch： 334\n",
      "epoch： 335\n",
      "epoch： 336\n",
      "epoch： 337\n",
      "epoch： 338\n",
      "epoch： 339\n",
      "epoch： 340\n",
      "epoch： 341\n",
      "epoch： 342\n",
      "epoch： 343\n",
      "epoch： 344\n",
      "epoch： 345\n",
      "epoch： 346\n",
      "epoch： 347\n",
      "epoch： 348\n",
      "epoch： 349\n",
      "epoch： 350\n",
      "epoch： 351\n",
      "epoch： 352\n",
      "epoch： 353\n",
      "epoch： 354\n",
      "epoch： 355\n",
      "epoch： 356\n",
      "epoch： 357\n",
      "epoch： 358\n",
      "epoch： 359\n",
      "epoch： 360\n",
      "epoch： 361\n",
      "epoch： 362\n",
      "epoch： 363\n",
      "epoch： 364\n",
      "epoch： 365\n",
      "epoch： 366\n",
      "epoch： 367\n",
      "epoch： 368\n",
      "epoch： 369\n",
      "epoch： 370\n",
      "epoch： 371\n",
      "epoch： 372\n",
      "epoch： 373\n",
      "epoch： 374\n",
      "epoch： 375\n",
      "epoch： 376\n",
      "epoch： 377\n",
      "epoch： 378\n",
      "epoch： 379\n",
      "epoch： 380\n",
      "epoch： 381\n",
      "epoch： 382\n",
      "epoch： 383\n",
      "epoch： 384\n",
      "epoch： 385\n",
      "epoch： 386\n",
      "epoch： 387\n",
      "epoch： 388\n",
      "epoch： 389\n",
      "epoch： 390\n",
      "epoch： 391\n",
      "epoch： 392\n",
      "epoch： 393\n",
      "epoch： 394\n",
      "epoch： 395\n",
      "epoch： 396\n",
      "epoch： 397\n",
      "epoch： 398\n",
      "epoch： 399\n",
      "epoch： 400\n",
      "epoch： 401\n",
      "epoch： 402\n",
      "epoch： 403\n",
      "epoch： 404\n",
      "epoch： 405\n",
      "epoch： 406\n",
      "epoch： 407\n",
      "epoch： 408\n",
      "epoch： 409\n",
      "epoch： 410\n",
      "epoch： 411\n",
      "epoch： 412\n",
      "epoch： 413\n",
      "epoch： 414\n",
      "epoch： 415\n",
      "epoch： 416\n",
      "epoch： 417\n",
      "epoch： 418\n",
      "epoch： 419\n",
      "epoch： 420\n",
      "epoch： 421\n",
      "epoch： 422\n",
      "epoch： 423\n",
      "epoch： 424\n",
      "epoch： 425\n",
      "epoch： 426\n",
      "epoch： 427\n",
      "epoch： 428\n",
      "epoch： 429\n",
      "epoch： 430\n",
      "epoch： 431\n",
      "epoch： 432\n",
      "epoch： 433\n",
      "epoch： 434\n",
      "epoch： 435\n",
      "epoch： 436\n",
      "epoch： 437\n",
      "epoch： 438\n",
      "epoch： 439\n",
      "epoch： 440\n",
      "epoch： 441\n",
      "epoch： 442\n",
      "epoch： 443\n",
      "epoch： 444\n",
      "epoch： 445\n",
      "epoch： 446\n",
      "epoch： 447\n",
      "epoch： 448\n",
      "epoch： 449\n",
      "epoch： 450\n",
      "epoch： 451\n",
      "epoch： 452\n",
      "epoch： 453\n",
      "epoch： 454\n",
      "epoch： 455\n",
      "epoch： 456\n",
      "epoch： 457\n",
      "epoch： 458\n",
      "epoch： 459\n",
      "epoch： 460\n",
      "epoch： 461\n",
      "epoch： 462\n",
      "epoch： 463\n",
      "epoch： 464\n",
      "epoch： 465\n",
      "epoch： 466\n",
      "epoch： 467\n",
      "epoch： 468\n",
      "epoch： 469\n",
      "epoch： 470\n",
      "epoch： 471\n",
      "epoch： 472\n",
      "epoch： 473\n",
      "epoch： 474\n",
      "epoch： 475\n",
      "epoch： 476\n",
      "epoch： 477\n",
      "epoch： 478\n",
      "epoch： 479\n",
      "epoch： 480\n",
      "epoch： 481\n",
      "epoch： 482\n",
      "epoch： 483\n",
      "epoch： 484\n",
      "epoch： 485\n",
      "epoch： 486\n",
      "epoch： 487\n",
      "epoch： 488\n",
      "epoch： 489\n",
      "epoch： 490\n",
      "epoch： 491\n",
      "epoch： 492\n",
      "epoch： 493\n",
      "epoch： 494\n",
      "epoch： 495\n",
      "epoch： 496\n",
      "epoch： 497\n",
      "epoch： 498\n",
      "epoch： 499\n",
      "epoch： 500\n",
      "epoch： 501\n",
      "epoch： 502\n",
      "epoch： 503\n",
      "epoch： 504\n",
      "epoch： 505\n",
      "epoch： 506\n",
      "epoch： 507\n",
      "epoch： 508\n",
      "epoch： 509\n",
      "epoch： 510\n",
      "epoch： 511\n",
      "epoch： 512\n",
      "epoch： 513\n",
      "epoch： 514\n",
      "epoch： 515\n",
      "epoch： 516\n",
      "epoch： 517\n",
      "epoch： 518\n",
      "epoch： 519\n",
      "epoch： 520\n",
      "epoch： 521\n",
      "epoch： 522\n",
      "epoch： 523\n",
      "epoch： 524\n",
      "epoch： 525\n",
      "epoch： 526\n",
      "epoch： 527\n",
      "epoch： 528\n",
      "epoch： 529\n",
      "epoch： 530\n",
      "epoch： 531\n",
      "epoch： 532\n",
      "epoch： 533\n",
      "epoch： 534\n",
      "epoch： 535\n",
      "epoch： 536\n",
      "epoch： 537\n",
      "epoch： 538\n",
      "epoch： 539\n",
      "epoch： 540\n",
      "epoch： 541\n",
      "epoch： 542\n",
      "epoch： 543\n",
      "epoch： 544\n",
      "epoch： 545\n",
      "epoch： 546\n",
      "epoch： 547\n",
      "epoch： 548\n",
      "epoch： 549\n",
      "epoch： 550\n",
      "epoch： 551\n",
      "epoch： 552\n",
      "epoch： 553\n",
      "epoch： 554\n",
      "epoch： 555\n",
      "epoch： 556\n",
      "epoch： 557\n",
      "epoch： 558\n",
      "epoch： 559\n",
      "epoch： 560\n",
      "epoch： 561\n",
      "epoch： 562\n",
      "epoch： 563\n",
      "epoch： 564\n",
      "epoch： 565\n",
      "epoch： 566\n",
      "epoch： 567\n",
      "epoch： 568\n",
      "epoch： 569\n",
      "epoch： 570\n",
      "epoch： 571\n",
      "epoch： 572\n",
      "epoch： 573\n",
      "epoch： 574\n",
      "epoch： 575\n",
      "epoch： 576\n",
      "epoch： 577\n",
      "epoch： 578\n",
      "epoch： 579\n",
      "epoch： 580\n",
      "epoch： 581\n",
      "epoch： 582\n",
      "epoch： 583\n",
      "epoch： 584\n",
      "epoch： 585\n",
      "epoch： 586\n",
      "epoch： 587\n",
      "epoch： 588\n",
      "epoch： 589\n",
      "epoch： 590\n",
      "epoch： 591\n",
      "epoch： 592\n",
      "epoch： 593\n",
      "epoch： 594\n",
      "epoch： 595\n",
      "epoch： 596\n",
      "epoch： 597\n",
      "epoch： 598\n",
      "epoch： 599\n",
      "epoch： 600\n",
      "epoch： 601\n",
      "epoch： 602\n",
      "epoch： 603\n",
      "epoch： 604\n",
      "epoch： 605\n",
      "epoch： 606\n",
      "epoch： 607\n",
      "epoch： 608\n",
      "epoch： 609\n",
      "epoch： 610\n",
      "epoch： 611\n",
      "epoch： 612\n",
      "epoch： 613\n",
      "epoch： 614\n",
      "epoch： 615\n",
      "epoch： 616\n",
      "epoch： 617\n",
      "epoch： 618\n",
      "epoch： 619\n",
      "epoch： 620\n",
      "epoch： 621\n",
      "epoch： 622\n",
      "epoch： 623\n",
      "epoch： 624\n",
      "epoch： 625\n",
      "epoch： 626\n",
      "epoch： 627\n",
      "epoch： 628\n",
      "epoch： 629\n",
      "epoch： 630\n",
      "epoch： 631\n",
      "epoch： 632\n",
      "epoch： 633\n",
      "epoch： 634\n",
      "epoch： 635\n",
      "epoch： 636\n",
      "epoch： 637\n",
      "epoch： 638\n",
      "epoch： 639\n",
      "epoch： 640\n",
      "epoch： 641\n",
      "epoch： 642\n",
      "epoch： 643\n",
      "epoch： 644\n",
      "epoch： 645\n",
      "epoch： 646\n",
      "epoch： 647\n",
      "epoch： 648\n",
      "epoch： 649\n",
      "epoch： 650\n",
      "epoch： 651\n",
      "epoch： 652\n",
      "epoch： 653\n",
      "epoch： 654\n",
      "epoch： 655\n",
      "epoch： 656\n",
      "epoch： 657\n",
      "epoch： 658\n",
      "epoch： 659\n",
      "epoch： 660\n",
      "epoch： 661\n",
      "epoch： 662\n",
      "epoch： 663\n",
      "epoch： 664\n",
      "epoch： 665\n",
      "epoch： 666\n",
      "epoch： 667\n",
      "epoch： 668\n",
      "epoch： 669\n",
      "epoch： 670\n",
      "epoch： 671\n",
      "epoch： 672\n",
      "epoch： 673\n",
      "epoch： 674\n",
      "epoch： 675\n",
      "epoch： 676\n",
      "epoch： 677\n",
      "epoch： 678\n",
      "epoch： 679\n",
      "epoch： 680\n",
      "epoch： 681\n",
      "epoch： 682\n",
      "epoch： 683\n",
      "epoch： 684\n",
      "epoch： 685\n",
      "epoch： 686\n",
      "epoch： 687\n",
      "epoch： 688\n",
      "epoch： 689\n",
      "epoch： 690\n",
      "epoch： 691\n",
      "epoch： 692\n",
      "epoch： 693\n",
      "epoch： 694\n",
      "epoch： 695\n",
      "epoch： 696\n",
      "epoch： 697\n",
      "epoch： 698\n",
      "epoch： 699\n",
      "epoch： 700\n",
      "epoch： 701\n",
      "epoch： 702\n",
      "epoch： 703\n",
      "epoch： 704\n",
      "epoch： 705\n",
      "epoch： 706\n",
      "epoch： 707\n",
      "epoch： 708\n",
      "epoch： 709\n",
      "epoch： 710\n",
      "epoch： 711\n",
      "epoch： 712\n",
      "epoch： 713\n",
      "epoch： 714\n",
      "epoch： 715\n",
      "epoch： 716\n",
      "epoch： 717\n",
      "epoch： 718\n",
      "epoch： 719\n",
      "epoch： 720\n",
      "epoch： 721\n",
      "epoch： 722\n",
      "epoch： 723\n",
      "epoch： 724\n",
      "epoch： 725\n",
      "epoch： 726\n",
      "epoch： 727\n",
      "epoch： 728\n",
      "epoch： 729\n",
      "epoch： 730\n",
      "epoch： 731\n",
      "epoch： 732\n",
      "epoch： 733\n",
      "epoch： 734\n",
      "epoch： 735\n",
      "epoch： 736\n",
      "epoch： 737\n",
      "epoch： 738\n",
      "epoch： 739\n",
      "epoch： 740\n",
      "epoch： 741\n",
      "epoch： 742\n",
      "epoch： 743\n",
      "epoch： 744\n",
      "epoch： 745\n",
      "epoch： 746\n",
      "epoch： 747\n",
      "epoch： 748\n",
      "epoch： 749\n",
      "epoch： 750\n",
      "epoch： 751\n",
      "epoch： 752\n",
      "epoch： 753\n",
      "epoch： 754\n",
      "epoch： 755\n",
      "epoch： 756\n",
      "epoch： 757\n",
      "epoch： 758\n",
      "epoch： 759\n",
      "epoch： 760\n",
      "epoch： 761\n",
      "epoch： 762\n",
      "epoch： 763\n",
      "epoch： 764\n",
      "epoch： 765\n",
      "epoch： 766\n",
      "epoch： 767\n",
      "epoch： 768\n",
      "epoch： 769\n",
      "epoch： 770\n",
      "epoch： 771\n",
      "epoch： 772\n",
      "epoch： 773\n",
      "epoch： 774\n",
      "epoch： 775\n",
      "epoch： 776\n",
      "epoch： 777\n",
      "epoch： 778\n",
      "epoch： 779\n",
      "epoch： 780\n",
      "epoch： 781\n",
      "epoch： 782\n",
      "epoch： 783\n",
      "epoch： 784\n",
      "epoch： 785\n",
      "epoch： 786\n",
      "epoch： 787\n",
      "epoch： 788\n",
      "epoch： 789\n",
      "epoch： 790\n",
      "epoch： 791\n",
      "epoch： 792\n",
      "epoch： 793\n",
      "epoch： 794\n",
      "epoch： 795\n",
      "epoch： 796\n",
      "epoch： 797\n",
      "epoch： 798\n",
      "epoch： 799\n",
      "epoch： 800\n",
      "epoch： 801\n",
      "epoch： 802\n",
      "epoch： 803\n",
      "epoch： 804\n",
      "epoch： 805\n",
      "epoch： 806\n",
      "epoch： 807\n",
      "epoch： 808\n",
      "epoch： 809\n",
      "epoch： 810\n",
      "epoch： 811\n",
      "epoch： 812\n",
      "epoch： 813\n",
      "epoch： 814\n",
      "epoch： 815\n",
      "epoch： 816\n",
      "epoch： 817\n",
      "epoch： 818\n",
      "epoch： 819\n",
      "epoch： 820\n",
      "epoch： 821\n",
      "epoch： 822\n",
      "epoch： 823\n",
      "epoch： 824\n",
      "epoch： 825\n",
      "epoch： 826\n",
      "epoch： 827\n",
      "epoch： 828\n",
      "epoch： 829\n",
      "epoch： 830\n",
      "epoch： 831\n",
      "epoch： 832\n",
      "epoch： 833\n",
      "epoch： 834\n",
      "epoch： 835\n",
      "epoch： 836\n",
      "epoch： 837\n",
      "epoch： 838\n",
      "epoch： 839\n",
      "epoch： 840\n",
      "epoch： 841\n",
      "epoch： 842\n",
      "epoch： 843\n",
      "epoch： 844\n",
      "epoch： 845\n",
      "epoch： 846\n",
      "epoch： 847\n",
      "epoch： 848\n",
      "epoch： 849\n",
      "epoch： 850\n",
      "epoch： 851\n",
      "epoch： 852\n",
      "epoch： 853\n",
      "epoch： 854\n",
      "epoch： 855\n",
      "epoch： 856\n",
      "epoch： 857\n",
      "epoch： 858\n",
      "epoch： 859\n",
      "epoch： 860\n",
      "epoch： 861\n",
      "epoch： 862\n",
      "epoch： 863\n",
      "epoch： 864\n",
      "epoch： 865\n",
      "epoch： 866\n",
      "epoch： 867\n",
      "epoch： 868\n",
      "epoch： 869\n",
      "epoch： 870\n",
      "epoch： 871\n",
      "epoch： 872\n",
      "epoch： 873\n",
      "epoch： 874\n",
      "epoch： 875\n",
      "epoch： 876\n",
      "epoch： 877\n",
      "epoch： 878\n",
      "epoch： 879\n",
      "epoch： 880\n",
      "epoch： 881\n",
      "epoch： 882\n",
      "epoch： 883\n",
      "epoch： 884\n",
      "epoch： 885\n",
      "epoch： 886\n",
      "epoch： 887\n",
      "epoch： 888\n",
      "epoch： 889\n",
      "epoch： 890\n",
      "epoch： 891\n",
      "epoch： 892\n",
      "epoch： 893\n",
      "epoch： 894\n",
      "epoch： 895\n",
      "epoch： 896\n",
      "epoch： 897\n",
      "epoch： 898\n",
      "epoch： 899\n",
      "epoch： 900\n",
      "epoch： 901\n",
      "epoch： 902\n",
      "epoch： 903\n",
      "epoch： 904\n",
      "epoch： 905\n",
      "epoch： 906\n",
      "epoch： 907\n",
      "epoch： 908\n",
      "epoch： 909\n",
      "epoch： 910\n",
      "epoch： 911\n",
      "epoch： 912\n",
      "epoch： 913\n",
      "epoch： 914\n",
      "epoch： 915\n",
      "epoch： 916\n",
      "epoch： 917\n",
      "epoch： 918\n",
      "epoch： 919\n",
      "epoch： 920\n",
      "epoch： 921\n",
      "epoch： 922\n",
      "epoch： 923\n",
      "epoch： 924\n",
      "epoch： 925\n",
      "epoch： 926\n",
      "epoch： 927\n",
      "epoch： 928\n",
      "epoch： 929\n",
      "epoch： 930\n",
      "epoch： 931\n",
      "epoch： 932\n",
      "epoch： 933\n",
      "epoch： 934\n",
      "epoch： 935\n",
      "epoch： 936\n",
      "epoch： 937\n",
      "epoch： 938\n",
      "epoch： 939\n",
      "epoch： 940\n",
      "epoch： 941\n",
      "epoch： 942\n",
      "epoch： 943\n",
      "epoch： 944\n",
      "epoch： 945\n",
      "epoch： 946\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4504\\3575246681.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m                     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m                     \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'differentiable'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure, grad_scaler)\u001b[0m\n\u001b[0;32m    232\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m             adam(params_with_grad,\n\u001b[0m\u001b[0;32m    235\u001b[0m                  \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m                  \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m     func(params,\n\u001b[0m\u001b[0;32m    301\u001b[0m          \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m          \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tom\\anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction2_sqrt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "from torch import nn\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    def __init__(self,input,output) -> None:\n",
    "        super(MyNet, self).__init__()\n",
    "        self.linear1=nn.Linear(in_features=13,out_features=200)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.linear2=nn.Linear(in_features=200,out_features=500)\n",
    "        self.relu2=nn.ReLU()\n",
    "        self.linear3=nn.Linear(in_features=500,out_features=300)\n",
    "        self.relu3=nn.ReLU()\n",
    "        self.linear4=nn.Linear(in_features=300,out_features=output)\n",
    "    def forward(self,x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.relu1(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.linear3(x)\n",
    "        x=self.relu3(x)\n",
    "        x=self.linear4(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    boston = load_boston()\n",
    "    print(\"载入数据tpye\",type(boston))\n",
    "    #获取数据键\n",
    "    print(boston.keys())\n",
    "    #print(\"数据集描述：\",boston['DESCR'])\n",
    "    print(\"自变量多元数据：\")\n",
    "    print(boston['feature_names'])\n",
    "    print(boston['data'][0:1])\n",
    "    #print(\"因变量：\")\n",
    "    #print(boston['target'])\n",
    "    ################################################################\n",
    "    #数据划分#\n",
    "    x=boston['data']\n",
    "    print(\"自变量数据类型：\",type(x),\"数据形状：\",x.shape)\n",
    "    y=boston['target'].reshape(-1,1)\n",
    "    print(\"因变量数据类型：\",type(y),\"数据形状：\",y.shape)\n",
    "    ################################################################\n",
    "    #数据预处理#\n",
    "    ## 1. 标准化因变量#\n",
    "    scaler=StandardScaler()\n",
    "    x=scaler.fit_transform(x)\n",
    "    ## 2. 数据打乱顺序\n",
    "    x,y=shuffle(x,y)\n",
    "    ## 3. 划分train，test\n",
    "    x_size=x.shape[0]\n",
    "    trian_size=int(x_size*0.6)\n",
    "    x_train,x_test,y_train,y_test=x[0:trian_size,:],x[trian_size:,:],y[0:trian_size,:],y[trian_size:,:]\n",
    "    #数据载入tensor\n",
    "    ## 1.转换numpy到tensor张量\n",
    "    x_train =torch.from_numpy(x_train)\n",
    "    y_train =torch.from_numpy(y_train)\n",
    "    x_test =torch.from_numpy(x_test)\n",
    "    y_test =torch.from_numpy(y_test)\n",
    "    ## 判断cuda是否可用，选择gpu或cpu载入数据处理\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    x_train.to(device)\n",
    "    y_train.to(device)\n",
    "    x_test.to(device)\n",
    "    y_test.to(device)\n",
    "    ################################################################\n",
    "    ## 1. 构建模型实例,注意要载入gpu或cpu中与数据对应,summary输出网络的结构信息\n",
    "    net=MyNet(input=13,output=1).to(device)\n",
    "    summary(net, input_size=(13,))\n",
    "    \n",
    "    ## 2.超参设置\n",
    "    learning_rate=0.02\n",
    "    epochs=10000\n",
    "    batch_size=20\n",
    "    total_steps=int(x_train.shape[0]/batch_size)\n",
    "    print(\"完成一个epoch，需要读取\",total_steps,\"个batchs\")\n",
    "    ## 3.创建损失函数，优化函数实例\n",
    "    loss_fn=nn.MSELoss()\n",
    "    opt=torch.optim.Adam(params=net.parameters(),lr=learning_rate)\n",
    "    ################################################################\n",
    "    ## 4.训练模型\n",
    "    for epoch in range(epochs):#循环10000个epochs\n",
    "        print(\"epoch：\",epoch)\n",
    "        for i in range(total_steps):#内部完成基于batch的一个epochs数据读取\n",
    "            x_train=x_train[i*batch_size:(i+1)*batch_size,:]\n",
    "            y_train=y_train[i*batch_size:(i+1)*batch_size,:]\n",
    "            y_train=y_train.to(torch.float32)\n",
    "            x_train=x_train.to(torch.float32)\n",
    "            y_pred=net.forward(x_train)\n",
    "            loss=loss_fn(y_pred,y_train)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "  \n",
    "     \n",
    "        \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4500649f3376875055274a6ea5762786a730155ceb79c8798b9983f2d85af345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
