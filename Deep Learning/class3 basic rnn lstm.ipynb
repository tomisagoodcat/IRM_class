{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前言\n",
    "\n",
    "RNN 与 LSTM 对比 CNN 更多用于文本问题。所以再学习的时候要结合nlp进行理解。\n",
    "\n",
    "NLP的教学见NLP文件夹下内容\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlp中最关键的一个概念就是上下文，即一个词的含义，同义词，反义词都可以通过上下文得到。\n",
    "\n",
    "为了表现上下文，就出现了很多不同的方法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP背景知识\n",
    "\n",
    "面向深度学习的NLP与面向计数的NLP在基础的词语向量化基础原理存在区别，但基本原理都包括基于上下文的出现概率。\n",
    "\n",
    "在此只考虑基于one-hot表示以及基于tf-idf表示词语的方法\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec\n",
    "\n",
    "word2vec可以视为一种无监督学习方法。\n",
    "\n",
    "本质而言，word2vec也是一个神经网络模型，其结构与之前（面向数值、面向图像）的神经网络模型没有区别，也是分为两个基本步骤\n",
    "\n",
    "\n",
    ">模型训练： 输入值$\\rightarrow$ forward()预测$\\rightarrow$ 预测值 $\\rightarrow$ 最小化 loss(预测值,实际值) $\\rightarrow$ backward()反向传播$\\rightarrow$更新权参、偏参$\\rightarrow$得到优化后模型\n",
    "\n",
    ">模型应用：输入值$\\rightarrow$ 优化后模型 forward()预测$\\rightarrow$预测值\n",
    "\n",
    "不同之处在于输入的值形式不同，输出的值形式不同，总结对比：\n",
    "* 数值（多元函数）：\n",
    "  * 输入: 矩阵 [行实例，列属性]  (不考虑mini-batch情况下 );三阶张量 [mini-batch,行实例，列属性]  \n",
    "  * 输出：单值（全连接层 out=1）\n",
    "* 图像：\n",
    "  * 输入: 4阶张量 [mini-batch,channel,H,W]\n",
    "  * 输出：多值n分类（softmax层 out=n）\n",
    "* 文本：\n",
    "  * 输入: 4阶张量 [mini-batch,channel,H,W]\n",
    "  * 输出：多值n分类（softmax层 out=n）\n",
    "\n",
    "\n",
    "  ----\n",
    "\n",
    "  one-hot可以直接通过sklearn实现，也可以通过自定义包实现。\n",
    "\n",
    "  word2vec实现包很多，在此通过pytroch实现\n",
    "\n",
    "  ----------------------------------------------------------------\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文one-hot，词典中单词来源于原文\n",
      "分词后list转化为集合，去重，并进行排序处理 ['and', 'goodbye', 'hello.', 'i', 'say', 'you']\n",
      "转换后的本地词典： {0: 'and', 1: 'goodbye', 2: 'hello.', 3: 'i', 4: 'say', 5: 'you'}\n",
      "one-hot矩阵大小： (6, 7)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba \n",
    " \n",
    " \n",
    "def token2onehot(words)->pd.DataFrame:\n",
    "    words_set=sorted(set(words))\n",
    "    print(\"分词后list转化为集合，去重，并进行排序处理\",words_set)\n",
    "    diction={}\n",
    "    for index,value in enumerate(words_set):\n",
    "       diction[index]=value\n",
    "    print(\"转换后的本地词典：\",diction)\n",
    "\n",
    "    column=len(words)\n",
    "    row=len(diction)\n",
    "    onehotMatrix=np.zeros((row,column),dtype=float)\n",
    "    print(\"one-hot矩阵大小：\",onehotMatrix.shape)\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(diction)):\n",
    "          if words[i]==diction[j]:\n",
    "              \n",
    "             onehotMatrix[j,i]=1\n",
    "    df=pd.DataFrame(onehotMatrix)\n",
    "    df.columns=words\n",
    "    return(df)\n",
    "if __name__==\"__main__\":\n",
    "    print(\"英文one-hot，词典中单词来源于原文\")\n",
    "    sents=\"you say goodbye and i say hello.\"\n",
    "    words=sents.split()\n",
    "    df=token2onehot(words)\n",
    "    #print(df)\n",
    "    #print(\"中文one-hot，词典中单词来源于原文\")\n",
    "    #sents=\"中国国家统计局15日公布的70个大中城市房价数据显示\"\n",
    "    #words=list(jieba.cut(sents))\n",
    "    #df2=token2onehot(words)\n",
    "    #print(df2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 使用sklearn进行onenote处理\n",
    "\n",
    "[onehot features](https://www.youtube.com/watch?v=NxLfpcfGzns&feature=youtu.be)\n",
    "[onehot sklearn](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/)\n",
    "\n",
    "2. 基于pyotch实现基本的word2vec对照书中通过numpy实现\n",
    "\n",
    "   1. 构建模型\n",
    "   2. 初始化权参\n",
    "   3. onehot代入模型并进行forward\n",
    "\n",
    "3. 初始化权重（随机）实现embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文one-hot，词典中单词来源于原文\n",
      "<class 'list'>\n",
      "[['you']\n",
      " ['say']\n",
      " ['goodbye']\n",
      " ['and']\n",
      " ['i']\n",
      " ['say']\n",
      " ['hello.']]\n",
      "(7, 6)\n",
      "onehot字典位置，以及对应的featurename\n",
      "['x0_and' 'x0_goodbye' 'x0_hello.' 'x0_i' 'x0_say' 'x0_you']\n",
      "得到权参矩阵的形状： (6, 5)\n",
      "得到第一层隐藏层形状： (5,)\n",
      "得到第二层隐藏层形状： (6,)\n",
      "[0.68164513 0.67242392 0.73547645 0.71407802 0.69188172 0.77509685]\n",
      "[0.16162037 0.16013688 0.17055903 0.16694811 0.1632833  0.17745231]\n",
      "[0. 0. 0. 0. 0. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tom\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing  \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np \n",
    "from scipy.special import softmax\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "if __name__=='__main__':\n",
    "    enc = OneHotEncoder(sparse=False)  # 注意创建的转换器对应稀疏矩阵\n",
    "    print(\"英文one-hot，词典中单词来源于原文\")\n",
    "    sents=\"you say goodbye and i say hello.\"\n",
    "    words=sents.split()\n",
    "    print(type(words))\n",
    "    words=np.array(words).reshape(-1,1)#转换为矩阵形式\n",
    "    print(words)\n",
    "    one_hot_word=enc.fit_transform(words)#实现one-hot转换\n",
    "    print(one_hot_word.shape)\n",
    "    print(\"onehot字典位置，以及对应的featurename\")\n",
    "    print(enc.get_feature_names())\n",
    "    #初始化参数实现权重相乘，即只实现forward，不实现backward\n",
    "    word_dim=5#第一层维度\n",
    "    \n",
    "    W1=np.random.rand(len(enc.get_feature_names()),word_dim)\n",
    "    \n",
    "    W2=np.random.rand( word_dim,len(enc.get_feature_names()))\n",
    "    print(f\"得到权参矩阵的形状：\",W1.shape)\n",
    "    #抽取其中第一个单词\n",
    "    firt_word= one_hot_word[0,:]\n",
    "    embedding=firt_word@W1\n",
    "    s1=sigmoid(embedding)#激活层\n",
    "    embedding2=embedding@W2#最后\n",
    "    s2=sigmoid(embedding2)#激活层\n",
    "    print(\"得到第一层隐藏层形状：\",embedding.shape)\n",
    "    print(\"得到第二层隐藏层形状：\",embedding2.shape)\n",
    "    print(s2)\n",
    "    out=softmax(s2)\n",
    "    print(out)\n",
    "    #返回对应的\n",
    "    max_index = np.argmax(out)\n",
    "    one_hot = np.zeros_like(out)\n",
    "    one_hot[max_index] = 1\n",
    "    print(one_hot)\n",
    "  \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到句子中每一个词的onehot表示，可以通过其代入预训练模型，推理出词上下文（词）。\n",
    "\n",
    "1. 输入一个$n$长度的onehot向量，$n$为句子长度，每个向量单位代表一个词\n",
    "2. 经过深度学习，一般是全连接层到softmax层\n",
    "3. 得到同样的$n$长度输出向量，每个向量的值代表对应词是输入词上下文的概率\n",
    "4. 通过max得到最大概率的词，推理完成\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"figs\\word embedding 1.jpg\" height=\"50%\" width=\"50%\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如图所示，word2vec基本的结构与一般神经网络没有太大区别，\n",
    "\n",
    "假设一句话句子长度为$l$，该句话对应的词典（lexcion）长度为$n$，lexcion去除了句子中重复的词，并可能根据字母排序。\n",
    "\n",
    "通过one-hot处理，该句话每个词对应一个长度为$l$向量，在词典出现位置上标识1，没有出现位置标识0。\n",
    "\n",
    "整个句子就可以用$m \\times l$矩阵表示。\n",
    "\n",
    "---\n",
    "\n",
    "此时目标预测一个词（在句子位置为$w_i$)后面一个词（位置为$w_{i-1}$）\n",
    "\n",
    "假设此时神经网络已经经过训练，得到了优化的权参，第一层全连接层\n",
    "\n",
    "1. one-hot 输入向量$A_{1 \\times n}$\n",
    "2. 全连接层中，权参矩阵 $W_{n \\times m}$ \n",
    "3. 得到中间层结果向量$Z_{ 1 \\times m}=A_{1 \\times n} \\cdot W_{n \\times m}$即embedding层\n",
    "4. 最后输出层softmax层输出为$l$长度向量，每个向量对应lexcion，\n",
    "\n",
    "---\n",
    "\n",
    "The shapes of the layers in Word2Vec depend on the vocabulary size and the size of the hidden layer. Assuming a vocabulary size of V and a hidden layer size of N, the shapes of the layers in both the Continuous Bag of Words (CBOW) and Skip-gram models are as follows:\n",
    "\n",
    "Input layer: The input layer is a one-hot encoded vector representing a word in the vocabulary. Its shape is (V, 1), where V is the size of the vocabulary.\n",
    "\n",
    "Hidden layer: The hidden layer contains the word embeddings, which are the numerical representations of the input words. Its shape is (N, 1), where N is the size of the hidden layer.\n",
    "\n",
    "Output layer: The output layer is a softmax function that produces the probability distribution of the words in the vocabulary given the context words or the target word. Its shape is (V, 1), where V is the size of the vocabulary.\n",
    "\n",
    "During training, the weights of the neural network are adjusted using backpropagation to minimize the negative log-likelihood loss function of the output layer. The weight matrix connecting the input layer to the hidden layer has a shape of (N, V), and the weight matrix connecting the hidden layer to the output layer has a shape of (V, N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'over': 0, 'jumped': 1, 'fox': 2, 'the': 3, 'quick': 4, 'dog': 5, 'lazy': 6, 'brown': 7}\n",
      "tensor([3, 4, 7, 2, 1, 0, 3, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "# define training data\n",
    "import torch\n",
    "corpus = [\n",
    "    'the quick brown fox',\n",
    "    'jumped over the lazy dog'\n",
    "]\n",
    "tokens = []\n",
    "for sentence in corpus:\n",
    "    tokens.extend(sentence.split())\n",
    "word2idx = {w: i for i, w in enumerate(set(tokens))}\n",
    "idx2word = {i: w for w, i in word2idx.items()}\n",
    "data = torch.tensor([word2idx[w] for w in tokens], dtype=torch.long)\n",
    "print(word2idx)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Word2Vec model, the size of the input vector is typically equal to the size of the vocabulary, which is the total number of unique words in the corpus. Each word in the vocabulary is assigned a unique index, and the input vector is a one-hot encoded vector of size vocab_size, where the value at the index corresponding to the current word is 1 and all other values are 0.\n",
    "\n",
    "For example, if the vocabulary contains 10,000 unique words, the input vector for a given word would be a one-hot encoded vector of size 10,000. However, in practice, the one-hot encoding of the input vector can be quite large and sparse, making it computationally expensive to train the model. Therefore, techniques like subsampling and negative sampling are used to reduce the size of the input vector and improve training efficiency, while still preserving the quality of the word embeddings.\n",
    "In the Word2Vec model, the parameters are the word embeddings and the weights of the linear layer that are learned during training. The shape of the parameters depends on the vocabulary size and the embedding dimension.\n",
    "\n",
    "Specifically, the Word2Vec model learns a matrix of word embeddings, where each row corresponds to the embedding of a single word in the vocabulary. If the vocabulary size is vocab_size and the embedding dimension is embedding_dim, then the shape of the embedding matrix is (vocab_size, embedding_dim).\n",
    "\n",
    "Additionally, the Word2Vec model also learns a weight matrix that maps the embedded center word to a predicted output distribution over all the words in the vocabulary. If the vocabulary size is vocab_size and the embedding dimension is embedding_dim, then the shape of the weight matrix is (embedding_dim, vocab_size).\n",
    "\n",
    "During training, these parameters are updated using backpropagation to minimize the loss between the predicted output distribution and the true target distribution. The optimized parameters are then used to obtain the final word embeddings that capture the semantic and syntactic relationships between words in the vocabulary.\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the Word2Vec model\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, center_word):\n",
    "        center_embed = self.embedding(center_word)\n",
    "        center_hidden = center_embed.sum(dim=1)\n",
    "        target_score = self.linear(center_hidden)\n",
    "        return target_score\n",
    "\n",
    "# Define the Word2Vec training function\n",
    "def train_word2vec(corpus, embedding_dim, window_size, batch_size, learning_rate, num_epochs):\n",
    "    # Build the vocabulary\n",
    "    vocab = list(set(corpus))\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Prepare the training data\n",
    "    data = []\n",
    "    for i in range(len(corpus)):\n",
    "        center_word = corpus[i]\n",
    "        for j in range(1, window_size + 1):\n",
    "            if i - j >= 0:\n",
    "                target_word = corpus[i - j]\n",
    "                data.append((center_word, target_word))\n",
    "            if i + j < len(corpus):\n",
    "                target_word = corpus[i + j]\n",
    "                data.append((center_word, target_word))\n",
    "\n",
    "    # Define the model, loss function, and optimizer\n",
    "    model = Word2Vec(vocab_size, embedding_dim)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i+batch_size]\n",
    "            center_word_batch = [word2idx[w[0]] for w in batch]\n",
    "            target_word_batch = [word2idx[w[1]] for w in batch]\n",
    "            center_word_batch = torch.tensor(center_word_batch).unsqueeze(1)\n",
    "            target_word_batch = torch.tensor(target_word_batch)\n",
    "            target_score = model(center_word_batch)\n",
    "            loss = criterion(target_score, target_word_batch)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, total_loss/len(data)))\n",
    "\n",
    "    # Return the trained model\n",
    "    return model, word2idx, idx2word\n",
    "\n",
    "# Example usage\n",
    "corpus = ['this', 'is', 'a', 'test', 'sentence', 'for', 'word2vec']\n",
    "embedding_dim = 50\n",
    "window_size = 2\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "model, word2idx, idx2word = train_word2vec(corpus, embedding_dim, window_size, batch_size, learning_rate, num_epochs)\n",
    "\n",
    "# Get the word embeddings for a specific word\n",
    "word = 'word2vec'\n",
    "word_idx = word2idx[word]\n",
    "word_embed = model.embedding(torch.tensor([word_idx]))\n",
    "print('Embedding for \"{}\": {}'.format(word, word_embed.squeeze().detach().numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we define a small vocabulary consisting of four words and convert each word to a one-hot vector. We create an embedding matrix with a dimension of 2 and random initial values. We define a simple neural network with one hidden layer to predict whether each word in the sentence is positive or negative. We train the model using mean squared error loss and gradient descent, and finally extract the learned word embeddings from the embedding matrix by multiplying each one-hot vector with the embedding matrix. We use the sigmoid activation function for the hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 4)\n",
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "vocabulary = {'hello': 0, 'world': 1, 'goodbye': 2, 'cruel': 3}\n",
    "one_hot = np.eye(len(vocabulary))\n",
    "inputs = np.array([one_hot[vocabulary[word]] for word in ['hello', 'world', 'hello', 'goodbye', 'cruel', 'world']])\n",
    "print(inputs.shape)\n",
    "inputs = inputs.reshape(-1, embedding_dim * len(vocabulary))\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (3,1) (6,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16588\\4160737813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[1;31m# Backward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0md_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[0md_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0md_W2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,1) (6,1) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the vocabulary and convert words to one-hot vectors\n",
    "vocabulary = {'hello': 0, 'world': 1, 'goodbye': 2, 'cruel': 3}\n",
    "one_hot = np.eye(len(vocabulary))\n",
    "\n",
    "# Define the embedding dimension and create the embedding matrix\n",
    "embedding_dim = 2\n",
    "embedding_matrix = np.random.randn(len(vocabulary), embedding_dim)\n",
    "\n",
    "# Define the model architecture\n",
    "hidden_dim = 5\n",
    "W1 = np.random.randn(embedding_dim * len(vocabulary), hidden_dim)\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, 1)\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "# Define the activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Train the model\n",
    "learning_rate = 0.1\n",
    "for epoch in range(100):\n",
    "    inputs = np.array([one_hot[vocabulary[word]] for word in ['hello', 'world', 'hello', 'goodbye', 'cruel', 'world']])\n",
    "    inputs = inputs.reshape(-1, embedding_dim * len(vocabulary))\n",
    "    labels = np.array([1, 0, 1, 0, 1, 0]).reshape(-1, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    hidden = sigmoid(np.dot(inputs, W1) + b1)\n",
    "    outputs = sigmoid(np.dot(hidden, W2) + b2)\n",
    "\n",
    "    # Backward pass\n",
    "    d_outputs = (outputs - labels) * outputs * (1 - outputs)\n",
    "    d_hidden = np.dot(d_outputs, W2.T) * hidden * (1 - hidden)\n",
    "    d_W2 = np.dot(hidden.T, d_outputs)\n",
    "    d_b2 = np.sum(d_outputs, axis=0, keepdims=True)\n",
    "    d_W1 = np.dot(inputs.T, d_hidden)\n",
    "    d_b1 = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    # Update the parameters\n",
    "    W2 -= learning_rate * d_W2\n",
    "    b2 -= learning_rate * d_b2\n",
    "    W1 -= learning_rate * d_W1\n",
    "    b1 -= learning_rate * d_b1\n",
    "\n",
    "    if epoch % 10 == 9:\n",
    "        loss = np.mean(np.square(outputs - labels))\n",
    "        print(f'Epoch {epoch+1}, loss: {loss}')\n",
    "\n",
    "# Get the word embeddings\n",
    "word_embeddings = embedding_matrix\n",
    "\n",
    "# Print the word embeddings\n",
    "for word, index in vocabulary.items():\n",
    "    one_hot_vector = one_hot[index]\n",
    "    embedding = np.dot(one_hot_vector, word_embeddings)\n",
    "    print(f'{word}: {embedding}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4500649f3376875055274a6ea5762786a730155ceb79c8798b9983f2d85af345"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
