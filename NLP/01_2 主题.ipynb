{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题向量\n",
    "\n",
    "通过隐形语义分析（latent senmantic analysis LSA）可以不仅仅把词的意义表示为向量，还可以用向量来表示通篇文档的意义。\n",
    "\n",
    "本章将学习这些语义或主题向量，通过TF-IDF向量的加权得分来计算所谓的主题得分，而将这些得分构成了主题向量的各个维度。\n",
    "\n",
    "将使用归一化词频直接的关联来将词归并到同意主题，每个归并结果定义了新主题向量的一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得到虚拟的每个词的tf-idf值\n",
      "{'petness': 0.3801465633219733, 'animalness': 0.4797383032984889, 'cityness': -0.2865590942806303}\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "topic={}\n",
    "random.seed(1)\n",
    "tfidf= dict(list(zip('cat dog apple lion NYC love'.split(),np.random.rand(6))))\n",
    "\n",
    "print(\"得到虚拟的每个词的tf-idf值\")\n",
    "tfidf\n",
    "topic['petness']=(0.3*tfidf['cat']+0.3*tfidf['dog']+0*tfidf['apple']+0*tfidf['lion']-0.2*tfidf['NYC']+0.2*tfidf['love'])\n",
    "topic['animalness']=(0.1*tfidf['cat']+0.1*tfidf['dog']-0.1*tfidf['apple']+0.5*tfidf['lion']+0.1*tfidf['NYC']-0.1*tfidf['love'])\n",
    "topic['cityness']=(0*tfidf['cat']-0.1*tfidf['dog']+0.2*tfidf['apple']-0.1*tfidf['lion']-0.5*tfidf['NYC']+0.1*tfidf['love'])\n",
    "print(topic)\n",
    "# 构建相应矩阵 \n",
    "topic_m=np.zeros(shape=(3,6))\n",
    "print(topic_m)\n",
    "word_tf=np.zeros(shape=(6,1))\n",
    "print(word_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LDA \n",
    "LDA 分类器是一种有监督算法，因此需要对文本进行标注，但是其需要训练的样本数相对较少。\n",
    "\n",
    "LDA是一维模型，所以其不需要SVD，可以只计算二类问题（如垃圾和非垃圾）问题中的每一类的所有TF-IDF向量的质心（平均值）。推导就变成了这两个质心之间的直线，TF-IDF向量与这条直线越近（TF-IDF向量与这两条直线的点积）就表示它与其中一个类接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理训练数据：...\n",
      "\n",
      "(5161, 200)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6447955  0.         0.\n",
      " 0.47170963 0.         0.         0.         0.35643069 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.40252561 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2695535  0.        ]\n",
      "得到(5161,)矩阵\n",
      "垃圾短信平均向量： (200,)\n",
      "短信平均向量： (200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "from cgitb import handler\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "print(\"处理训练数据：...\\n\")\n",
    "train_txt = pd.read_table('sms/train.txt',sep='\\t',header=None)  \n",
    "train_txt.columns = ['label', 'text']\n",
    "label_map = {'ham': 0, 'spam': 1 }#1为垃圾短信\n",
    "train_txt['label'] = train_txt['label'].map(label_map)\n",
    "\n",
    "#train_txt = pd.get_dummies(train_txt, columns=['label'])# 将标签onehot编码\n",
    "\n",
    "def pre_clean_text(origin_text):\n",
    "    # 去掉标点符号和非法字符\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", origin_text)\n",
    "    # 将字符全部转化为小写，并通过空格符进行分词处理\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # 将剩下的词还原成str类型\n",
    "    cleaned_text = \" \".join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    #清理数据\n",
    "    train_txt['text'] = train_txt['text'].apply(lambda x: pre_clean_text(x))\n",
    "\n",
    "    #删去空值.测试时若无效词删去后为空则直接为垃圾信息(实际测试中没有)\n",
    "    #print(train_txt.shape)\n",
    "    train_txt = train_txt.loc[train_txt['text'] != '',:]\n",
    "    # 查看数据\n",
    "     \n",
    "    #print(train_txt.shape)\n",
    "    #实现tf-id数据向量化\n",
    "    \n",
    "    tfidf = TfidfVectorizer (\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=200)\n",
    "    word_vict=tfidf.fit_transform(train_txt['text']).toarray()\n",
    "    print(word_vict.shape)\n",
    "    print(word_vict[20,:])\n",
    "    \n",
    "    mask=np.array(train_txt['label'].astype(bool))\n",
    "    print(\"得到{}矩阵\".format(mask.shape))\n",
    "    spam_centroid=word_vict[mask].mean(axis=0).round(2)#axis=0 计算列平均值\n",
    "    print(\"垃圾短信平均向量：\",spam_centroid.shape)\n",
    "    ham_centroid=word_vict[mask].mean(axis=0).round(2)\n",
    " \n",
    "    print(\"短信平均向量：\",ham_centroid.shape)\n",
    "    sh=spam_centroid-ham_centroid\n",
    "    print(sh.shape)\n",
    "    spamsocre=word_vict@sh \n",
    "   \n",
    "    spamscore2=word_vict@ham_centroid\n",
    "     \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    spam1=MinMaxScaler().fit_transform(spamsocre.reshape(-1,1))#reshape(-1,1)转换成1列：\n",
    " \n",
    "    spam2=MinMaxScaler().fit_transform(spamscore2.reshape(-1,1))\n",
    "  \n",
    "    train_txt['lda_score']=spam1\n",
    "    train_txt['lda_pred']=(train_txt['lda_score']>0.2).astype(int)\n",
    "    train_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐性语义分析(LSA,Latent Semantic Analysis)\n",
    "> 参考\n",
    "> * https://zhuanlan.zhihu.com/p/46376672\n",
    "> * https://zhuanlan.zhihu.com/p/144367432\n",
    "\n",
    "LSA的底层是SVD技术，利用SVD将TF_IDF矩阵分解3个矩阵，而后根据其方差贡献率（信息载荷）进行降维，当在NLL中使用SVD时，将其称为隐性语义分析（LSA），\n",
    "\n",
    "LSA揭示了被隐含并等待被发现的词的语义或意义。\n",
    "\n",
    "LSA是一种属性技术，用于寻找对任意一组NLP向量进行最佳线性变换（旋转和拉伸）的方法，这些NLP向量包括TF-IDF向量或词袋向量。对许多应用而言，最好的变换方法是将\n",
    "\n",
    "坐标轴（维度）对齐到新向量中，使得其在词频上具有最大方差。然后可以在新向量空间中去掉哪些对不同文档向量贡献不大的维度。\n",
    "\n",
    "<font color='red'>LSA 中单词-文本-svd关系</font>\n",
    "<img src=\"https://pic2.zhimg.com/v2-b3eb29a45d1fc11f57b858fd5af7571d_r.jpg\"/>\n",
    "> LSA 步骤\n",
    "1. 构建TF-IDF或其他文档-词矩阵向量,行为文档(doc)，列为词(term)\n",
    "<img src=\"https://pic4.zhimg.com/80/v2-288292d4fd98b748c4b5e37786c06bd3_720w.jpg\"/>\n",
    "2. 对矩阵向量进行SVD分解\n",
    "3. 根据主题数目，或方差贡献率累计比，选择降维数目\n",
    "4. 对原有矩阵进行降维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting jieba\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     -------------------------------------- 19.2/19.2 MB 361.1 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314459 sha256=1120f87868820b5f1c4d22cd9bf9a07c0f8622ce0e42b2a1f7ffbb599c2251b8\n",
      "  Stored in directory: c:\\users\\tomis\\appdata\\local\\pip\\cache\\wheels\\f3\\30\\86\\64b88bf0241f0132806c61b1e2686b44f1327bfc5642f9d77d\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tf\n",
    "import numpy as np \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import jieba \n",
    "\n",
    "def loadData():\n",
    "    '''实现载入sklearn中的“20 Newsgroup”数据\n",
    "    '''\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "    return pd.DataFrame(dataset.data)\n",
    "\n",
    " \n",
    "def clearData(df:pd.DataFrame):\n",
    "    \"\"\"开始之前，我们先尝试着清理文本数据。主要思想就是清除其中的标点、数字和特殊字符。之后，我们需要删除较短的单词，因为通常它们不会包含什么有用的信息。最后，我们将文本变为不区分大小写。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _原始文本_\n",
    "    \"\"\"\n",
    "# removing everything except alphabets`\n",
    "    \n",
    "    df['clean_doc'] = df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "    df['clean_doc'] = df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# make all text lowercase\n",
    "    df['clean_doc'] = df['clean_doc'].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def cutChineseWord(data,stopwordfile):\n",
    "  \n",
    " \n",
    "    stopwords = [line.strip() for line in open(stopwordfile, 'r',encoding='utf-8').readlines()]\n",
    "     ## 首先实现切词\n",
    "    for i in range(len(data)):\n",
    "        doc=data.iloc[i-1,0]\n",
    "        seq_list=jieba.cut(doc,use_paddle=True)\n",
    "        token=[]\n",
    "    \n",
    "        for seq in seq_list:\n",
    "            if seq not in stopwords:\n",
    "                token.append(seq)\n",
    "        \n",
    "        data.at[i,'words']=list(token)  # 每次在words列第i个位置插入对象list\n",
    "    return data\n",
    " \n",
    "\n",
    "def stopWords(df:pd.DataFrame):\n",
    "    \"\"\"之后我们要删除没有特别意义的停止词，例如“it”、“they”、“am”、“been”、“about”、“because”、“while”等等。为了实现这一目的，我们要对文本进行标记化，也就是将一串文本分割成独立的标记或单词。删除停止词之后，再把这些标记组合在一起。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _description_\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    # tokenization分词\n",
    "    tokenized_doc = df['clean_doc'].apply(lambda x: x.split())\n",
    "    # remove stop-words\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# de-tokenization\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    df['clean_doc'] = detokenized_doc\n",
    "    return df  \n",
    "\n",
    "def vec_words(document:pd.DataFrame,max_feature:int=200)->np.array:\n",
    "    \"\"\"基于sklearn实现文档向量化，其中元素为tf-idf，注意TfidfVectorizer参数\n",
    "\n",
    "    Args:\n",
    "        document (pd.DataFrame): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tfidf =tf(analyzer=\"word\",tokenizer=None,max_features=max_feature)\n",
    "    tfidf.fit(document)\n",
    "    lexcion=tfidf.vocabulary_ # 返回向量化对应的词典（字典形式）\n",
    "    lexcion_len=tfidf.vocabulary_.__len__()  # 返回向量化对应的词典长度\n",
    "    vec=tfidf.fit_transform(document).toarray()\n",
    "    df=pd.DataFrame(vec,columns=list(lexcion.keys()))\n",
    "    return vec,lexcion,lexcion_len,df,tfidf\n",
    "\n",
    "def svd(vec:np.array):\n",
    "    \"\"\"基于np中线性代数函数实现svd，注意np.linalg.svd（）返回的特征值矩阵是一个向量而非矩阵，这是由于方法自动缩减了特征值\n",
    "    矩阵中0值，k的大小为A(m*n)中最小的值，为此本方法将特征值矩阵还原为m*n大小的矩阵其中k个特征值构成其对角值。\n",
    "\n",
    "    Args:\n",
    "        vec (np.array): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "     \n",
    "     \n",
    "    u,sigma,vt=np.linalg.svd(vec)\n",
    "    \n",
    "    Sigma=np.zeros(np.shape(vec))\n",
    "    len_ar=len(sigma)\n",
    "    Sigma[:len_ar,:len_ar]=np.diag(sigma)\n",
    "    return u,Sigma,vt,sigma\n",
    "\n",
    "\n",
    "def RightdemsionRe(A:np.array,u:np.array,Sigma:np.array,k:int)->np.array:\n",
    "    \"\"\"_实现基于SVD的文本矩阵降维,由于是进行主题降维度、所以采用左奇异矩阵将维，实现词的降维_\n",
    "        SVD 中左（行）奇异矩阵降维 为 B=Ut（k）*A  其中Ut为U左奇异矩阵转置，k为转置后保留的行数\n",
    "\n",
    "    Args:\n",
    "        A(np.array):_原文档-词矩阵_\n",
    "        u (np.array): _左奇异矩阵_\n",
    "        Sigma (np.array): _奇异值矩阵_\n",
    "        vt (np.array): _右奇异矩阵_\n",
    "        n (int): _降维数目（话题数）_\n",
    "\n",
    "    Returns:\n",
    "        np.array: _description_\n",
    "    \"\"\"\n",
    "    B=u.T[:k,:]@A\n",
    "    #B=u[:,:n]@Sigma[:n,:n]@vt[:n,:]\n",
    "    \n",
    "    return B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转为Data数据观察\n",
      "                                                   0\n",
      "0  Well i'm not sure about the story nad it did s...\n",
      "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
      "2  Although I realize that principle is not one o...\n",
      "            0\n",
      "count   11314\n",
      "unique  10994\n",
      "top          \n",
      "freq      218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomis\\AppData\\Local\\Temp\\ipykernel_24216\\3207423743.py:24: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_doc'] = df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#clear data\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m documents\u001b[39m.\u001b[39mcolumns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdocument\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m documents\u001b[39m=\u001b[39m clearData(documents)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(documents\u001b[39m.\u001b[39mhead(\u001b[39m3\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# tf-idf 向量化 \u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 12\u001b[0m in \u001b[0;36mclearData\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m\"\"\"开始之前，我们先尝试着清理文本数据。主要思想就是清除其中的标点、数字和特殊字符。之后，我们需要删除较短的单词，因为通常它们不会包含什么有用的信息。最后，我们将文本变为不区分大小写。\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m        df (pd.DataFrame): _原始文本_\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# removing everything except alphabets`\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mclean_doc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mdocument\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m[^a-zA-Z#]\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# removing short words\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X20sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     df[\u001b[39m'\u001b[39m\u001b[39mclean_doc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mclean_doc\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([w \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39msplit() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(w)\u001b[39m>\u001b[39m\u001b[39m3\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:130\u001b[0m, in \u001b[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    126\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot use .str.\u001b[39m\u001b[39m{\u001b[39;00mfunc_name\u001b[39m}\u001b[39;00m\u001b[39m with values of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    127\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minferred dtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m     )\n\u001b[0;32m    129\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 130\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\strings\\accessor.py:1505\u001b[0m, in \u001b[0;36mStringMethods.replace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m case \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     case \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m-> 1505\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data\u001b[39m.\u001b[39;49marray\u001b[39m.\u001b[39;49m_str_replace(\n\u001b[0;32m   1506\u001b[0m     pat, repl, n\u001b[39m=\u001b[39;49mn, case\u001b[39m=\u001b[39;49mcase, flags\u001b[39m=\u001b[39;49mflags, regex\u001b[39m=\u001b[39;49mregex\n\u001b[0;32m   1507\u001b[0m )\n\u001b[0;32m   1508\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_result(result)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:165\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_replace\u001b[1;34m(self, pat, repl, n, case, flags, regex)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(pat, repl, n)\n\u001b[1;32m--> 165\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_str_map(f, dtype\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:71\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_map\u001b[1;34m(self, f, na_value, dtype, convert)\u001b[0m\n\u001b[0;32m     69\u001b[0m map_convert \u001b[39m=\u001b[39m convert \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(mask)\n\u001b[0;32m     70\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     result \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer_mask(arr, f, mask\u001b[39m.\u001b[39;49mview(np\u001b[39m.\u001b[39;49muint8), map_convert)\n\u001b[0;32m     72\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mAttributeError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m     73\u001b[0m     \u001b[39m# Reraise the exception if callable `f` got wrong number of args.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m     \u001b[39m# The user may want to be warned by this, instead of getting NaN\u001b[39;00m\n\u001b[0;32m     75\u001b[0m     p_err \u001b[39m=\u001b[39m (\n\u001b[0;32m     76\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m((takes)|(missing)) (?(2)from \u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ to )?\u001b[39m\u001b[39m\\\u001b[39m\u001b[39md+ \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(?(3)required )positional arguments?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     78\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2871\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer_mask\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\strings\\object_array.py:161\u001b[0m, in \u001b[0;36mObjectStringArrayMixin._str_replace.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    158\u001b[0m         pat \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(pat, flags\u001b[39m=\u001b[39mflags)\n\u001b[0;32m    160\u001b[0m     n \u001b[39m=\u001b[39m n \u001b[39mif\u001b[39;00m n \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 161\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: pat\u001b[39m.\u001b[39msub(repl\u001b[39m=\u001b[39mrepl, string\u001b[39m=\u001b[39mx, count\u001b[39m=\u001b[39mn)\n\u001b[0;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    163\u001b[0m     f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(pat, repl, n)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print(\"转为Data数据观察\")\n",
    "    documents=loadData()\n",
    "    print(documents.head(3))\n",
    "    print(documents.describe())\n",
    "    #clear data\n",
    "    documents.columns=['document']\n",
    "    documents= clearData(documents)\n",
    "    print(documents.head(3))\n",
    "    # tf-idf 向量化 \n",
    "    vec,lexcion,lexcion_len,df,tfidf=vec_words(documents['clean_doc'])\n",
    "    print(type(lexcion))\n",
    "    print(\"得到向量化矩阵形状(行：文本，列：词）：{},词典个数：{}\".format(vec.shape,lexcion_len))\n",
    "    vect=vec.T# 对词向量转置\n",
    "    print(\"得到向量化矩阵形状(行：词，列：文本）：{},词典个数：{}\".format(vect.shape,lexcion_len))\n",
    "    print(\"词典 \\n\",lexcion)\n",
    "    #print(\"向量化后文本(行：文本，列：词\\n \",vec[:8,:])\n",
    "    print(\"向量化后的文本矩阵(行：文本，列：词）：\",df)\n",
    "    print(\"在进行svd之前，由于我们的最终目标是进行主题降维，使用np.linga.svd构建u，sigma,vt时候 \\n u对应的行向量就应该是其词（最初主题），故应该先对文本向量进行转置，输入（行：词，列：文本）矩阵，\\n 在基础上进行左奇异矩阵降维\")\n",
    "\n",
    "    print(\"由于输入的向量化文本为行：text，列：word，但np.linalg.svd对应的原始矩阵为，行：word，列：text故需要先转置\")\n",
    "    u,Sigma,vt,sigma=svd(vect)\n",
    "    print('sigma:',sigma.shape)\n",
    "    print(\"svg后的左奇异矩阵大小:{},奇异值矩阵大小:{},右奇异矩阵大小：{},奇异矩阵中对角线奇异值数量:{}\".format(u.shape,Sigma.shape,vt.shape,sigma.shape))\n",
    "    B=RightdemsionRe(vect,u,Sigma,20)\n",
    "    print(\"降维后数据大小：\",B.shape)\n",
    "    print(B[:,15])\n",
    "    \n",
    "    ### 通过sklearn中TruncatedSVD函数实现svd，但注意，其输入向量矩阵为（行：文本，列：词）\n",
    "    svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "    sk_vec=svd_model.fit(vec)\n",
    "    print(\"使用sklearn中TruncatedSVD进行降维，注意矩阵形态\")\n",
    "    print(len(svd_model.components_))\n",
    "    \n",
    "    '''\n",
    "    svd_model中的元素就是我们的主题，\n",
    "    我们可以用svdmodel.components查看。最后，在20个主题中输入几个重要单词，看模型会做出什么反应。\n",
    "    '''\n",
    "    terms=tfidf.get_feature_names()\n",
    "    print(terms)\n",
    "    print(\"#############################\")\n",
    "    print(lexcion)\n",
    "    \n",
    "    # 查看主题中信息\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openpyxl\n",
      "  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/7b/60/9afac4fd6feee0ac09339de4101ee452ea643d26e9ce44c7708a0023f503/openpyxl-3.0.10-py2.py3-none-any.whl (242 kB)\n",
      "Collecting et-xmlfile\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/96/c2/3dd434b0108730014f1b96fd286040dc3bcb70066346f7e01ec2ac95865f/et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.0.10\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     斗篷山谷江村八组河边有电线漏电马被电死的问题，公安部门已前往现场，请你部门立即与群众联系核实...\n",
      "1     本人罗国喜在306监狱跟黔南州建开塔吊到现在得不到工资，请你部门立即与群众联系核实，并反馈完...\n",
      "2     群众称：小围寨河阳乡蒲林村6组占用其土地未得到赔偿款，要强挖土地，请你部门立即与报警人联系前...\n",
      "3     群众反映称：在都匀市平桥北路中房停水，请你部门立即与群众联系核实，并反馈完整的处理结果。（供...\n",
      "4      群众称： 在东山隧道路口的住户供水不正常问题，请你部门立即与群众联系核实，并反馈完整的处理结果。\n",
      "5      报警人称：在小围寨剑中社区水管被烧断漏水的问题，请你部门立即与群众联系核实，并反馈完整的处理结果\n",
      "6     群众称：在老师院附中对面工地噪音扰民，请你部门立即与群众联系核实，并反馈完整的处理结果。_x...\n",
      "7              群众称在三中旁边工地噪音扰民，请你部门立即与群众联系核实，并反馈完整的处理结果。\n",
      "8     在马鞍山安置房A区楼上亮丽工程灯一直不关影响居民休息的问题，请你部门立即与群众联系核实，并反...\n",
      "9     群众称：在黔南电视台门口井盖被大货车压碎了存在安全隐患。请你部门立即与群众联系核实，并反馈完...\n",
      "10    黄女士称：开发区龙彩花园小区水管爆裂几天了，一直往外流水，很浪费，之前工作人员去看过，但是一...\n",
      "11    报警人称：从夜市街到啤酒厂手机遗失在出租车上，已报警调监控。请你部门立即与群众联系核实，并反...\n",
      "12        群众称：在裤裆街商家占道经营导致交通堵塞的问题，请你部门立即前往处置并反馈完整的处理结果。\n",
      "13      报警人称：老火车站环球中心工地施工噪音扰民，请你部门立即与群众联系核实，并反馈完整的处理结果。\n",
      "14    报警人称打车从大十字到庆云宫，手机遗失在出租车上，记不得车牌，已转接都匀公安110.请你部门...\n",
      "15       群众称：在东来尚城桥梁厂有人烧垃圾味道重，请你部门立即与群众联系核实，并反馈完整的处理结果。\n",
      "16           民警称：在都匀一号停车收费的问题，请你部门立即与民警联系核实，并反馈完整的处理结果。\n",
      "17    群众反映称：在都匀市南洲国际马鞍山小区工地噪音扰民，中午也施工，晚上也施工严重影响孩子学习，...\n",
      "18    群众称：在都匀市城南新家园1栋十单元402号被物业停水，请你部门立即与群众联系核实，并反馈完...\n",
      "19    报警人称： 在义乌批发商场干活，老板开除其员工，还不发工资，请你部门立即与报警人联系核实，并...\n",
      "Name: workorder_content, dtype: object\n",
      "缺失行数： 0\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Too many indexers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# 数据EDA\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m缺失行数：\u001b[39m\u001b[39m\"\u001b[39m,data\u001b[39m.\u001b[39misnull()\u001b[39m.\u001b[39msum())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m data_clear\u001b[39m=\u001b[39mcutChineseWord(data,stopwordfile\u001b[39m=\u001b[39;49mstopwordfile)\n",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 14\u001b[0m in \u001b[0;36mcutChineseWord\u001b[1;34m(data, stopwordfile)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m  \u001b[39m## 首先实现切词\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(data)):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     doc\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39;49miloc[i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     seq_list\u001b[39m=\u001b[39mjieba\u001b[39m.\u001b[39mcut(doc,use_paddle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     token\u001b[39m=\u001b[39m[]\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\indexing.py:1068\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[0;32m   1067\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[1;32m-> 1068\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[0;32m   1069\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1070\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[0;32m   1071\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\indexing.py:1564\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_tuple\u001b[1;34m(self, tup)\u001b[0m\n\u001b[0;32m   1562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_getitem_tuple\u001b[39m(\u001b[39mself\u001b[39m, tup: \u001b[39mtuple\u001b[39m):\n\u001b[1;32m-> 1564\u001b[0m     tup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_tuple_indexer(tup)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mwith\u001b[39;00m suppress(IndexingError):\n\u001b[0;32m   1566\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_lowerdim(tup)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\indexing.py:870\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_tuple_indexer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[0;32m    866\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_tuple_indexer\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mtuple\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[0;32m    867\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[39m    Check the key for valid keys across my indexer.\u001b[39;00m\n\u001b[0;32m    869\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 870\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_key_length(key)\n\u001b[0;32m    871\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_ellipsis(key)\n\u001b[0;32m    872\u001b[0m     \u001b[39mfor\u001b[39;00m i, k \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(key):\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandas\\core\\indexing.py:909\u001b[0m, in \u001b[0;36m_LocationIndexer._validate_key_length\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[39mraise\u001b[39;00m IndexingError(_one_ellipsis_message)\n\u001b[0;32m    908\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key_length(key)\n\u001b[1;32m--> 909\u001b[0m     \u001b[39mraise\u001b[39;00m IndexingError(\u001b[39m\"\u001b[39m\u001b[39mToo many indexers\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    910\u001b[0m \u001b[39mreturn\u001b[39;00m key\n",
      "\u001b[1;31mIndexingError\u001b[0m: Too many indexers"
     ]
    }
   ],
   "source": [
    "from multiprocessing.resource_sharer import stop\n",
    "import pandas as pd\n",
    "if __name__=='__main__':\n",
    "    df=pd.read_excel(\"data/TOP25.xlsx\")\n",
    "    \n",
    "    data=df['workorder_content'].astype(str)#要保证切词前数据全部为str\n",
    "    stopwordfile='data/baidu_stopwords.txt'\n",
    "    print(data.head(20))\n",
    "    # 数据EDA\n",
    "    print(\"缺失行数：\",data.isnull().sum())\n",
    "    \n",
    "    data_clear=cutChineseWord(data,stopwordfile=stopwordfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "from nlpia.book.examples.ch04_catdog_lsa_3x6x16 import word_topic_vectors\n",
    "print(word_topic_vectors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "873f1daa290faadfff74b052d0d871b15a7f0efff482ba627e1adff1831ed568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
