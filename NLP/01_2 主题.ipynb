{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题向量\n",
    "\n",
    "\n",
    "\n",
    "本章将学习这些语义或主题向量，通过TF-IDF向量的加权得分来计算所谓的主题得分，而将这些得分构成了主题向量的各个维度。\n",
    "\n",
    "将使用归一化词频直接的关联来将词归并到同意主题，每个归并结果定义了新主题向量的一个维度。\n",
    "----\n",
    "本章的学习主要包括以下部分\n",
    "* 根据nlp流水线，实现文本的读取、清洗、分词、以及tf-idf向量化的过程\n",
    "* 基于tf-idf文本向量化矩阵，实现主题向量分析，\n",
    "> tf-idf文本向量从本质而言只是从统计词频上体现了文本的特征，但缺少对文本含义的反映。\n",
    "我们需要一种方法来从词的统计数据中提取一些额外信息，即意义信息。需要更好地估算文档中的词到底意味着什么，也需要知道这些词的组合在一篇具体的文档中意味着什么，我们需要对tf-idf文本向量矩阵进一步处理，得到新的向量更加紧凑，更有意义。\n",
    "p\n",
    "**主题向量**\n",
    "主题向量可以对应词的意义向量：“词-主题向量”（word-topic vector），以及文档意义向量“文档-主题向量”（document-topic vector）。两者都额可以称为主题向量，不过表现对象一个是词一个是文档。\n",
    "简单地理解就是将文本矩阵进行降维，行（文本数量）不变，列（原为统一后分词数，例如1000个）根据算法降低为新的列（主题数），使得文本能对应不同主题。\n",
    "\n",
    "主题分析涉及多种不同的方法，包括：\n",
    "\n",
    "1. 通过隐形语义分析（latent senmantic analysis LSA）可以不仅仅把词的意义表示为向量，还可以用向量来表示通篇文档的意义。\n",
    ">LSA是一种分析tf-idf文本向量矩阵的算法，他将词分组到主题中，LSA也可以对词袋向量进行处理，但是tf-idf向量给出的结果更好。\n",
    ">LSA 还可以对这些主题进行优化，以保持主题维度的多样性。当使用这些新主题而不是原始词时，我们依然可以捕获文档的大部分含义（语义）。该模型中用于捕获文档含义所需要主题数量远远少于tf-idf向量词汇表中词的数量。\n",
    ">LSA通常被认为是一种降维技术，减少捕获文档含义所需要的维数。\n",
    ">其数学背景为PCA以及SVD，实际上，由于tf-idf矩阵不太可能是方阵，故使用的是SVD。\n",
    "2. 线性判别分析（linear discriminant analysis，LDA）(不深度讲解)\n",
    "3. 潜在狄利克雷分布(Latent Dirichlet Allocation,LDiA)\n",
    ">LDiA还可以用来生成捕捉词或文档语义的向量。LDiA和LSA在数学上并不一样，它使用非线性统计算法将词分组。因此，它通常会比线性方法（如LSA）的训练时间长很多。这常常使LDiA在许多实际应用中并不实用。尽管如此，它所创建的主题的统计数据有时更接近于人类对词和主题的直觉，所以LDiA主题通常更易于向上级解释。此外，LDiA对于一些单文档问题有用，如文档摘要。这时，语料库就是单篇文档，文档的句子就是该“语料库”中的一篇篇“文档”。这就是gensim和其他软件包使用LDiA来识别文档中最核心句子的做法。然后这些句子可以串在一起形成一篇由机器生成的摘要[11]。对于大多数分类或回归问题，通常最好使用LSA。但结合可视化时其主要使用的是LDiA算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "topic = {}\n",
    "random.seed(1)\n",
    "tfidf = dict(\n",
    "    list(zip('cat dog apple lion NYC love'.split(), np.random.rand(6))))\n",
    "\n",
    "print(\"得到虚拟的每个词的tf-idf值\")\n",
    "tfidf\n",
    "topic['petness'] = (0.3*tfidf['cat']+0.3*tfidf['dog']+0 *\n",
    "                    tfidf['apple']+0*tfidf['lion']-0.2*tfidf['NYC']+0.2*tfidf['love'])\n",
    "topic['animalness'] = (0.1*tfidf['cat']+0.1*tfidf['dog']-0.1 *\n",
    "                       tfidf['apple']+0.5*tfidf['lion']+0.1*tfidf['NYC']-0.1*tfidf['love'])\n",
    "topic['cityness'] = (0*tfidf['cat']-0.1*tfidf['dog']+0.2*tfidf['apple'] -\n",
    "                     0.1*tfidf['lion']-0.5*tfidf['NYC']+0.1*tfidf['love'])\n",
    "print(topic)\n",
    "# 构建相应矩阵\n",
    "topic_m = np.zeros(shape=(3, 6))\n",
    "print(topic_m)\n",
    "word_tf = np.zeros(shape=(6, 1))\n",
    "print(word_tf)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA \n",
    "\n",
    "LDA 分类器是一种有监督算法，因此需要对文本进行标注，但是其需要训练的样本数相对较少。\n",
    "\n",
    "LDA是一维模型，所以其不需要SVD，可以只计算二类问题（如垃圾和非垃圾）问题中的每一类的所有TF-IDF向量的质心（平均值）。推导就变成了这两个质心之间的直线，TF-IDF向量与这条直线越近（TF-IDF向量与这两条直线的点积）就表示它与其中一个类接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cgitb import handler\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "print(\"处理训练数据：...\\n\")\n",
    "train_txt = pd.read_table('sms/train.txt', sep='\\t', header=None)\n",
    "train_txt.columns = ['label', 'text']\n",
    "label_map = {'ham': 0, 'spam': 1}  # 1为垃圾短信\n",
    "train_txt['label'] = train_txt['label'].map(label_map)\n",
    "\n",
    "# train_txt = pd.get_dummies(train_txt, columns=['label'])# 将标签onehot编码\n",
    "\n",
    "\n",
    "def pre_clean_text(origin_text):\n",
    "    # 去掉标点符号和非法字符\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", origin_text)\n",
    "    # 将字符全部转化为小写，并通过空格符进行分词处理\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # 将剩下的词还原成str类型\n",
    "    cleaned_text = \" \".join(words)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # 清理数据\n",
    "    train_txt['text'] = train_txt['text'].apply(lambda x: pre_clean_text(x))\n",
    "\n",
    "    # 删去空值.测试时若无效词删去后为空则直接为垃圾信息(实际测试中没有)\n",
    "    # print(train_txt.shape)\n",
    "    train_txt = train_txt.loc[train_txt['text'] != '', :]\n",
    "    # 查看数据\n",
    "\n",
    "    # print(train_txt.shape)\n",
    "    # 实现tf-id数据向量化\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer=\"word\",\n",
    "        tokenizer=None,\n",
    "        preprocessor=None,\n",
    "        stop_words=None,\n",
    "        max_features=200)\n",
    "    word_vict = tfidf.fit_transform(train_txt['text']).toarray()\n",
    "    print(word_vict.shape)\n",
    "    print(word_vict[20, :])\n",
    "\n",
    "    mask = np.array(train_txt['label'].astype(bool))\n",
    "    print(\"得到{}矩阵\".format(mask.shape))\n",
    "    spam_centroid = word_vict[mask].mean(axis=0).round(2)  # axis=0 计算列平均值\n",
    "    print(\"垃圾短信平均向量：\", spam_centroid.shape)\n",
    "    ham_centroid = word_vict[mask].mean(axis=0).round(2)\n",
    "\n",
    "    print(\"短信平均向量：\", ham_centroid.shape)\n",
    "    sh = spam_centroid-ham_centroid\n",
    "    print(sh.shape)\n",
    "    spamsocre = word_vict@sh\n",
    "\n",
    "    spamscore2 = word_vict@ham_centroid\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    spam1 = MinMaxScaler().fit_transform(\n",
    "        spamsocre.reshape(-1, 1))  # reshape(-1,1)转换成1列：\n",
    "\n",
    "    spam2 = MinMaxScaler().fit_transform(spamscore2.reshape(-1, 1))\n",
    "\n",
    "    train_txt['lda_score'] = spam1\n",
    "    train_txt['lda_pred'] = (train_txt['lda_score'] > 0.2).astype(int)\n",
    "    train_txt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐性语义分析(LSA,Latent Semantic Analysis)\n",
    "> 参考\n",
    "> * https://zhuanlan.zhihu.com/p/46376672\n",
    "> * https://zhuanlan.zhihu.com/p/144367432\n",
    "\n",
    "LSA 是主题分析中最重要的技术,是主题分析讲解的重点\n",
    "LSA的底层是SVD技术，利用SVD将TF_IDF矩阵分解3个矩阵，而后根据其方差贡献率（信息载荷）进行降维，当在NLL中使用SVD时，将其称为隐性语义分析（LSA），\n",
    "\n",
    "LSA揭示了被隐含并等待被发现的词的语义或意义。\n",
    "\n",
    "LSA是一种属性技术，用于寻找对任意一组NLP向量进行最佳线性变换（旋转和拉伸）的方法，这些NLP向量包括TF-IDF向量或词袋向量。对许多应用而言，最好的变换方法是将\n",
    "\n",
    "坐标轴（维度）对齐到新向量中，使得其在词频上具有最大方差。然后可以在新向量空间中去掉哪些对不同文档向量贡献不大的维度。\n",
    "\n",
    "<font color='red'>LSA 中单词-文本-svd关系</font>\n",
    "<img src=\"https://pic2.zhimg.com/v2-b3eb29a45d1fc11f57b858fd5af7571d_r.jpg\"/>\n",
    "> LSA 步骤\n",
    "1. 构建TF-IDF或其他文档-词矩阵向量,行为文档(doc)，列为词(term)\n",
    "<img src=\"https://pic4.zhimg.com/80/v2-288292d4fd98b748c4b5e37786c06bd3_720w.jpg\"/>\n",
    "2. 对矩阵向量进行SVD分解\n",
    "3. 根据主题数目，或方差贡献率累计比，选择降维数目\n",
    "4. 对原有矩阵进行降维\n",
    "\n",
    "\n",
    "---\n",
    "潜在语义分析基于最古老和最常用的降维技术——奇异值分解（SVD）。SVD甚至早在“机器学习”这个术语出现之前就已经广泛使用。SVD将一个矩阵分解成3个方阵，其中一个是对角矩阵。SVD的一个应用是求逆矩阵。一个矩阵可以分解成3个更简单的方阵，然后对这些方阵求转置后再把它们相乘，就得到了原始矩阵的逆矩阵。\n",
    "\n",
    "SVD是一种可以将任何矩阵分解成3个因子矩阵的算法，而这3个因子矩阵可以相乘来重建原始矩阵。这类似于为一个大整数找到恰好3个整数因子，但是这里的因子不是标量整数，而是具有特殊性质的二维实矩阵。通过SVD计算出的3个因子矩阵具有一些有用的数学性质，这些性质可以用于降维和LSA。在线性代数课上，我们可能已经用过SVD来求逆矩阵。这里，我们将使用它为LSA计算出主题***（相关词的组合）***。无论是在基于词袋的词项-文档矩阵还是基于TF-IDF的词项-文档矩阵上运行SVD，SVD都会找到属于同类的词组合。SVD通过计算词项-文档矩阵的列（词项）之间的相关度来寻找那些同时出现的词。SVD能同时发现文档间词项使用的相关性和文档之间的相关性。利用这两条信息，SVD还可以计算出语料库中方差最大的词项的线性组合。这些词项频率的线性组合将成为我们的主题。我们将只保留那些在语料库中包含信息最多、方差最大的主题。SVD同时也提供了词项-文档向量的一个线性变换（旋转），它可以将每篇文档的向量转换为更短的主题向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install jieba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下为直接通过np中SVD方法进行降维，在实际应用中不会用到这种方法，但通过代码可以了解背后的数学含义。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tf\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def loadData():\n",
    "    '''实现载入sklearn中的“20 Newsgroup”数据\n",
    "    '''\n",
    "    dataset = fetch_20newsgroups(\n",
    "        shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "    return pd.DataFrame(dataset.data)\n",
    "\n",
    "\n",
    "def clearData(df: pd.DataFrame):\n",
    "    \"\"\"开始之前，我们先尝试着清理文本数据。主要思想就是清除其中的标点、数字和特殊字符。之后，我们需要删除较短的单词，因为通常它们不会包含什么有用的信息。最后，我们将文本变为不区分大小写。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _原始文本_\n",
    "    \"\"\"\n",
    "# removing everything except alphabets`\n",
    "\n",
    "    df['clean_doc'] = df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "    df['clean_doc'] = df['clean_doc'].apply(\n",
    "        lambda x: ' '.join([w for w in x.split() if len(w) > 3]))\n",
    "# make all text lowercase\n",
    "    df['clean_doc'] = df['clean_doc'].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "\n",
    "def cutChineseWord(data, stopwordfile):\n",
    "\n",
    "    stopwords = [line.strip() for line in open(\n",
    "        stopwordfile, 'r', encoding='utf-8').readlines()]\n",
    "    # 首先实现切词\n",
    "    for i in range(len(data)):\n",
    "        doc = data.iloc[i-1, 0]\n",
    "        seq_list = jieba.cut(doc, use_paddle=True)\n",
    "        token = []\n",
    "\n",
    "        for seq in seq_list:\n",
    "            if seq not in stopwords:\n",
    "                token.append(seq)\n",
    "\n",
    "        data.at[i, 'words'] = list(token)  # 每次在words列第i个位置插入对象list\n",
    "    return data\n",
    "\n",
    "\n",
    "def stopWords(df: pd.DataFrame):\n",
    "    \"\"\"之后我们要删除没有特别意义的停止词，例如“it”、“they”、“am”、“been”、“about”、“because”、“while”等等。为了实现这一目的，我们要对文本进行标记化，也就是将一串文本分割成独立的标记或单词。删除停止词之后，再把这些标记组合在一起。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _description_\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    # tokenization分词\n",
    "    tokenized_doc = df['clean_doc'].apply(lambda x: x.split())\n",
    "    # remove stop-words\n",
    "    tokenized_doc = tokenized_doc.apply(\n",
    "        lambda x: [item for item in x if item not in stop_words])\n",
    "# de-tokenization\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    df['clean_doc'] = detokenized_doc\n",
    "    return df\n",
    "\n",
    "\n",
    "def vec_words(document: pd.DataFrame, max_feature: int = 200) -> np.array:\n",
    "    \"\"\"基于sklearn实现文档向量化，其中元素为tf-idf，注意TfidfVectorizer参数\n",
    "\n",
    "    Args:\n",
    "        document (pd.DataFrame): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tfidf = tf(analyzer=\"word\", tokenizer=None, max_features=max_feature)\n",
    "    tfidf.fit(document)\n",
    "    lexcion = tfidf.vocabulary_  # 返回向量化对应的词典（字典形式）\n",
    "    lexcion_len = tfidf.vocabulary_.__len__()  # 返回向量化对应的词典长度\n",
    "    vec = tfidf.fit_transform(document).toarray()\n",
    "    df = pd.DataFrame(vec, columns=list(lexcion.keys()))\n",
    "    return vec, lexcion, lexcion_len, df, tfidf\n",
    "\n",
    "\n",
    "def svd(vec: np.array):\n",
    "    \"\"\"基于np中线性代数函数实现svd，注意np.linalg.svd（）返回的特征值矩阵是一个向量而非矩阵，这是由于方法自动缩减了特征值\n",
    "    矩阵中0值，k的大小为A(m*n)中最小的值，为此本方法将特征值矩阵还原为m*n大小的矩阵其中k个特征值构成其对角值。\n",
    "\n",
    "    Args:\n",
    "        vec (np.array): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    u, sigma, vt = np.linalg.svd(vec)\n",
    "\n",
    "    Sigma = np.zeros(np.shape(vec))\n",
    "    len_ar = len(sigma)\n",
    "    Sigma[:len_ar, :len_ar] = np.diag(sigma)\n",
    "    return u, Sigma, vt, sigma\n",
    "\n",
    "\n",
    "def RightdemsionRe(A: np.array, u: np.array, Sigma: np.array, k: int) -> np.array:\n",
    "    \"\"\"_实现基于SVD的文本矩阵降维,由于是进行主题降维度、所以采用左奇异矩阵将维，实现词的降维_\n",
    "        SVD 中左（行）奇异矩阵降维 为 B=Ut（k）*A  其中Ut为U左奇异矩阵转置，k为转置后保留的行数\n",
    "\n",
    "    Args:\n",
    "        A(np.array):_原文档-词矩阵_\n",
    "        u (np.array): _左奇异矩阵_\n",
    "        Sigma (np.array): _奇异值矩阵_\n",
    "        vt (np.array): _右奇异矩阵_\n",
    "        n (int): _降维数目（话题数）_\n",
    "\n",
    "    Returns:\n",
    "        np.array: _description_\n",
    "    \"\"\"\n",
    "    B = u.T[:k, :]@A\n",
    "    # B=u[:,:n]@Sigma[:n,:n]@vt[:n,:]\n",
    "\n",
    "    return B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    print(\"转为Data数据观察\")\n",
    "    documents=loadData()\n",
    "    print(documents.head(3))\n",
    "    print(documents.describe())\n",
    "    #clear data\n",
    "    documents.columns=['document']\n",
    "    documents= clearData(documents)\n",
    "    print(documents.head(3))\n",
    "    # tf-idf 向量化 \n",
    "    vec,lexcion,lexcion_len,df,tfidf=vec_words(documents['clean_doc'])\n",
    "    print(type(lexcion))\n",
    "    print(\"得到向量化矩阵形状(行：文本，列：词）：{},词典个数：{}\".format(vec.shape,lexcion_len))\n",
    "    vect=vec.T# 对词向量转置\n",
    "    print(\"得到向量化矩阵形状(行：词，列：文本）：{},词典个数：{}\".format(vect.shape,lexcion_len))\n",
    "    print(\"词典 \\n\",lexcion)\n",
    "    #print(\"向量化后文本(行：文本，列：词\\n \",vec[:8,:])\n",
    "    print(\"向量化后的文本矩阵(行：文本，列：词）：\",df)\n",
    "    print(\"在进行svd之前，由于我们的最终目标是进行主题降维，使用np.linga.svd构建u，sigma,vt时候 \\n u对应的行向量就应该是其词（最初主题），故应该先对文本向量进行转置，输入（行：词，列：文本）矩阵，\\n 在基础上进行左奇异矩阵降维\")\n",
    "\n",
    "    print(\"由于输入的向量化文本为行：text，列：word，但np.linalg.svd对应的原始矩阵为，行：word，列：text故需要先转置\")\n",
    "    u,Sigma,vt,sigma=svd(vect)\n",
    "    print('sigma:',sigma.shape)\n",
    "    print(\"svg后的左奇异矩阵大小:{},奇异值矩阵大小:{},右奇异矩阵大小：{},奇异矩阵中对角线奇异值数量:{}\".format(u.shape,Sigma.shape,vt.shape,sigma.shape))\n",
    "    B=RightdemsionRe(vect,u,Sigma,20)\n",
    "    print(\"降维后数据大小：\",B.shape)\n",
    "    print(B[:,15])\n",
    "    \n",
    "    ### 通过sklearn中TruncatedSVD函数实现svd，但注意，其输入向量矩阵为（行：文本，列：词）\n",
    "    svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "    sk_vec=svd_model.fit(vec)\n",
    "    print(\"使用sklearn中TruncatedSVD进行降维，注意矩阵形态\")\n",
    "    print(len(svd_model.components_))\n",
    "    \n",
    "    '''\n",
    "    svd_model中的元素就是我们的主题，\n",
    "    我们可以用svdmodel.components查看。最后，在20个主题中输入几个重要单词，看模型会做出什么反应。\n",
    "    '''\n",
    "    terms=tfidf.get_feature_names()\n",
    "    print(terms)\n",
    "    print(\"#############################\")\n",
    "    print(lexcion)\n",
    "    \n",
    "    # 查看主题中信息\n",
    "    for i, comp in enumerate(svd_model.components_):\n",
    "        terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "    print(\"Topic \"+str(i)+\": \")\n",
    "    for t in sorted_terms:\n",
    "        print(t[0])\n",
    "        print(\" \")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于sklearn来实现LSA主题分析，并且针对中文文档实现降维操作。并实现NLP处理的API流水线方式\n",
    "> 参考资料\n",
    "* [TfidfVectorizer](https://zhuanlan.zhihu.com/p/166636681#:~:text=%E5%85%B6%E4%B8%AD%20vectorizer.get_feature_names%20%28%29%20%E5%8C%85%E5%90%AB%E4%BA%86%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%89%80%E6%9C%89%E5%8D%95%E8%AF%8D%E5%8E%BB%E9%87%8D%E5%90%8E%E7%9A%84%E9%9B%86%E5%90%88%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E4%B8%80%E4%B8%AA%E8%AF%8D%E5%85%B8%E3%80%82,%E5%BD%93%E7%84%B6%E4%BD%A0%E4%B9%9F%E5%8F%AF%E4%BB%A5%E7%BB%99%20CountVectorizer%20%E6%8F%90%E4%BE%9B%E4%B8%80%E4%B8%AA%E5%8D%95%E7%8B%AC%E7%9A%84%E8%AF%8D%E5%85%B8%EF%BC%8C%E5%90%A6%E5%88%99%20CountVectorizer%20%E4%BC%9A%E8%87%AA%E5%B7%B1%E4%BB%8E%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%AD%A6%E4%B9%A0%E5%88%B0%E8%AF%8D%E5%85%B8%E3%80%82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.resource_sharer import stop\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import jieba \n",
    "def stopwordslist(filepath):\n",
    "     stopwords = [line.strip() for line in open(filepath, 'r',encoding='utf-8').readlines()]\n",
    "     return stopwords\n",
    " \n",
    "def vec_words(document: pd.DataFrame,max_features=200,output=True) -> np.array:\n",
    "    \"\"\"基于sklearn实现文档向量化，其中元素为tf-idf，注意TfidfVectorizer参数\n",
    "\n",
    "    Args:\n",
    "        document (pd.DataFrame): _description_\n",
    "        max_features(int): 词典大小\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    tfidf = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\", max_df=0.6,max_features=max_features).fit(document)  \n",
    "    tfidf.fit(document)\n",
    "    lexcion = tfidf.vocabulary_  # 返回向量化对应的词典（字典形式）\n",
    "    lexcion_len = tfidf.vocabulary_.__len__()  # 返回向量化对应的词典长度\n",
    "     \n",
    "    vec_origal=tfidf.fit_transform(document)# 为了实现可视化，还需要返回tfidf化文本的原始形式\n",
    "    vec=vec_origal.toarray()#返回数组化后的tfidf矩阵\n",
    "    df = pd.DataFrame(vec, columns=list(lexcion.keys()))\n",
    "    vect=vec.T# 对词向量转置\n",
    "    terms=tfidf.get_feature_names()\n",
    "    #其中 vectorizer.get_feature_names()包含了数据中出现的所有单词去重后的集合，相当于一个词典。\n",
    "    # 当然你也可以给 CountVectorizer 提供一个单独的词典，否则 CountVectorizer 会自己从数据中学习到词典。\n",
    "    if output==True:\n",
    "        print(\"得到向量化矩阵形状(行：文本，列：词）：{},词典个数：{}\".format(vec.shape,lexcion_len))\n",
    "        \n",
    "        print(\"得到向量化矩阵形状(行：词，列：文本）：{},词典个数：{}\".format(vect.shape,lexcion_len))\n",
    "        print(\"词典 \\n\",lexcion)\n",
    "        print(\"数据中出现所有单词去重后集合 \\n\",terms)\n",
    "        #print(\"向量化后文本(行：文本，列：词\\n \",vec[:8,:])\n",
    "        print(\"向量化后的文本矩阵(行：文本，列：词）：\",df)\n",
    "    return vec, lexcion, lexcion_len, df, tfidf,terms,vec_origal\n",
    "\n",
    "def cutDoc2list(df:pd.DataFrame,stopwordfile:str,user_dic:str)->list:\n",
    "    \"\"\"自定义切词方法，返回新的切词列以及包含切词结果的list\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): 输入包含文本的DataFrame\n",
    "        stopwordfile (string): 停用词目录\n",
    "        user_dic (string): 用户字典目录\n",
    "\n",
    "    Returns:\n",
    "        list: 返回切词后的list\n",
    "    \"\"\"\n",
    "    \n",
    "    stopwords=stopwordslist(stopwordfile)\n",
    "    stopwords.append(\"x000D\")\n",
    "    stopwords.append(\"报警人\")\n",
    "    \n",
    "    jieba.load_userdict(user_dic)#用户定义词典\n",
    "    df['content']\n",
    "    df['words']='' #新增一列words\n",
    "    df['words']=df['words'].astype('object') #先转换成object类型\n",
    "    for i in range(len( df['content'])):\n",
    "        doc= df['content'][i]\n",
    "  \n",
    "        token=[]\n",
    "        seq_list=jieba.cut(doc)\n",
    "        for word  in seq_list:\n",
    "            if word not in stopwords:\n",
    "              token.append(word)\n",
    "    #print(token)\n",
    "            df.at[i,'words']=list(token)  # 每次在words列第i个位置插入对象list\n",
    "    word_list=list(df['words'])\n",
    "    doc_list=[\" \".join(s) for s in word_list]\n",
    "    return df,doc_list\n",
    "   \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #实现数据读取以及切词处理\n",
    "    df_0 = pd.read_excel(\"data/TOP25.xlsx\")\n",
    "    stopwordfile = 'data/baidu_stopwords.txt'\n",
    " \n",
    "    user_dic='data/dic.txt'\n",
    "    df_0['content']= df_0['workorder_content'].astype(str)# 要保证切词前数据全部为str\n",
    "    df,doc_list=cutDoc2list(df_0,stopwordfile,user_dic)\n",
    "    #####切词后矩阵生成tf-idf矩阵\n",
    "    vec,lexcion,lexcion_len,df,tfidf,terms,vec_origal=vec_words(doc_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[中文lSA参考1](https://cloud.tencent.com/developer/article/1530432)\n",
    "\n",
    "使用TruncatedSVD,把原先规模为(文本数，词汇数)的特征矩阵X化为规模为(文本数，主题数)的新特征矩阵X2：\n",
    "\n",
    "(由于主题数一般比词汇数少，这一方法也可以用来降维，以进行分类或聚类操作)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# SVD represent documents and terms in vectors \n",
    "topics_num=15#设置主题数目\n",
    "svd_model = TruncatedSVD(n_components=topics_num, algorithm='randomized', n_iter=100, random_state=122)\n",
    "\n",
    "svd_model.fit(vec)\n",
    "svd_matrix=svd_model.fit_transform(vec)\n",
    "len(svd_model.components_)\n",
    "print(\"降维后矩阵\",svd_matrix)\n",
    "print(\"降维后矩阵大小\",svd_matrix.shape)\n",
    "print(\"svd_matrix[i,t]为第i篇文档在第t个主题上的分布，所以该值越高的文档i，可以认为在主题t上更有代表性，我们便以此筛选出最能代表该主题的文档。\")\n",
    "#通过svd后的矩阵，根据文档在t个主题对应的相关数值，得到与第t个主题最相关的n_pic_docs个文档\n",
    "n_pick_docs= 2# 返回相关的文档数目\n",
    "topic_docs_id = [svd_matrix[:,t].argsort()[:-(n_pick_docs+1):-1] for t in range(topics_num)]#根据排序得到最相关的文档\n",
    "topic_docs_id\n",
    "print(\"降维后矩阵大小\",svd_matrix.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取每个主题代表性的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(topic_docs_id)):\n",
    "    doc=topic_docs_id[i]\n",
    "    doc1=doc[0]\n",
    "    doc2=doc[1]\n",
    "    print(\"第\",i,\"个主题对应的文档编号为：\")\n",
    "    print(topic_docs_id[i])\n",
    "    print(df_0.iloc[doc1,2])\n",
    "    print(df_0.iloc[doc2,2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取每个主题关键字,以及对应的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pick_keywords = 4#\n",
    "topic_keywords_id = [svd_model.components_[t].argsort()[:-(n_pick_keywords+1):-1] for t in range(topics_num)]\n",
    "topic_keywords_id\n",
    "for t in range(topics_num):\n",
    "    print(\"topic %d:\" % t)\n",
    "    print(\"    keywords: %s\" % \", \".join(terms[topic_keywords_id[t][j]] for j in range(n_pick_keywords)))\n",
    "    for i in range(n_pick_docs):\n",
    "        print(\"    doc %d\" % i)\n",
    "        print(\"\\t\"+doc_list[topic_docs_id[t][i]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###　"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主题的可视化分析\n",
    "[参考1](https://blog.csdn.net/m0_49263811/article/details/122216150)\n",
    "\n",
    "```python\n",
    "\n",
    "import pyLDAvis\n",
    "\n",
    "data = pyLDAvis.sklearn.prepare(sklearn_lda, sklearn_dtm, vectorizer)\n",
    "#data = pyLDAvis.gensim.prepare(gensim_lda, gensim_dtm, dictionary)\n",
    "#data = pyLDAvis.graphlab.prepare(topic_model, docs)\n",
    "``` \n",
    "\n",
    "参数解读：\n",
    "\n",
    "\n",
    "\n",
    "sklearnlda/gensimlda: 计算好的话题模型\n",
    "\n",
    "sklearndtm/gensimdtm: 文档词频矩阵\n",
    "\n",
    "vectorizer/dictionary: 词语空间\n",
    "\n",
    "topic_model: graphlab生成的话题模型\n",
    "\n",
    "docs: 语料集\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########可视化\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_tfidf = LatentDirichletAllocation(n_components=15, random_state=0)\n",
    "lda_tfidf.fit(vec_origal)\n",
    "pyLDAvis.enable_notebook()\n",
    "pic = pyLDAvis.sklearn.prepare(lda_tfidf, vec_origal, tfidf)\n",
    "pyLDAvis.display(pic)\n",
    "pyLDAvis.save_html(pic, 'lda_pass'+str(topics_num )+'.html')\n",
    "#去工作路径下找保存好的html文件\n",
    " \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现\n",
    "其中 vectorizer.get_feature_names()包含了数据中出现的所有单词去重后的集合，相当于一个词典。当然你也可以给 CountVectorizer 提供一个单独的词典，否则 CountVectorizer 会自己从数据中学习到词典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_cut_list=[list(jieba.cut(sent)) for sent in doc_list]\n",
    "\n",
    "doc=[\" \".join(s) for s in doc_cut_list]\n",
    "\n",
    "vec=TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\") #  token_pattern这个参数使用正则表达式来分词，其默认参数为r\"(?u)\\b\\w\\w+\\b\"，其中的两个\\w决定了其匹配长度至少为2的单词，所以这边减到1个。对这个参数进行更多修改，可以满足其他要求，比如这里依然没有得到标点符号， \n",
    "\n",
    "model=vec.fit(doc)\n",
    "lexcion=model.vocabulary_\n",
    "print(\"通过slearn建立的词典：\",lexcion)\n",
    "print(\"词典长度\",len(lexcion))\n",
    "sparse_result = model.transform(doc)\n",
    "print(sparse_result)\n",
    "sklearn_matrix=sparse_result.todense().round(2)\n",
    "print(sklearn_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Mapping\n",
    "from nlpia.book.examples.ch04_catdog_lsa_3x6x16 import word_topic_vectors\n",
    "print(word_topic_vectors.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "873f1daa290faadfff74b052d0d871b15a7f0efff482ba627e1adff1831ed568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
