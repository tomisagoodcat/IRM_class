{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题向量\n",
    "\n",
    "通过隐形语义分析（latent senmantic analysis LSA）可以不仅仅把词的意义表示为向量，还可以用向量来表示通篇文档的意义。\n",
    "\n",
    "本章将学习这些语义或主题向量，通过TF-IDF向量的加权得分来计算所谓的主题得分，而将这些得分构成了主题向量的各个维度。\n",
    "\n",
    "将使用归一化词频直接的关联来将词归并到同意主题，每个归并结果定义了新主题向量的一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得到虚拟的每个词的tf-idf值\n",
      "{'petness': 0.3801465633219733, 'animalness': 0.4797383032984889, 'cityness': -0.2865590942806303}\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "topic={}\n",
    "random.seed(1)\n",
    "tfidf= dict(list(zip('cat dog apple lion NYC love'.split(),np.random.rand(6))))\n",
    "\n",
    "print(\"得到虚拟的每个词的tf-idf值\")\n",
    "tfidf\n",
    "topic['petness']=(0.3*tfidf['cat']+0.3*tfidf['dog']+0*tfidf['apple']+0*tfidf['lion']-0.2*tfidf['NYC']+0.2*tfidf['love'])\n",
    "topic['animalness']=(0.1*tfidf['cat']+0.1*tfidf['dog']-0.1*tfidf['apple']+0.5*tfidf['lion']+0.1*tfidf['NYC']-0.1*tfidf['love'])\n",
    "topic['cityness']=(0*tfidf['cat']-0.1*tfidf['dog']+0.2*tfidf['apple']-0.1*tfidf['lion']-0.5*tfidf['NYC']+0.1*tfidf['love'])\n",
    "print(topic)\n",
    "# 构建相应矩阵 \n",
    "topic_m=np.zeros(shape=(3,6))\n",
    "print(topic_m)\n",
    "word_tf=np.zeros(shape=(6,1))\n",
    "print(word_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LDA \n",
    "LDA 分类器是一种有监督算法，因此需要对文本进行标注，但是其需要训练的样本数相对较少。\n",
    "\n",
    "LDA是一维模型，所以其不需要SVD，可以只计算二类问题（如垃圾和非垃圾）问题中的每一类的所有TF-IDF向量的质心（平均值）。推导就变成了这两个质心之间的直线，TF-IDF向量与这条直线越近（TF-IDF向量与这两条直线的点积）就表示它与其中一个类接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理训练数据：...\n",
      "\n",
      "(5161, 200)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6447955  0.         0.\n",
      " 0.47170963 0.         0.         0.         0.35643069 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.40252561 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2695535  0.        ]\n",
      "得到(5161,)矩阵\n",
      "垃圾短信平均向量： (200,)\n",
      "短信平均向量： (200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "from cgitb import handler\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "print(\"处理训练数据：...\\n\")\n",
    "train_txt = pd.read_table('sms/train.txt',sep='\\t',header=None)  \n",
    "train_txt.columns = ['label', 'text']\n",
    "label_map = {'ham': 0, 'spam': 1 }#1为垃圾短信\n",
    "train_txt['label'] = train_txt['label'].map(label_map)\n",
    "\n",
    "#train_txt = pd.get_dummies(train_txt, columns=['label'])# 将标签onehot编码\n",
    "\n",
    "def pre_clean_text(origin_text):\n",
    "    # 去掉标点符号和非法字符\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", origin_text)\n",
    "    # 将字符全部转化为小写，并通过空格符进行分词处理\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # 将剩下的词还原成str类型\n",
    "    cleaned_text = \" \".join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    #清理数据\n",
    "    train_txt['text'] = train_txt['text'].apply(lambda x: pre_clean_text(x))\n",
    "\n",
    "    #删去空值.测试时若无效词删去后为空则直接为垃圾信息(实际测试中没有)\n",
    "    #print(train_txt.shape)\n",
    "    train_txt = train_txt.loc[train_txt['text'] != '',:]\n",
    "    # 查看数据\n",
    "     \n",
    "    #print(train_txt.shape)\n",
    "    #实现tf-id数据向量化\n",
    "    \n",
    "    tfidf = TfidfVectorizer (\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=200)\n",
    "    word_vict=tfidf.fit_transform(train_txt['text']).toarray()\n",
    "    print(word_vict.shape)\n",
    "    print(word_vict[20,:])\n",
    "    \n",
    "    mask=np.array(train_txt['label'].astype(bool))\n",
    "    print(\"得到{}矩阵\".format(mask.shape))\n",
    "    spam_centroid=word_vict[mask].mean(axis=0).round(2)#axis=0 计算列平均值\n",
    "    print(\"垃圾短信平均向量：\",spam_centroid.shape)\n",
    "    ham_centroid=word_vict[mask].mean(axis=0).round(2)\n",
    " \n",
    "    print(\"短信平均向量：\",ham_centroid.shape)\n",
    "    sh=spam_centroid-ham_centroid\n",
    "    print(sh.shape)\n",
    "    spamsocre=word_vict@sh \n",
    "   \n",
    "    spamscore2=word_vict@ham_centroid\n",
    "     \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    spam1=MinMaxScaler().fit_transform(spamsocre.reshape(-1,1))#reshape(-1,1)转换成1列：\n",
    " \n",
    "    spam2=MinMaxScaler().fit_transform(spamscore2.reshape(-1,1))\n",
    "  \n",
    "    train_txt['lda_score']=spam1\n",
    "    train_txt['lda_pred']=(train_txt['lda_score']>0.2).astype(int)\n",
    "    train_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐性语义分析\n",
    "LSA的底层是SVD技术，利用SVD将TF_IDF矩阵分解3个矩阵，而后根据其方差贡献率（信息载荷）进行降维，当在NLL中使用SVD时，将其称为隐性语义分析（LSA），\n",
    "\n",
    "LSA揭示了被隐含并等待被发现的词的语义或意义。\n",
    "\n",
    "LSA是一种属性技术，用于寻找对任意一组NLP向量进行最佳线性变换（旋转和拉伸）的方法，这些NLP向量包括TF-IDF向量或词袋向量。对许多应用而言，最好的变换方法是将\n",
    "\n",
    "坐标轴（维度）对齐到新向量中，使得其在词频上具有最大方差。然后可以在新向量空间中去掉哪些对不同文档向量贡献不大的维度。\n",
    "\n",
    "> LSA 步骤\n",
    "1. 构建TF-IDF或其他文档-词矩阵向量,行为文档(doc)，列为词(term)\n",
    "<gif url=\"https://pic4.zhimg.com/80/v2-288292d4fd98b748c4b5e37786c06bd3_720w.jpg\"/>\n",
    "2. 对矩阵向量进行SVD分解\n",
    "3. 根据主题数目，或方差贡献率累计比，选择降维数目\n",
    "4. 对原有矩阵进行降维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sklearn.datasets._twenty_newsgroups:Downloading 20news dataset. This may take a few minutes.\n",
      "INFO:sklearn.datasets._twenty_newsgroups:Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting PandasGUI\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/70/60/dfbe9817d621bb6868427283273e17c1209d0c5b763106a75acccd44ebda/pandasgui-0.2.13.tar.gz (215 kB)\n",
      "     -------------------------------------- 215.9/215.9 kB 1.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pandas in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (1.23.3)\n",
      "Requirement already satisfied: PyQt5 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (5.15.7)\n",
      "Requirement already satisfied: PyQt5-sip in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (12.11.0)\n",
      "Collecting PyQtWebEngine\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/65/d9/4b480349ac6d05cb5e82a37582479696beb6717bb717ad64c7d543d1a56a/PyQtWebEngine-5.15.6-cp37-abi3-win_amd64.whl (182 kB)\n",
      "     ------------------------------------ 182.7/182.7 kB 480.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: plotly in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (5.10.0)\n",
      "Collecting wordcloud\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/61/19099314c93861629db41678df8ad39fdf33423365aa56ad19e886a845d2/wordcloud-1.8.2.2-cp38-cp38-win_amd64.whl (152 kB)\n",
      "     ------------------------------------ 152.9/152.9 kB 914.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (63.4.1)\n",
      "Collecting appdirs\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting pynput\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/02/27/4de87850ff87c8dcecaaf8d27f28cec89ef17eeb6938f250449cb2635e06/pynput-1.7.6-py2.py3-none-any.whl (89 kB)\n",
      "     ---------------------------------------- 89.2/89.2 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: IPython in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (8.4.0)\n",
      "Collecting pyarrow\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/30/f3/d30d189ef1244da6dc964851d74e7cdc24ac07f73d353d24b118216dfba6/pyarrow-9.0.0-cp38-cp38-win_amd64.whl (19.6 MB)\n",
      "     ---------------------------------------- 19.6/19.6 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting astor\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/88/97eef84f48fa04fbd6750e62dcceafba6c63c81b7ac1420856c8dcc0a3f9/astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from PandasGUI) (4.3.0)\n",
      "Collecting qtstylish>=0.1.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b5/b6/77b24a0111fbfa7f1822c70070c91f0f00fcda696f35ecc13e806e0ac54d/qtstylish-0.1.5.tar.gz (983 kB)\n",
      "     -------------------------------------- 983.9/983.9 kB 2.1 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (0.1.6)\n",
      "Requirement already satisfied: backcall in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (3.0.20)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (2.11.2)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (0.18.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (0.4.5)\n",
      "Requirement already satisfied: decorator in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (5.1.1)\n",
      "Requirement already satisfied: traitlets>=5 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from IPython->PandasGUI) (5.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from pandas->PandasGUI) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from pandas->PandasGUI) (2022.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from plotly->PandasGUI) (8.1.0)\n",
      "Requirement already satisfied: six in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from pynput->PandasGUI) (1.16.0)\n",
      "Collecting PyQtWebEngine-Qt5>=5.15.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e0/db/0f29bd882aee8b5754f3e1ab104d2d09cdd9138a889558b43badcd81ce11/PyQtWebEngine_Qt5-5.15.2-py3-none-win_amd64.whl (60.0 MB)\n",
      "     ---------------------------------------- 60.0/60.0 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from wordcloud->PandasGUI) (3.6.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from wordcloud->PandasGUI) (9.2.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from jedi>=0.16->IPython->PandasGUI) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->PandasGUI) (0.2.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from matplotlib->wordcloud->PandasGUI) (4.37.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from matplotlib->wordcloud->PandasGUI) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from matplotlib->wordcloud->PandasGUI) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from matplotlib->wordcloud->PandasGUI) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from matplotlib->wordcloud->PandasGUI) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from matplotlib->wordcloud->PandasGUI) (1.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from stack-data->IPython->PandasGUI) (0.2.2)\n",
      "Requirement already satisfied: asttokens in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from stack-data->IPython->PandasGUI) (2.0.5)\n",
      "Requirement already satisfied: executing in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from stack-data->IPython->PandasGUI) (0.8.3)\n",
      "Building wheels for collected packages: PandasGUI, qtstylish\n",
      "  Building wheel for PandasGUI (setup.py): started\n",
      "  Building wheel for PandasGUI (setup.py): finished with status 'done'\n",
      "  Created wheel for PandasGUI: filename=pandasgui-0.2.13-py3-none-any.whl size=233706 sha256=1e6d61e4d2e6b0656b04389ddc525566a5457dde36820c5f2b3e98e9dfe0a78e\n",
      "  Stored in directory: c:\\users\\tomis\\appdata\\local\\pip\\cache\\wheels\\ba\\bf\\9b\\2ff0655f658bc536a49bb26a496cfbe1f3f44fd3f49935c75c\n",
      "  Building wheel for qtstylish (setup.py): started\n",
      "  Building wheel for qtstylish (setup.py): finished with status 'done'\n",
      "  Created wheel for qtstylish: filename=qtstylish-0.1.5-py3-none-any.whl size=1029268 sha256=447d88b21842b2aa348e2b5438bded572bf43c020d3828a543bdd8791660bd24\n",
      "  Stored in directory: c:\\users\\tomis\\appdata\\local\\pip\\cache\\wheels\\21\\d2\\99\\44e2da7906d932b03ac04be2fa9b67b10b1450bbb7585d496b\n",
      "Successfully built PandasGUI qtstylish\n",
      "Installing collected packages: PyQtWebEngine-Qt5, appdirs, pynput, pyarrow, astor, qtstylish, PyQtWebEngine, wordcloud, PandasGUI\n",
      "Successfully installed PandasGUI-0.2.13 PyQtWebEngine-5.15.6 PyQtWebEngine-Qt5-5.15.2 appdirs-1.4.4 astor-0.8.1 pyarrow-9.0.0 pynput-1.7.6 qtstylish-0.1.5 wordcloud-1.8.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install PandasGUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing QtWebEngineWidgets: 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m fetch_20newsgroups\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m \u001b[39mimport\u001b[39;00m show\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloadData\u001b[39m():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'''实现载入sklearn中的“20 Newsgroup”数据\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandasgui\\__init__.py:15\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m logger\u001b[39m.\u001b[39maddHandler(sh)\n\u001b[0;32m     14\u001b[0m \u001b[39m# Imports\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgui\u001b[39;00m \u001b[39mimport\u001b[39;00m show\n\u001b[0;32m     17\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mshow\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m__version__\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandasgui\\gui.py:18\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjson_viewer\u001b[39;00m \u001b[39mimport\u001b[39;00m JsonViewer\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnavigator\u001b[39;00m \u001b[39mimport\u001b[39;00m Navigator\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfigure_viewer\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureViewer\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandasgui\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mwidgets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msettings_editor\u001b[39;00m \u001b[39mimport\u001b[39;00m SettingsEditor\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mqtstylish\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pandasgui\\widgets\\figure_viewer.py:26\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m app \u001b[39m=\u001b[39m QtWidgets\u001b[39m.\u001b[39mQApplication\u001b[39m.\u001b[39minstance()\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m app \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mPyQt5\u001b[39;00m \u001b[39mimport\u001b[39;00m QtWebEngineWidgets\n\u001b[0;32m     27\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mReinitializing existing QApplication to allow import of QtWebEngineWidgets. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mThis may cause problems. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mTo avoid this, import pandasgui or PyQt5.QtWebEngineWidgets before a QApplication is created.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing QtWebEngineWidgets: 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd \n",
    "from pandasgui import show\n",
    "def loadData():\n",
    "    '''实现载入sklearn中的“20 Newsgroup”数据\n",
    "    '''\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "    return pd.DataFrame(dataset.data)\n",
    "\n",
    " \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    print(\"转为Data数据观察\")\n",
    "    document=loadData()\n",
    "    print(type(document))\n",
    "    print(document.describe())\n",
    "    print(document.head(15))\n",
    "    show(document)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing QtWebEngineWidgets: 找不到指定的模块。",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPyQt5\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mQtWebEngineWidgets\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing QtWebEngineWidgets: 找不到指定的模块。"
     ]
    }
   ],
   "source": [
    "import PyQt5.QtWebEngineWidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('c:\\\\Users\\\\tomis\\\\.conda\\\\envs\\\\py3.8\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('c:\\\\Users\\\\tomis\\\\.conda\\\\envs\\\\py3.8\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x94 in position 7333: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m Mapping\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnlpia\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexamples\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mch04_catdog_lsa_3x6x16\u001b[39;00m \u001b[39mimport\u001b[39;00m word_topic_vectors\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(word_topic_vectors\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\book\\examples\\ch04_catdog_lsa_3x6x16.py:130\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m         bow_pretty\u001b[39m.\u001b[39mloc[bow_pretty[col] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, col] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    126\u001b[0m     \u001b[39m# print(bow_pretty.head())\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \n\u001b[0;32m    129\u001b[0m \u001b[39m# do it all over again on a tiny portion of the corpus and vocabulary\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m corpus \u001b[39m=\u001b[39m get_data(\u001b[39m'\u001b[39;49m\u001b[39mcats_and_dogs_sorted\u001b[39;49m\u001b[39m'\u001b[39;49m)[:NUM_PRETTY]\n\u001b[0;32m    131\u001b[0m docs \u001b[39m=\u001b[39m normalize_corpus_words(corpus)\n\u001b[0;32m    132\u001b[0m tfidfer \u001b[39m=\u001b[39m TfidfVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_df\u001b[39m=\u001b[39m\u001b[39m.99\u001b[39m, stop_words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_pattern\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?u)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    133\u001b[0m                           vocabulary\u001b[39m=\u001b[39mfun_stems)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\loaders.py:1122\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(name, nrows, limit)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m filepaths[name]\n\u001b[0;32m   1121\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m DATASET_NAME2FILENAME:\n\u001b[1;32m-> 1122\u001b[0m     \u001b[39mreturn\u001b[39;00m read_named_csv(name, nrows\u001b[39m=\u001b[39;49mnrows)\n\u001b[0;32m   1123\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m DATA_NAMES:\n\u001b[0;32m   1124\u001b[0m     \u001b[39mreturn\u001b[39;00m read_named_csv(DATA_NAMES[name], nrows\u001b[39m=\u001b[39mnrows)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\loaders.py:1008\u001b[0m, in \u001b[0;36mread_named_csv\u001b[1;34m(name, data_path, nrows, verbose)\u001b[0m\n\u001b[0;32m   1006\u001b[0m name \u001b[39m=\u001b[39m DATASET_NAME2FILENAME[name]\n\u001b[0;32m   1007\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt.gz\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m-> 1008\u001b[0m     \u001b[39mreturn\u001b[39;00m read_text(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, name), nrows\u001b[39m=\u001b[39;49mnrows)\n\u001b[0;32m   1009\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1010\u001b[0m     \u001b[39mreturn\u001b[39;00m read_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, name), nrows\u001b[39m=\u001b[39mnrows)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\futil.py:420\u001b[0m, in \u001b[0;36mread_text\u001b[1;34m(forfn, nrows, verbose)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\" Read all the lines (up to nrows) from a text file or txt.gz file\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[0;32m    415\u001b[0m \u001b[39m>>> fn = os.path.join(DATA_PATH, 'mavis-batey-greetings.txt')\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39m>>> len(read_text(fn, nrows=3))\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m3\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m tqdm_prog \u001b[39m=\u001b[39m tqdm \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m no_tqdm\n\u001b[1;32m--> 420\u001b[0m nrows \u001b[39m=\u001b[39m wc(forfn, nrows\u001b[39m=\u001b[39;49mnrows)  \u001b[39m# not necessary when nrows==None\u001b[39;00m\n\u001b[0;32m    421\u001b[0m lines \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m, shape\u001b[39m=\u001b[39mnrows)\n\u001b[0;32m    422\u001b[0m \u001b[39mwith\u001b[39;00m ensure_open(forfn) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\futil.py:51\u001b[0m, in \u001b[0;36mwc\u001b[1;34m(f, verbose, nrows)\u001b[0m\n\u001b[0;32m     49\u001b[0m tqdm_prog \u001b[39m=\u001b[39m tqdm \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m no_tqdm\n\u001b[0;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m ensure_open(f, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m tqdm_prog(\u001b[39menumerate\u001b[39m(fin)):\n\u001b[0;32m     52\u001b[0m         \u001b[39mif\u001b[39;00m nrows \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m nrows \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     53\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x94 in position 7333: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "from collections.abc import Mapping\n",
    "from nlpia.book.examples.ch04_catdog_lsa_3x6x16 import word_topic_vectors\n",
    "print(word_topic_vectors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "873f1daa290faadfff74b052d0d871b15a7f0efff482ba627e1adff1831ed568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
