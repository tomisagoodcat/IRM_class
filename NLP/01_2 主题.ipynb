{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主题向量\n",
    "\n",
    "通过隐形语义分析（latent senmantic analysis LSA）可以不仅仅把词的意义表示为向量，还可以用向量来表示通篇文档的意义。\n",
    "\n",
    "本章将学习这些语义或主题向量，通过TF-IDF向量的加权得分来计算所谓的主题得分，而将这些得分构成了主题向量的各个维度。\n",
    "\n",
    "将使用归一化词频直接的关联来将词归并到同意主题，每个归并结果定义了新主题向量的一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "得到虚拟的每个词的tf-idf值\n",
      "{'petness': 0.3801465633219733, 'animalness': 0.4797383032984889, 'cityness': -0.2865590942806303}\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "topic={}\n",
    "random.seed(1)\n",
    "tfidf= dict(list(zip('cat dog apple lion NYC love'.split(),np.random.rand(6))))\n",
    "\n",
    "print(\"得到虚拟的每个词的tf-idf值\")\n",
    "tfidf\n",
    "topic['petness']=(0.3*tfidf['cat']+0.3*tfidf['dog']+0*tfidf['apple']+0*tfidf['lion']-0.2*tfidf['NYC']+0.2*tfidf['love'])\n",
    "topic['animalness']=(0.1*tfidf['cat']+0.1*tfidf['dog']-0.1*tfidf['apple']+0.5*tfidf['lion']+0.1*tfidf['NYC']-0.1*tfidf['love'])\n",
    "topic['cityness']=(0*tfidf['cat']-0.1*tfidf['dog']+0.2*tfidf['apple']-0.1*tfidf['lion']-0.5*tfidf['NYC']+0.1*tfidf['love'])\n",
    "print(topic)\n",
    "# 构建相应矩阵 \n",
    "topic_m=np.zeros(shape=(3,6))\n",
    "print(topic_m)\n",
    "word_tf=np.zeros(shape=(6,1))\n",
    "print(word_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#LDA \n",
    "LDA 分类器是一种有监督算法，因此需要对文本进行标注，但是其需要训练的样本数相对较少。\n",
    "\n",
    "LDA是一维模型，所以其不需要SVD，可以只计算二类问题（如垃圾和非垃圾）问题中的每一类的所有TF-IDF向量的质心（平均值）。推导就变成了这两个质心之间的直线，TF-IDF向量与这条直线越近（TF-IDF向量与这两条直线的点积）就表示它与其中一个类接近。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理训练数据：...\n",
      "\n",
      "(5161, 200)\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.6447955  0.         0.\n",
      " 0.47170963 0.         0.         0.         0.35643069 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.40252561 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.2695535  0.        ]\n",
      "得到(5161,)矩阵\n",
      "垃圾短信平均向量： (200,)\n",
      "短信平均向量： (200,)\n",
      "(200,)\n"
     ]
    }
   ],
   "source": [
    "from cgitb import handler\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "print(\"处理训练数据：...\\n\")\n",
    "train_txt = pd.read_table('sms/train.txt',sep='\\t',header=None)  \n",
    "train_txt.columns = ['label', 'text']\n",
    "label_map = {'ham': 0, 'spam': 1 }#1为垃圾短信\n",
    "train_txt['label'] = train_txt['label'].map(label_map)\n",
    "\n",
    "#train_txt = pd.get_dummies(train_txt, columns=['label'])# 将标签onehot编码\n",
    "\n",
    "def pre_clean_text(origin_text):\n",
    "    # 去掉标点符号和非法字符\n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", origin_text)\n",
    "    # 将字符全部转化为小写，并通过空格符进行分词处理\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # 将剩下的词还原成str类型\n",
    "    cleaned_text = \" \".join(words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "if __name__=='__main__':\n",
    "\n",
    "    #清理数据\n",
    "    train_txt['text'] = train_txt['text'].apply(lambda x: pre_clean_text(x))\n",
    "\n",
    "    #删去空值.测试时若无效词删去后为空则直接为垃圾信息(实际测试中没有)\n",
    "    #print(train_txt.shape)\n",
    "    train_txt = train_txt.loc[train_txt['text'] != '',:]\n",
    "    # 查看数据\n",
    "     \n",
    "    #print(train_txt.shape)\n",
    "    #实现tf-id数据向量化\n",
    "    \n",
    "    tfidf = TfidfVectorizer (\n",
    "    analyzer=\"word\",\n",
    "    tokenizer=None,\n",
    "    preprocessor=None,\n",
    "    stop_words=None,\n",
    "    max_features=200)\n",
    "    word_vict=tfidf.fit_transform(train_txt['text']).toarray()\n",
    "    print(word_vict.shape)\n",
    "    print(word_vict[20,:])\n",
    "    \n",
    "    mask=np.array(train_txt['label'].astype(bool))\n",
    "    print(\"得到{}矩阵\".format(mask.shape))\n",
    "    spam_centroid=word_vict[mask].mean(axis=0).round(2)#axis=0 计算列平均值\n",
    "    print(\"垃圾短信平均向量：\",spam_centroid.shape)\n",
    "    ham_centroid=word_vict[mask].mean(axis=0).round(2)\n",
    " \n",
    "    print(\"短信平均向量：\",ham_centroid.shape)\n",
    "    sh=spam_centroid-ham_centroid\n",
    "    print(sh.shape)\n",
    "    spamsocre=word_vict@sh \n",
    "   \n",
    "    spamscore2=word_vict@ham_centroid\n",
    "     \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    spam1=MinMaxScaler().fit_transform(spamsocre.reshape(-1,1))#reshape(-1,1)转换成1列：\n",
    " \n",
    "    spam2=MinMaxScaler().fit_transform(spamscore2.reshape(-1,1))\n",
    "  \n",
    "    train_txt['lda_score']=spam1\n",
    "    train_txt['lda_pred']=(train_txt['lda_score']>0.2).astype(int)\n",
    "    train_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 隐性语义分析(LSA,Latent Semantic Analysis)\n",
    "> 参考\n",
    "> * https://zhuanlan.zhihu.com/p/46376672\n",
    "> * https://zhuanlan.zhihu.com/p/144367432\n",
    "\n",
    "LSA的底层是SVD技术，利用SVD将TF_IDF矩阵分解3个矩阵，而后根据其方差贡献率（信息载荷）进行降维，当在NLL中使用SVD时，将其称为隐性语义分析（LSA），\n",
    "\n",
    "LSA揭示了被隐含并等待被发现的词的语义或意义。\n",
    "\n",
    "LSA是一种属性技术，用于寻找对任意一组NLP向量进行最佳线性变换（旋转和拉伸）的方法，这些NLP向量包括TF-IDF向量或词袋向量。对许多应用而言，最好的变换方法是将\n",
    "\n",
    "坐标轴（维度）对齐到新向量中，使得其在词频上具有最大方差。然后可以在新向量空间中去掉哪些对不同文档向量贡献不大的维度。\n",
    "\n",
    "<font color='red'>LSA 中单词-文本-svd关系</font>\n",
    "<img src=\"https://pic2.zhimg.com/v2-b3eb29a45d1fc11f57b858fd5af7571d_r.jpg\"/>\n",
    "> LSA 步骤\n",
    "1. 构建TF-IDF或其他文档-词矩阵向量,行为文档(doc)，列为词(term)\n",
    "<img src=\"https://pic4.zhimg.com/80/v2-288292d4fd98b748c4b5e37786c06bd3_720w.jpg\"/>\n",
    "2. 对矩阵向量进行SVD分解\n",
    "3. 根据主题数目，或方差贡献率累计比，选择降维数目\n",
    "4. 对原有矩阵进行降维\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: nltk in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: click in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.set_proxy('SYSTEM PROXY')\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转为Data数据观察\n",
      "                                                   0\n",
      "0  Well i'm not sure about the story nad it did s...\n",
      "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...\n",
      "2  Although I realize that principle is not one o...\n",
      "            0\n",
      "count   11314\n",
      "unique  10994\n",
      "top          \n",
      "freq      218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomis\\AppData\\Local\\Temp\\ipykernel_7496\\3389439240.py:22: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_doc'] = df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            document  \\\n",
      "0  Well i'm not sure about the story nad it did s...   \n",
      "1  \\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to re...   \n",
      "2  Although I realize that principle is not one o...   \n",
      "\n",
      "                                           clean_doc  \n",
      "0  well sure about story seem biased what disagre...  \n",
      "1  yeah expect people read actually accept hard a...  \n",
      "2  although realize that principle your strongest...  \n",
      "得到向量化矩阵形状：(11314, 200),词典个数：200\n",
      "词典 \n",
      " {'well': 182, 'sure': 152, 'about': 1, 'what': 184, 'with': 192, 'your': 199, 'that': 159, 'most': 99, 'world': 195, 'having': 68, 'such': 150, 'have': 67, 'them': 161, 'least': 82, 'same': 135, 'think': 168, 'might': 97, 'they': 165, 'more': 98, 'from': 57, 'government': 63, 'some': 143, 'after': 4, 'look': 90, 'other': 111, 'when': 185, 'power': 122, 'people': 115, 'read': 130, 'actually': 3, 'hard': 66, 'need': 103, 'little': 88, 'these': 164, 'just': 78, 'will': 189, 'maybe': 95, 'much': 100, 'would': 196, 'still': 148, 'like': 85, 'know': 80, 'question': 128, 'this': 169, 'want': 181, 'must': 101, 'work': 194, 'over': 113, 'last': 81, 'several': 139, 'group': 65, 'nothing': 107, 'than': 157, 'name': 102, 'seems': 137, 'rather': 129, 'drive': 41, 'down': 40, 'going': 61, 'probably': 123, 'doesn': 38, 'something': 145, 'real': 131, 'year': 197, 'line': 86, 'even': 46, 'never': 104, 'time': 174, 'right': 133, 'post': 121, 'does': 37, 'good': 62, 'mail': 92, 'jesus': 77, 'part': 114, 'which': 187, 'also': 7, 'without': 193, 'enough': 45, 'great': 64, 'card': 26, 'into': 76, 'better': 21, 'there': 163, 'between': 22, 'under': 176, 'problems': 125, 'called': 25, 'file': 50, 'mean': 96, 'because': 15, 'anything': 10, 'many': 94, 'could': 32, 'make': 93, 'others': 112, 'then': 162, 'anyone': 9, 'years': 198, 'life': 84, 'used': 177, 'however': 72, 'tell': 156, 'thanks': 158, 'list': 87, 'computer': 30, 'available': 12, 'send': 138, 'help': 69, 'find': 52, 'information': 75, 'order': 110, 'only': 109, 'subject': 149, 'public': 127, 'free': 56, 'keep': 79, 'please': 118, 'software': 142, 'image': 73, 'data': 34, 'files': 51, 'where': 186, 'space': 146, 'info': 74, 'through': 173, 'before': 17, 'every': 47, 'another': 8, 'place': 117, 'general': 59, 'system': 153, 'following': 54, 'being': 18, 'call': 24, 'above': 2, 'note': 106, 'someone': 144, 'number': 108, 'things': 167, 'email': 44, 'version': 179, 'next': 105, 'based': 14, 'though': 171, 'been': 16, 'program': 126, 'code': 28, 'either': 43, 'support': 151, 'give': 60, 'windows': 191, 'first': 53, 'window': 190, 'best': 20, 'very': 180, 'back': 13, 'here': 70, 'state': 147, 'course': 33, 'should': 140, 'different': 36, 'come': 29, 'using': 178, 'since': 141, 'those': 170, 'example': 48, 'point': 119, 'long': 89, 'their': 160, 'found': 55, 'possible': 120, 'each': 42, 'person': 116, 'three': 172, 'both': 23, 'really': 132, 'again': 5, 'control': 31, 'against': 6, 'while': 188, 'were': 183, 'case': 27, 'said': 134, 'believe': 19, 'fact': 49, 'thing': 166, 'team': 155, 'second': 136, 'done': 39, 'game': 58, 'high': 71, 'able': 0, 'problem': 124, 'around': 11, 'didn': 35, 'true': 175, 'take': 154, 'made': 91, 'less': 83}\n",
      "sigma: (200,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer as tf\n",
    "import numpy as np \n",
    " \n",
    "def loadData():\n",
    "    '''实现载入sklearn中的“20 Newsgroup”数据\n",
    "    '''\n",
    "    dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "    return pd.DataFrame(dataset.data)\n",
    "\n",
    " \n",
    "def clearData(df:pd.DataFrame):\n",
    "    \"\"\"开始之前，我们先尝试着清理文本数据。主要思想就是清除其中的标点、数字和特殊字符。之后，我们需要删除较短的单词，因为通常它们不会包含什么有用的信息。最后，我们将文本变为不区分大小写。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _原始文本_\n",
    "    \"\"\"\n",
    "# removing everything except alphabets`\n",
    "    \n",
    "    df['clean_doc'] = df['document'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "# removing short words\n",
    "    df['clean_doc'] = df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "# make all text lowercase\n",
    "    df['clean_doc'] = df['clean_doc'].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "def stopWords(df:pd.DataFrame):\n",
    "    \"\"\"之后我们要删除没有特别意义的停止词，例如“it”、“they”、“am”、“been”、“about”、“because”、“while”等等。为了实现这一目的，我们要对文本进行标记化，也就是将一串文本分割成独立的标记或单词。删除停止词之后，再把这些标记组合在一起。\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): _description_\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    # tokenization分词\n",
    "    tokenized_doc = df['clean_doc'].apply(lambda x: x.split())\n",
    "    # remove stop-words\n",
    "    tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "# de-tokenization\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(df)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "\n",
    "    df['clean_doc'] = detokenized_doc\n",
    "    return df  \n",
    "\n",
    "def vec_words(document:pd.DataFrame,max_feature:int=200)->np.array:\n",
    "    \"\"\"基于sklearn实现文档向量化，其中元素为tf-idf，注意TfidfVectorizer参数\n",
    "\n",
    "    Args:\n",
    "        document (pd.DataFrame): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    tfidf =tf(analyzer=\"word\",tokenizer=None,max_features=max_feature)\n",
    "    tfidf.fit(document)\n",
    "    lexcion=tfidf.vocabulary_ # 返回向量化对应的词典\n",
    "    lexcion_len=tfidf.vocabulary_.__len__()  # 返回向量化对应的词典长度\n",
    "    vec=tfidf.fit_transform(document).toarray()\n",
    "    return vec,lexcion,lexcion_len\n",
    "\n",
    "def svd(vec:np.array):\n",
    "    # 由于输入的向量化文本为行：text，列：word，但np.linalg.svd对应的原始矩阵为行：wword，列：text故需要先转置\n",
    "    vecT=vec.T\n",
    "    u,sigma,vt=np.linalg.svd(vecT)\n",
    "    return u,sigma,vt\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print(\"转为Data数据观察\")\n",
    "    documents=loadData()\n",
    "    print(documents.head(3))\n",
    " \n",
    "    print(documents.describe())\n",
    " \n",
    "    #clear data\n",
    " \n",
    "    documents.columns=['document']\n",
    " \n",
    "    documents= clearData(documents)\n",
    "    print(documents.head(3))\n",
    "    # tf-idf 向量化 \n",
    "    vec,lexcion,lexcion_len=vec_words(documents['clean_doc'])\n",
    "    print(\"得到向量化矩阵形状：{},词典个数：{}\".format(vec.shape,lexcion_len))\n",
    "    print(\"词典 \\n\",lexcion)\n",
    "    #print(\"向量化后文本\\n \",vec[:8,:])\n",
    "    \n",
    "    u,sigma,vt=svd(vec)\n",
    "    print('sigma:',sigma.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\constants.py:136: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  [datetime.datetime, pd.datetime, pd.Timestamp])\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\constants.py:158: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  MIN_TIMESTAMP = pd.Timestamp(pd.datetime(1677, 9, 22, 0, 12, 44), tz='utc')\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\tutil.py:100: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\pugnlp\\util.py:80: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.constants:Starting logger in nlpia.constants...\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\futil.py:30: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\loaders.py:78: FutureWarning: The pandas.np module is deprecated and will be removed from pandas in a future version. Import numpy directly instead.\n",
      "  np = pd.np\n",
      "INFO:nlpia.loaders:No BIGDATA index found in c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv so copy c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\data\\bigdata_info.latest.csv to c:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\data\\bigdata_info.csv if you want to \"freeze\" it.\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('c:\\\\Users\\\\tomis\\\\.conda\\\\envs\\\\py3.8\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\mavis-batey-greetings.csv',), **{'low_memory': False})`...\n",
      "INFO:nlpia.futil:Reading CSV with `read_csv(*('c:\\\\Users\\\\tomis\\\\.conda\\\\envs\\\\py3.8\\\\lib\\\\site-packages\\\\nlpia\\\\data\\\\sms-spam.csv',), **{'low_memory': False})`...\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'gbk' codec can't decode byte 0x94 in position 7333: illegal multibyte sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\NLP\\01_2 主题.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mabc\u001b[39;00m \u001b[39mimport\u001b[39;00m Mapping\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnlpia\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbook\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexamples\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mch04_catdog_lsa_3x6x16\u001b[39;00m \u001b[39mimport\u001b[39;00m word_topic_vectors\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/NLP/01_2%20%E4%B8%BB%E9%A2%98.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(word_topic_vectors\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\book\\examples\\ch04_catdog_lsa_3x6x16.py:130\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    125\u001b[0m         bow_pretty\u001b[39m.\u001b[39mloc[bow_pretty[col] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, col] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    126\u001b[0m     \u001b[39m# print(bow_pretty.head())\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \n\u001b[0;32m    129\u001b[0m \u001b[39m# do it all over again on a tiny portion of the corpus and vocabulary\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m corpus \u001b[39m=\u001b[39m get_data(\u001b[39m'\u001b[39;49m\u001b[39mcats_and_dogs_sorted\u001b[39;49m\u001b[39m'\u001b[39;49m)[:NUM_PRETTY]\n\u001b[0;32m    131\u001b[0m docs \u001b[39m=\u001b[39m normalize_corpus_words(corpus)\n\u001b[0;32m    132\u001b[0m tfidfer \u001b[39m=\u001b[39m TfidfVectorizer(min_df\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, max_df\u001b[39m=\u001b[39m\u001b[39m.99\u001b[39m, stop_words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, token_pattern\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m(?u)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    133\u001b[0m                           vocabulary\u001b[39m=\u001b[39mfun_stems)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\loaders.py:1122\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(name, nrows, limit)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m filepaths[name]\n\u001b[0;32m   1121\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m DATASET_NAME2FILENAME:\n\u001b[1;32m-> 1122\u001b[0m     \u001b[39mreturn\u001b[39;00m read_named_csv(name, nrows\u001b[39m=\u001b[39;49mnrows)\n\u001b[0;32m   1123\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39min\u001b[39;00m DATA_NAMES:\n\u001b[0;32m   1124\u001b[0m     \u001b[39mreturn\u001b[39;00m read_named_csv(DATA_NAMES[name], nrows\u001b[39m=\u001b[39mnrows)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\loaders.py:1008\u001b[0m, in \u001b[0;36mread_named_csv\u001b[1;34m(name, data_path, nrows, verbose)\u001b[0m\n\u001b[0;32m   1006\u001b[0m name \u001b[39m=\u001b[39m DATASET_NAME2FILENAME[name]\n\u001b[0;32m   1007\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m name\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt.gz\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m-> 1008\u001b[0m     \u001b[39mreturn\u001b[39;00m read_text(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(data_path, name), nrows\u001b[39m=\u001b[39;49mnrows)\n\u001b[0;32m   1009\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1010\u001b[0m     \u001b[39mreturn\u001b[39;00m read_csv(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(data_path, name), nrows\u001b[39m=\u001b[39mnrows)\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\futil.py:420\u001b[0m, in \u001b[0;36mread_text\u001b[1;34m(forfn, nrows, verbose)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\" Read all the lines (up to nrows) from a text file or txt.gz file\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[0;32m    415\u001b[0m \u001b[39m>>> fn = os.path.join(DATA_PATH, 'mavis-batey-greetings.txt')\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39m>>> len(read_text(fn, nrows=3))\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \u001b[39m3\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    419\u001b[0m tqdm_prog \u001b[39m=\u001b[39m tqdm \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m no_tqdm\n\u001b[1;32m--> 420\u001b[0m nrows \u001b[39m=\u001b[39m wc(forfn, nrows\u001b[39m=\u001b[39;49mnrows)  \u001b[39m# not necessary when nrows==None\u001b[39;00m\n\u001b[0;32m    421\u001b[0m lines \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty(dtype\u001b[39m=\u001b[39m\u001b[39mobject\u001b[39m, shape\u001b[39m=\u001b[39mnrows)\n\u001b[0;32m    422\u001b[0m \u001b[39mwith\u001b[39;00m ensure_open(forfn) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\tomis\\.conda\\envs\\py3.8\\lib\\site-packages\\nlpia\\futil.py:51\u001b[0m, in \u001b[0;36mwc\u001b[1;34m(f, verbose, nrows)\u001b[0m\n\u001b[0;32m     49\u001b[0m tqdm_prog \u001b[39m=\u001b[39m tqdm \u001b[39mif\u001b[39;00m verbose \u001b[39melse\u001b[39;00m no_tqdm\n\u001b[0;32m     50\u001b[0m \u001b[39mwith\u001b[39;00m ensure_open(f, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mfor\u001b[39;00m i, line \u001b[39min\u001b[39;00m tqdm_prog(\u001b[39menumerate\u001b[39m(fin)):\n\u001b[0;32m     52\u001b[0m         \u001b[39mif\u001b[39;00m nrows \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m nrows \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     53\u001b[0m             \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'gbk' codec can't decode byte 0x94 in position 7333: illegal multibyte sequence"
     ]
    }
   ],
   "source": [
    "from collections.abc import Mapping\n",
    "from nlpia.book.examples.ch04_catdog_lsa_3x6x16 import word_topic_vectors\n",
    "print(word_topic_vectors.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "873f1daa290faadfff74b052d0d871b15a7f0efff482ba627e1adff1831ed568"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
