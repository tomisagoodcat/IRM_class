{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本向量化\n",
    "### 基本的one-hot向量化\n",
    "1. 首先构建词典：词典来源可以来源于现有文本中抽取，得到本地词典。\n",
    "> 在词典构建之前首先必须进行分词，英文直接使用str.split()即可，但中文分词需要使用其他工具\n",
    "2. 在本地词典基础上，通过one-hot可以构建最基本的文本向量矩阵，在词典中出现的为1，没有出现的为0\n",
    "\n",
    "---\n",
    "* 实现英文的文本one-hot 向量化构建\n",
    "* 实现基于sklearn的one-hot 向量化构建\n",
    "* 实现中文one-hot 向量化构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba \n",
    " \n",
    " \n",
    "def token2onehot(words)->pd.DataFrame:\n",
    "    words_set=sorted(set(words))\n",
    "    print(\"分词后list转化为集合，去重，并进行排序处理\",words_set)\n",
    "    diction={}\n",
    "    for index,value in enumerate(words_set):\n",
    "       diction[index]=value\n",
    "    print(\"转换后的本地词典：\",diction)\n",
    "\n",
    "    column=len(words)\n",
    "    row=len(diction)\n",
    "    onehotMatrix=np.zeros((row,column),dtype=float)\n",
    "    print(\"one-hot矩阵大小：\",onehotMatrix.shape)\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(diction)):\n",
    "          if words[i]==diction[j]:\n",
    "              \n",
    "             onehotMatrix[j,i]=1\n",
    "    df=pd.DataFrame(onehotMatrix)\n",
    "    df.columns=words\n",
    "    return(df)\n",
    "if __name__==\"__main__\":\n",
    "    print(\"英文one-hot，词典中单词来源于原文\")\n",
    "    sents=\"Life is like music. It must be composed by ear, feeling and instinct, not by rule.\"\n",
    "    words=sents.split()\n",
    "    df=token2onehot(words)\n",
    "    #print(df)\n",
    "    print(\"中文one-hot，词典中单词来源于原文\")\n",
    "    sents=\"中国国家统计局15日公布的70个大中城市房价数据显示\"\n",
    "    words=list(jieba.cut(sents))\n",
    "    df2=token2onehot(words)\n",
    "    print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自然语言处理书中方法\n",
    "import numpy as np \n",
    "sentence=\"Life is like music. It must be composed by ear, feeling and instinct, not by rule.\"\n",
    "token=sentence.split()\n",
    "vocad=sorted(set(token))# 对英文文本进行排序，数字最先，大写在前，小写在后\n",
    " \n",
    "','.join(vocad)\n",
    "print(token)\n",
    "print(vocad)\n",
    "num_tokens=len(token)\n",
    "vocad_size=len(vocad)\n",
    "#print(num_tokens)\n",
    "#print(vocad_size)\n",
    "onehot_vec=np.zeros((num_tokens,vocad_size),int)\n",
    "print(onehot_vec)\n",
    "for i, word in enumerate(token):\n",
    "        onehot_vec[i,vocad.index(word)]=1\n",
    "        \n",
    "print(onehot_vec)\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#  CountVectorizer函数，属于常见的特征数值计算类，是一个文本特征提取方法。对于每一个训练文本，它只考虑每种词汇在该训练文本中出现的频率，将文本中的词语转换为词频矩阵。CountVectorizer同样适用于中文。\n",
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自然语言处理书中方法\n",
    "import numpy as np \n",
    "sentence=\"Life is like music. It must be composed by ear, feeling and instinct, not by rule.\"\n",
    "token=sentence.split()\n",
    "vocad=sorted(set(token))# 对英文文本进行排序，数字最先，大写在前，小写在后\n",
    " \n",
    "','.join(vocad)\n",
    "print(token)\n",
    "print(vocad)\n",
    "num_tokens=len(token)\n",
    "vocad_size=len(vocad)\n",
    "#print(num_tokens)\n",
    "#print(vocad_size)\n",
    "onehot_vec=np.zeros((num_tokens,vocad_size),int)\n",
    "print(onehot_vec)\n",
    "for i, word in enumerate(token):\n",
    "        onehot_vec[i,vocad.index(word)]=1\n",
    "        \n",
    "print(onehot_vec)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df=pd.DataFrame(onehot_vec,columns=vocad)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bow={}\n",
    "for token in sentence.split():\n",
    "    sentence_bow[token]=1\n",
    "print(type(sentence_bow))\n",
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.Series(sentence_bow)\n",
    "df2=pd.DataFrame(s,columns=[\"sent\"])\n",
    "df2=df2.T\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents=\"To be both a speaker of words and a doer of deeds.\\n\"\n",
    "sents+=\"Only they who fulfill their duties in everyday matters will fulfill them on great occasions.\\n\"\n",
    "sents+=\"The shortest way to do many things is to only one thing at a time. \\n\"\n",
    "sents+=\"Life is like music. It must be composed by ear, feeling and instinct, not by rule.\\n\"\n",
    "sents+=\"Life is a great big canvas, and you should throw all the paint on it you can.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram\n",
    "one-hot 方法虽然能够完全数值化非结构化的文本，任何语义信息没有丢失是其优点。但是其矩阵太大，不易处理。实际上可以将其进行降维，矩阵中数值代表不是是否出现，而是出现的次数（频次）。\n",
    "> 对比\n",
    "### n-gram\n",
    "n-gram 是一个最多包含n个元素的序列，这些元素从由他们组成的序列（通常是字符串）中提取而成。一般而言，n-gram的“元素”可以是字符、音节、词，甚至像“A”、“T”、“G”、“C”等表示DNA的符号。\n",
    "实际上，单个汉字（或字符）也可以形成n-gram，但一般的单位是词，例如。\n",
    "\n",
    "“今天天气真好呀。”\n",
    "对用的2-gram为\n",
    "今天 天天 天气 \n",
    "\n",
    "如果1-gram实际就是对单个汉字的分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基于ntlk实现2-gram，并转换为list\n",
      "基于ntlk实现2-gram，并转换为list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['To be',\n",
       " 'be both',\n",
       " 'both a',\n",
       " 'a speaker',\n",
       " 'speaker of',\n",
       " 'of words',\n",
       " 'words and',\n",
       " 'and a',\n",
       " 'a doer',\n",
       " 'doer of',\n",
       " 'of deeds.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "sents=\"To be both a speaker of words and a doer of deeds.\"\n",
    "token=sents.split()\n",
    "gramList=list(ngrams(token,2))\n",
    "print(\"基于ntlk实现2-gram，并转换为list\")\n",
    "list([\" \".join(x) for x in gramList ])\n",
    "sents=\"To be both a speaker of words and a doer of deeds.\"\n",
    "token=sents.split()\n",
    "gramList=list(ngrams(token,2))\n",
    "print(\"基于ntlk实现2-gram，并转换为list\")\n",
    "list([\" \".join(x) for x in gramList ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在词袋基础上，进一步可以对文本进行分析，为了简化one-hot以及提供更多信息，在向量矩阵中，不保存词是否在词典中出现，而是记录其在文本中的出现频率，或者tf-idf等信息。\n",
    "\n",
    "<font color='red'>概念</font>\n",
    "* 词袋-词出现的频率或词频向量\n",
    "* n-gram袋: 词对（2-gram)、三元对（3-gram）等的计数\n",
    "\n",
    "collections.Counter对象是一个无序集合（collection），也称为袋（bag）或者多重集合（multiset）。以下通过Counter计算词频，形成词袋。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0.029411764705882353, '游': 0.029411764705882353, '侠客': 0.04411764705882353, '是': 0.029411764705882353, '一家': 0.014705882352941176, '提供': 0.014705882352941176, '网络': 0.014705882352941176, '出游': 0.014705882352941176, '和': 0.014705882352941176, '线路': 0.014705882352941176, '服务': 0.014705882352941176, '的': 0.029411764705882353, '公司': 0.029411764705882353, '，': 0.10294117647058823, '总部': 0.014705882352941176, '在': 0.014705882352941176, '浙江': 0.014705882352941176, '杭州': 0.014705882352941176, '。': 0.07352941176470588, '此次': 0.014705882352941176, '安排': 0.014705882352941176, '龙': 0.029411764705882353, '漕沟': 0.029411764705882353, '行程': 0.014705882352941176, '成都': 0.014705882352941176, '分公司': 0.014705882352941176, '8': 0.014705882352941176, '月': 0.014705882352941176, '15': 0.014705882352941176, '日': 0.014705882352941176, '部分': 0.014705882352941176, '遇险': 0.014705882352941176, '团友': 0.014705882352941176, '对游': 0.014705882352941176, '提出': 0.014705882352941176, '正式': 0.014705882352941176, '追责': 0.014705882352941176, '诉求': 0.014705882352941176, '此前': 0.014705882352941176, '据': 0.014705882352941176, '澎湃': 0.014705882352941176, '新闻报道': 0.014705882352941176, '彭州': 0.014705882352941176, '山洪': 0.014705882352941176, '亲历者': 0.014705882352941176, '张': 0.014705882352941176, '先生': 0.014705882352941176, '称': 0.014705882352941176, '其': 0.014705882352941176, '报名': 0.014705882352941176, '旅行社': 0.04411764705882353, '跟团': 0.014705882352941176, '到': 0.014705882352941176, '耍': 0.014705882352941176, '水': 0.014705882352941176, '质疑': 0.014705882352941176, '设计': 0.014705882352941176, '路线': 0.014705882352941176, '时未': 0.014705882352941176, '考虑': 0.014705882352941176, '风险': 0.014705882352941176, '该': 0.014705882352941176, '客服': 0.014705882352941176, '表示': 0.014705882352941176, '会': 0.014705882352941176, '加强': 0.014705882352941176, '产品安全': 0.014705882352941176, '评估': 0.014705882352941176}\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    " \n",
    "from collections import Counter\n",
    " \n",
    "#print(bag_of_words)\n",
    " \n",
    "def countWord(sents)->dict:\n",
    "    \"\"\"计算词频并返回相应数据\n",
    "    \n",
    "    Args:\n",
    "        sents:输入原始汉语句子。\n",
    "    \n",
    "    Returns:\n",
    "        返回dict形式的词袋，形如{词：词频}\n",
    "    \"\"\"\n",
    "    tokens=jieba.cut(sents)\n",
    "    bag_of_words=Counter(tokens)#\n",
    "    tf={}\n",
    "    len_word=len(bag_of_words)\n",
    "    for word in bag_of_words:\n",
    "     \n",
    "        count=bag_of_words[word]\n",
    "        tf_w=count/len_word\n",
    "        tf[word]=tf_w\n",
    "    return(tf)\n",
    "if __name__==\"__main__\":\n",
    "    sents=\"\"\"\n",
    "游侠客是一家提供网络出游和线路服务的公司，总部在浙江杭州。此次安排龙漕沟行程的是成都分公司。8月15日，部分遇险团友对游侠客公司提出正式追责诉求。\n",
    "此前，据澎湃新闻报道，彭州山洪亲历者张先生称，其报名游侠客旅行社跟团到龙漕沟耍水，质疑旅行社设计路线时未考虑风险。该旅行社客服表示，会加强产品安全评估。\"\"\"\n",
    "    tf=countWord(sents)\n",
    "    print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现文档的向量化\n",
    "\n",
    ">单独对一篇文档对应一个向量意义不大，需要对应多个文档，并为每篇文档创建其对应的向量。\n",
    "这需要实现每篇文档对应向量的值相对于其他向量有可比性，或者说一致性。需要进行以下步骤。\n",
    "1. 创建词库（lexicon），构成一个词库向量\n",
    "2. 计算多个文档中，每个文档的词袋，并于词库对应，构成每个文档的词袋向量。\n",
    "3. 注意文档向量与词库向量必须对应，包括长度一致；词位置一致。可以使用\n",
    "``` python\n",
    "np.zero_like(lexicon)\n",
    "```\n",
    "来构建与词库相同的0向量\n",
    "\n",
    "**使用方式构建的词向量矩阵，每一行对应一个文本，其顺序都为词典顺序而非原文顺序，或者说这种方式丢失了原文的词序语义信息**\n",
    "\n",
    "---\n",
    "<font color='red'>词频（TF）归一化</font>\n",
    "\n",
    "在构建多个文档对应向量构成矩阵时候，必须进行一致性处理，其涉及两个方面：\n",
    "\n",
    "1. 词频的归一化：之前的词频为$tf= \\frac{词在对应文档出现次数}{对应文档长度}$，所对应的是一个文档。但对于多个文档而言，为使得标准统一归一化词频为$tf_{归一化}=\\frac{词在对应出现次数}{词典长度}$。\n",
    "2. 向量长度与位置统一：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典单词数（维度数）： 60\n",
      "字典内容： ['50', '。', '一个', '一套', '一段', '不', '不能', '买', '买不起', '买房', '了', '他们', '住', '你', '公务员', '再', '几套', '又', '发到', '可以', '台湾', '吃', '各住', '回归祖国', '多', '大胆', '妈妈', '娘妈', '子女', '工资', '平都', '开始', '怀抱', '怎么样', '想法', '感化', '我', '房子', '把', '抓', '拿', '挤', '早日', '有', '然后', '爸爸', '的', '给', '视频', '贪', '贪钱', '贫农', '起来', '过', '这下子', '那么', '顶账', '！', '，', '？']\n",
      "构建一个矩阵，保存词袋向量，行为文本数量，列为字典单词数目，实现文本的统一： (5, 60)\n",
      "[[0.01666667 0.01666667 0.         0.05       0.         0.01666667\n",
      "  0.         0.01666667 0.         0.         0.01666667 0.\n",
      "  0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.01666667 0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.01666667 0.         0.         0.06666667 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[0.01666667 0.01666667 0.         0.05       0.         0.01666667\n",
      "  0.         0.01666667 0.         0.         0.01666667 0.\n",
      "  0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.01666667 0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.01666667 0.         0.         0.06666667 0.        ]\n",
      " [0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.\n",
      "  0.01666667 0.01666667 0.         0.         0.01666667 0.\n",
      "  0.         0.01666667 0.         0.         0.03333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.03333333 0.01666667]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[0.01666667 0.01666667 0.         0.05       0.         0.01666667\n",
      "  0.         0.01666667 0.         0.         0.01666667 0.\n",
      "  0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.01666667 0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.01666667 0.         0.         0.06666667 0.        ]\n",
      " [0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.\n",
      "  0.01666667 0.01666667 0.         0.         0.01666667 0.\n",
      "  0.         0.01666667 0.         0.         0.03333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.03333333 0.01666667]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.01666667 0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[0.01666667 0.01666667 0.         0.05       0.         0.01666667\n",
      "  0.         0.01666667 0.         0.         0.01666667 0.\n",
      "  0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.01666667 0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.01666667 0.         0.         0.06666667 0.        ]\n",
      " [0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.\n",
      "  0.01666667 0.01666667 0.         0.         0.01666667 0.\n",
      "  0.         0.01666667 0.         0.         0.03333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.03333333 0.01666667]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.01666667 0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.        ]\n",
      " [0.         0.         0.         0.         0.01666667 0.\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.01666667 0.         0.01666667 0.01666667\n",
      "  0.         0.         0.         0.         0.01666667 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "[[0.01666667 0.01666667 0.         0.05       0.         0.01666667\n",
      "  0.         0.01666667 0.         0.         0.01666667 0.\n",
      "  0.03333333 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.01666667 0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.01666667 0.         0.         0.06666667 0.        ]\n",
      " [0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.\n",
      "  0.01666667 0.01666667 0.         0.         0.01666667 0.\n",
      "  0.         0.01666667 0.         0.         0.03333333 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.03333333 0.01666667]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.01666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.01666667 0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.01666667 0.         0.         0.\n",
      "  0.01666667 0.         0.         0.         0.01666667 0.\n",
      "  0.01666667 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01666667 0.01666667 0.        ]\n",
      " [0.         0.         0.         0.         0.01666667 0.\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.01666667 0.         0.         0.01666667\n",
      "  0.         0.         0.01666667 0.         0.01666667 0.01666667\n",
      "  0.         0.         0.         0.         0.01666667 0.        ]\n",
      " [0.         0.01666667 0.         0.         0.         0.\n",
      "  0.01666667 0.         0.01666667 0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.01666667 0.01666667\n",
      "  0.         0.01666667 0.         0.01666667 0.         0.\n",
      "  0.01666667 0.         0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.01666667 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.01666667 0.\n",
      "  0.         0.01666667 0.         0.01666667 0.         0.\n",
      "  0.         0.         0.         0.         0.05       0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import jieba \n",
    "from collections import Counter\n",
    "\n",
    " \n",
    "def createLexicon(doc_vec:np.array)->list:\n",
    "    \"\"\" 创建词典\n",
    "    \n",
    "    Args:\n",
    "        doc_vec:输入矩阵形式的原始句子集。\n",
    "        \n",
    "    Returns:\n",
    "        去重并排序后得词典，list形式\n",
    "    \"\"\"\n",
    "    lexicon=[]\n",
    "    for sent in doc_vect:\n",
    "        doc_token=list(jieba.cut(sent))\n",
    "        lexicon.append(doc_token)\n",
    "    lex=sum(lexicon,[])\n",
    "    lex_set=sorted(set(lex))\n",
    "    return lex_set\n",
    "def countWord(sents,lexicon):\n",
    "    \"\"\"计算词频并返回相应数据\n",
    "    \n",
    "    Args:\n",
    "        sents:输入原始汉语句子。\n",
    "    \n",
    "    Returns:\n",
    "        返回dict形式的词袋，形如{词：词频}\n",
    "    \"\"\"\n",
    "    tokens=jieba.cut(sents)\n",
    "    bag_of_words=Counter(tokens)\n",
    "    len_lexicon=len(lexicon)#字典中单词数目\n",
    "    tf={}\n",
    "    for word in bag_of_words:\n",
    "     \n",
    "        count=bag_of_words[word]\n",
    "        tf_w=count/len_lexicon\n",
    "        tf[word]=tf_w\n",
    "    return tf\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    doc_list=[]\n",
    "    doc_list.append(\"买那么多，妈妈住一套，爸爸住一套，子女各住一套，这下子50平都不挤了。\")\n",
    "    doc_list.append(\"我有一个大胆的想法，公务员的工资拿房子顶账，怎么样？\")\n",
    "    doc_list.append(\"把视频发到台湾，感化他们早日回归祖国的怀抱！\")\n",
    "    doc_list.append(\"然后开始贪钱买房，过一段再给抓起来\")\n",
    "    doc_list.append(\"娘妈的，房子又不能吃，你可以多贪几套，贫农买不起。\")\n",
    "    doc_vect=np.array(doc_list)\n",
    "    lexicon=createLexicon(doc_vect)\n",
    "    print(\"字典单词数（维度数）：\",len(lexicon))\n",
    "    print(\"字典内容：\",lexicon)\n",
    "    #import copy \n",
    "    #vec=copy.copy(lexicon)\n",
    "    #print(vec)\n",
    " \n",
    "    sent_matrix=np.zeros((len(doc_list),len(lexicon)))\n",
    "    print(\"构建一个矩阵，保存词袋向量，行为文本数量，列为字典单词数目，实现文本的统一：\",sent_matrix.shape)\n",
    "    j=0\n",
    "    for sent in doc_list:\n",
    "         \n",
    "        count=countWord(sent,lexicon)\n",
    "        #print(count)\n",
    "        for key,value in count.items():#通过查找在字典中的index，为对应行中值赋值。\n",
    "            index=lexicon.index(key)\n",
    "            #print(\"词:\",key)\n",
    "            #print(\"对应字典index\",index)\n",
    "            #sent_matrix[j,index]=value\n",
    "            sent_matrix[j,index]=value\n",
    "        print(sent_matrix)\n",
    "        j=j+1# 换行\n",
    "#print(sent_matrix)\n",
    "    #print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基于书中方法\n",
    "from collections import OrderedDict\n",
    "zero_vec=OrderedDict((token,0) for token in lexicon) \n",
    "print(zero_vec)\n",
    "import copy \n",
    "doc_vec=[]\n",
    "doc_list=[]\n",
    "doc_list.append(\"买那么多，妈妈住一套，爸爸住一套，子女各住一套，这下子50平都不挤了。\")\n",
    "doc_list.append(\"我有一个大胆的想法，公务员的工资拿房子顶账，怎么样？\")\n",
    "doc_list.append(\"把视频发到台湾，感化他们早日回归祖国的怀抱！\")\n",
    "doc_list.append(\"然后开始贪钱买房，过一段再给抓起来\")\n",
    "doc_list.append(\"娘妈的，房子又不能吃，你可以多贪几套，贫农买不起。\")\n",
    "for doc in doc_list:\n",
    "    vec=copy.copy(zero_vec)\n",
    "    tokens=list(jieba.cut(doc))\n",
    "    token_count=Counter(tokens)\n",
    "    #print(token_count) \n",
    "    for key,value in token_count.items():\n",
    "\n",
    "        vec[key]=value/len(lexicon)\n",
    "    doc_vec.append(vec)\n",
    "    print(doc_vec)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本相似度计算\n",
    "在对文本进行词袋向量化的基础上，基于向量dot可以计算不同向量（文本）的$\\cos(\\theta)$从而得到向量的接近程度。\n",
    "$$\\cos(\\theta)=\\frac{\\vec{v1}dot\\vec{v2}}{|vec1||vec2|}$$\n",
    "\n",
    "<font color='red'>文本向量空间的维度</font>\n",
    "\n",
    "对于自然语言文档的空间向量，向量空间的维度是整个语料库（字典）中出现的不同词的数量，词库中有K个词就称为$K$维空间。\n",
    "\n",
    "在学术论文中也称为$|V|$，例如上面的代码中词库lexicon有50个词，那么其为50维空间。\n",
    "\n",
    "###齐普夫定律\n",
    ">　齐普夫定律是美国语言学家G.K.齐普夫（George Kingsley Zipf）于本世纪40年代提出的词频分布定律。它可以表述为：如果把一篇较长文章中每个词出现的频次统计起来，按照高频词在前、低频词在后的递减顺序排列，并用自然数个这些词编上的等级序号，即频次最高的词等级为1，频次次之的等级为2，......，频次最小的词等级为D，。若用f表示频次，r 表示序号，则有fr=C（C为常数）。人们称该式为齐普夫定律。\n",
    "\n",
    ">齐普夫定律是描述一系列实际现象的特点非常到位的经验定律之一。它认为，如果我们按照大小或者流行程度给某个大集合中的各项进行排序，集合中第二项的比重大约是第一项的一半，而第三项的比重大约是第一项的三分之一，以此类推。换句话来说，一般来讲，排在第k位的项目其比重为第一项的1/k。\n",
    "\n",
    ">齐普夫定律还从定量角度描述了目前流行的一个主题: 长尾巴定律（The Long Tail）。以一个集合中按流行程度排名的物品（如亚马逊网站上销售的图书）为例。表示流行程度的图表会向下倾斜，位于左上角的是几十本最流行的图书。该图会向右下角逐渐下降，那条长尾巴会列出每年销量只有一两本的几十万种图书。换成英文即齐普夫定律最初应用的领域，这条长尾巴就是你很少会遇到的几十万个单词，譬如floriferous或者refulgent。\n",
    "\n",
    ">把流行程度作为大致衡量价值的标准，齐普夫定律随后就会得出每一个物品的价值。也就是说，假设有100万个物品，那么最流行的100个物品将贡献总价值的三分之一，其次的10000个物品将贡献另外的三分之一; 剩余的98.99万个将贡献剩下的三分之一。有n个物品的集合其价值与log(n)成正比。\n",
    "\n",
    "<img src=\"https://p1.ssl.qhimg.com/t01caf1e134491a0504.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用TF-IDF替代TF构建文本向量\n",
    "之前词向量的构建单纯使用TF（Term Frequency）词频作为向量中值，但这种方法无法体现文档的特征。故一般结合“逆文档频率\"（Inverse Document Frequency，缩写为IDF），使用\n",
    "$$tf \\times idf$$\n",
    "代表每个词的特征，并载入词典对应向量位置，构建文本向量，其步骤如下：\n",
    "1. 对所有文档进行切词\n",
    "2. 构建词典（lexicon），包括去重、排序\n",
    "3. 对每个文档进行词典对应，即统一文档对应向量长度都为词典的长度，每个词对应一个槽位（slot），对应词库中位置。\n",
    "4. 槽位中的值为该词的TF-IDF值，完成文本向量构建，返回一个矩阵，矩阵大小为[len(文本数),len(lexicon)]\n",
    "\n",
    "---\n",
    "\n",
    "* 定义idf方法\n",
    "* 计算文档tf-idf并构建文本向量\n",
    "* 计算cos文本相似度\n",
    "* 使用sklearn实现tf-idf向量构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\tomis\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.672 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字典单词数（维度数）： 74\n",
      "字典内容： ['50', '。', '一个', '一套', '一段', '不', '不能', '买', '买不起', '买房', '了', '交流', '他们', '任务', '住', '你', '公务员', '其他', '再', '几套', '到', '单位', '又', '发到', '可以', '台湾', '吃', '各住', '商品房', '回归祖国', '多', '大胆', '妈妈', '娘妈', '子女', '完成', '将', '工作', '工资', '干部职工', '平都', '开始', '怀抱', '怎么样', '想法', '感化', '我', '房子', '把', '抓', '拿', '挤', '早日', '有', '期限内', '没有', '然后', '爸爸', '的', '给', '规定', '视频', '贪', '贪钱', '贫农', '购置', '起来', '过', '这下子', '那么', '顶账', '！', '，', '？']\n",
      "文本向量相关度： 0.0050314144312208885\n"
     ]
    }
   ],
   "source": [
    "#z'j\n",
    "import numpy as np \n",
    "import numpy as np \n",
    "import jieba \n",
    "from collections import Counter\n",
    "\n",
    " \n",
    "def createLexicon(doc_vec:np.array)->list:\n",
    "    \"\"\"构建字典\n",
    "    \n",
    "    Parameter:\n",
    "       doc_vec:输入一个包含多个文本的数组。\n",
    "    \n",
    "    Returns:\n",
    "       返回一个list包含去重，排序后的词典。\n",
    "    \"\"\"\n",
    "    lexicon=[]\n",
    "    for sent in doc_vect:\n",
    "        doc_token=list(jieba.cut(sent))\n",
    "        lexicon.append(doc_token)\n",
    "    lex=sum(lexicon,[])\n",
    "    lex_set=sorted(set(lex))\n",
    "    return lex_set\n",
    "def tf(sents,lexicon):\n",
    "    \"\"\"计算词频并返回相应数据\n",
    "    \n",
    "    Parameter:\n",
    "        sents:输入原始汉语句子。\n",
    "    \n",
    "    Returns:\n",
    "        返回dict形式的词袋，形如{词：词频}\n",
    "    \"\"\"\n",
    "    tokens=jieba.cut(sents)\n",
    "    bag_of_words=Counter(tokens)# Counter 返回一个词出现次数的集合\n",
    "    len_lexicon=len(lexicon)#字典中单词数目\n",
    "    tf={}\n",
    "    for word in bag_of_words:\n",
    "     \n",
    "        count=bag_of_words[word]#返回词出现次数\n",
    "        tf_w=count/len_lexicon #于词典中词数目相除\n",
    "        tf[word]=tf_w#得到该词对应的tf\n",
    "    return tf\n",
    "\n",
    "def cos_sim(vec1:np.array,vec2:np.array)->float:\n",
    "    \"\"\"计算两个向量的cos夹角值\"\"\"\n",
    "    n1=np.linalg.norm(vec1)\n",
    "    n2=np.linalg.norm(vec2)\n",
    "    v1_2=vec1@vec2\n",
    "    cos=v1_2/(n1*n2)\n",
    "    return cos\n",
    "def tf_idf(sents:np.array,lexicon:list)->np.array:\n",
    "    \"\"\"计算tf_idf并返回相应数据\n",
    "    \n",
    "    Parameter:\n",
    "        sents:输入包含所有文本。\n",
    "        lexicon:词典\n",
    "    \n",
    "    Returns:\n",
    "        返回矩阵，包含向量化后的文本，对应槽位上为tf-idf值\n",
    "    \"\"\"\n",
    "    sent_matrix=np.zeros((len(sents),len(lexicon)))#构建一个矩阵，保存词袋向量，行为文本数量，列为字典单词数目，实现文本的统一\n",
    "    j=0#矩阵起始行数\n",
    "    for sent in sents:   \n",
    "        tokens=jieba.cut(sent)\n",
    "        bag_of_words=Counter(tokens)# Counter 返回一个词出现次数的集合\n",
    "        len_lexicon=len(lexicon)#字典中单词数目\n",
    "        tf={}\n",
    "        for word,value in bag_of_words.items():\n",
    "             \n",
    "            word_contain=0\n",
    "            for _doc in sents:\n",
    "                if word in _doc:\n",
    "                    word_contain+=1\n",
    "            count=bag_of_words[word]#返回词出现次数\n",
    "            tf_w=count/len_lexicon #于词典中词数目相除\n",
    "           \n",
    "            idf=np.log(len(sents)/(word_contain+1))\n",
    "            \n",
    "            tf_idf=tf_w*idf\n",
    "            tf[word]=tf_idf#得到该词对应的tf_idf\n",
    "            index=lexicon.index(word)\n",
    "            sent_matrix[j,index]=tf_idf\n",
    "        j=j+1# 换行\n",
    "    return sent_matrix\n",
    " \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    doc_list=[]\n",
    "    doc_list.append(\"买那么多，妈妈住一套，爸爸住一套，子女各住一套，这下子50平都不挤了。\")\n",
    "    doc_list.append(\"我有一个大胆的想法，公务员的工资拿房子顶账，怎么样？\")\n",
    "    doc_list.append(\"把视频发到台湾，感化他们早日回归祖国的怀抱！\")\n",
    "    doc_list.append(\"然后开始贪钱买房，过一段再给抓起来\")\n",
    "    doc_list.append(\"娘妈的，房子又不能吃，你可以多贪几套，贫农买不起。\")\n",
    "    doc_list.append(\"规定期限内没有完成商品房购置任务的干部职工，将交流到其他单位工作。\")\n",
    "    doc_vect=np.array(doc_list)\n",
    "    lexicon=createLexicon(doc_vect)\n",
    "    print(\"字典单词数（维度数）：\",len(lexicon))\n",
    "    print(\"字典内容：\",lexicon)\n",
    "    sent_matrix=tf_idf(doc_vect,lexicon)\n",
    "    #print(sent_matrix)\n",
    "    test1=sent_matrix[0,:]\n",
    "    test2=sent_matrix[2,:]\n",
    "    rs=cos_sim(test1,test2)\n",
    "    print(\"文本向量相关度：\",rs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "sklearn 可以构建tf-idf文本向量，但对于中文需要进行一些处理\n",
    "\n",
    "https://blog.csdn.net/word_mhg/article/details/90317975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting jieba\n",
      "  Using cached jieba-0.42.1-py3-none-any.whl\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "doc_list=[]\n",
    "doc_list.append(\"买那么多，妈妈住一套，爸爸住一套，子女各住一套，这下子50平都不挤了。\")\n",
    "doc_list.append(\"我有一个大胆的想法，公务员的工资拿房子顶账，怎么样？\")\n",
    "doc_list.append(\"把视频发到台湾，感化他们早日回归祖国的怀抱！\")\n",
    "doc_list.append(\"然后开始贪钱买房，过一段再给抓起来\")\n",
    "doc_list.append(\"娘妈的，房子又不能吃，你可以多贪几套，贫农买不起。\")\n",
    "doc_list.append(\"规定期限内没有完成商品房购置任务的干部职工，将交流到其他单位工作。\")\n",
    "doc_list\n",
    "doc_cut_list=[list(jieba.cut(sent)) for sent in doc_list]\n",
    "\n",
    "doc=[\" \".join(s) for s in doc_cut_list]\n",
    "\n",
    "vec=TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\") #  token_pattern这个参数使用正则表达式来分词，其默认参数为r\"(?u)\\b\\w\\w+\\b\"，其中的两个\\w决定了其匹配长度至少为2的单词，所以这边减到1个。对这个参数进行更多修改，可以满足其他要求，比如这里依然没有得到标点符号， \n",
    "\n",
    "model=vec.fit(doc)\n",
    "lexcion=model.vocabulary_\n",
    "print(\"通过slearn建立的词典：\",lexcion)\n",
    "print(\"词典长度\",len(lexcion))\n",
    "sparse_result = model.transform(doc)\n",
    "print(sparse_result)\n",
    "sklearn_matrix=sparse_result.todense().round(2)\n",
    "print(sklearn_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['买那么多，妈妈住一套，爸爸住一套，子女各住一套，这下子50平都不挤了。',\n",
       " '我有一个大胆的想法，公务员的工资拿房子顶账，怎么样？',\n",
       " '把视频发到台湾，感化他们早日回归祖国的怀抱！',\n",
       " '然后开始贪钱买房，过一段再给抓起来',\n",
       " '娘妈的，房子又不能吃，你可以多贪几套，贫农买不起。',\n",
       " '规定期限内没有完成商品房购置任务的干部职工，将交流到其他单位工作。']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分词\n",
    "在NLP中，分词（tokenization）是一种特殊的文档切分（segmentation）过程。而文档切分能够将文本拆分成更小的文本块或片段，其中含有更为集中的信息内容。\n",
    "\n",
    "文档切分可以是将文档分成段落，将段落分成句子，将句子分为短语，将短语分成词条（通常是词），和标点。\n",
    "\n",
    "分词 是 NLP 流水线 的 第一步，因此他对流水线的后续处理有重要影响。分词器将自然语言文本这种非结构化数据切分成多个信息块，每个块都可以看成可计数的离散元素。这些元素在文档中的出现频率可以直接用于该文档向量表示。 上述过程立即将非结构化字符串（文本文档）转化为适合机器学习的数值型数据结构。元素出现频率可以直接被计算机用于触发有用的行动回复。或者也可以以特制方式用于某个机器学习流水线来触发更复杂的决策或行为。通过这种方式构建的词袋向量最常应用于文档检索或搜索。\n",
    "\n",
    "新增加一列，保存切词后的结果（word） 停用词：定义函数，去除停用词，其中停用词可以使用现有词表。\n",
    "\n",
    "用户自定义字典：\n",
    "可以指定自己自定义的词典，以便包含 jieba 词库里没有的词。虽然 jieba 有新词识别能力，但是自行添加新词可以保证更高的正确率\n",
    "\n",
    "用法： jieba.load_userdict(file_name) # file_name 为文件类对象或自定义词典的路径\n",
    "\n",
    "词典格式和 dict.txt 一样，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），\n",
    "\n",
    "用空格隔开，顺序不可颠倒。file_name 若为路径或二进制方式打开的文件，则文件必须为 UTF-8 编码。\n",
    "\n",
    "词频省略时使用自动计算的能保证分出该词的词频。 例如：\n",
    "\n",
    "创新办 3 i\n",
    "\n",
    "云计算 5\n",
    "\n",
    "凱特琳 nz\n",
    "\n",
    "台中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4b54236f4c0c00fb874f9f007f344520403f3fc507691b2728f3b89f19e3e2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
