{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性代数基础\n",
    "\n",
    "### 对称矩阵\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Dimensionality reduction with principle components\n",
    "\n",
    "**Principle component analysis**, or **PCA**, is an alternative to regularization and straight-forward feature elimination. PCA is particularly useful for problems with very large numbers of features compared to the number of training cases. For example, when faced with a problem with many thousands of features and perhaps a few thousand cases, PCA can be a good choice to **reduce the dimensionality** of the feature space.  \n",
    "\n",
    "PCA is one of a family of transformation methods that reduce dimensionality. PCA is the focus here, since it is the most widely used of these methods. \n",
    "\n",
    "The basic idea of PCA is rather simple: Find a linear transformation of the feature space which **projects the majority of the variance（方差）** onto a few orthogonal（正交的） dimensions in the transformed space. The PCA transformation maps the data values to a new coordinate system defined by the principle components. Assuming the highest variance directions, or **components**, are the most informative, low variance components can be eliminated from the space with little loss of information. \n",
    "\n",
    "The projection along which the greatest variance occurs is called the **first principle component**. The next projection, orthogonal to the first, with the greatest variance is call the **second principle component**. Subsequent components are all mutually orthogonal with decreasing variance along the projected direction.  \n",
    "\n",
    "Widely used PCA algorithms compute the components sequentially, starting with the first principle component. This means that it is computationally efficient to compute the first several components from a very large number of features. Thus, PCA can make problems with very large numbers of features computationally tractable（易处理的）. \n",
    "\n",
    "****\n",
    "**Note:** It may help your understanding to realize that principle components are a scaled version of the **eigenvectors** of the feature matrix. The scale for each dimensions is given by the **eigenvalues**. The eigenvalues are the fraction of the variance explained by the components. \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "To cement the concepts of PCA you will now work through a simple example. This example is restricted to 2-d data so that the results are easy to visualize. \n",
    "\n",
    "As a first step, execute the code in cell below to load the packages required for the rest of this notebook.\n",
    "The code in the cell below simulates data from a bivariate Normal distribution. The distribution is deliberately centered on $\\{ 0,0 \\}$ and with unit variance on each dimension. There is considerable correlation between the two dimensions leading to a covariance matrix:\n",
    "\n",
    "$$cov(X) =  \\begin{bmatrix}\n",
    "  1.0 & 0.6 \\\\\n",
    "  0.6 & 1.0\n",
    " \\end{bmatrix}$$\n",
    "\n",
    "Given the covariance matrix 100 draws from this distribution are computed using the `multivariate_normal` function from the Numpy `random` package. Execute this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.model_selection as ms\n",
    "from sklearn import linear_model\n",
    "import sklearn.metrics as sklm\n",
    "import sklearn.decomposition as skde\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from pandas_profiling import ProfileReport\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#模拟一个协方差为指定正态分布的二特征矩阵。\n",
    "nr.seed(124)\n",
    "cov = np.array([[1.0, 0.6], [0.6, 1.0]])\n",
    "mean = np.array([0.0, 0.0])\n",
    "\n",
    "sample = nr.multivariate_normal(mean, cov, 100)\n",
    "sample.shape\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel for this data, execute the code in the cell below to display a plot and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sample[:,0], sample[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the data have a roughly elliptical(椭圆的) pattern. The correlation between the two dimensions is also visible. \n",
    "\n",
    "With the simulated data set created, it is time to compute the PCA model. The code in the cell below does the following:\n",
    "1. Define a PCA model object using the `PCA` function from the scikit-learn `decomposition` package.\n",
    "2. Fit the PCA model to the sample data.\n",
    "3. Display the ratio of the **variance explained** by each of the components, where, for a matrix X, this ratio is given by:\n",
    "\n",
    "$$VE(X) = \\frac{Var_{X-component}(X)}{Var_{X-total}(X)}$$\n",
    "\n",
    "Notice that by construction:\n",
    "\n",
    "$$VE(X) = \\sum_{i=1}^N VE_i(X) = 1.0$$\n",
    "\n",
    "In other words, the sum of the variance explained for each component must add to the total variance or 1.0 for standardized data. \n",
    "不同因子载荷由其对应的方差除以总方差表示\n",
    "\n",
    "或者在对方差进行标准化处理后，其总方差为1。\n",
    "\n",
    "Execute this code and examine the result.\n",
    "> pca的方法explained_variance_ratio_计算了每个特征方差贡献率，所有总和为1，explained_variance_为方差值，通过合理使用这两个参数可以画出方差贡献率图或者方差值图，便于观察PCA降维最佳值。\n",
    "> 相关介绍 https://blog.csdn.net/qq_36523839/article/details/82558636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = skde.PCA()\n",
    "pca_fit = pca_model.fit(sample)\n",
    "print(pca_fit.explained_variance_ratio_)\n",
    "#### 使用numpy计算特征值和特征值所构成的对角化矩阵\n",
    "sample.T \n",
    "cov_s=np.cov(sample.T)\n",
    "lamda,Q=np.linalg.eig(cov_s)\n",
    "print(lamda)\n",
    "print(Q.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the explained variance of the first component is many times larger than for the second component. This is exactly the desired result indicating the first principle component explains the majority of the variance of the sample data. \n",
    "\n",
    "The code in the cell below computes and prints the scaled components. Mathematically, the scaled components are the eigenvectors scaled by the eigenvalues. Execute this code:\n",
    "在此为了得到不同特征向量的重要度，需要进行缩放，为特征向量与对应的特征值相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = pca_fit.components_\n",
    "for i in range(2):\n",
    "    comps[:,i] = comps[:,i] * pca_fit.explained_variance_ratio_\n",
    "print(comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the two vectors have their origins at $[ 0,0 ]$, and are quite different magnitude, and are pointing in different directions.  \n",
    "\n",
    "To better understand how the projections of the components relate to the data, execute the code to plot the data along with the principle components. Execute this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sample[:,0], sample[:,1])\n",
    "# plt.plot([0.0, comps[0,0]], [0.0,comps[0,1]], color = 'red', linewidth = 5)\n",
    "# plt.plot([0.0, comps[1,0]], [0.0,comps[1,1]], color = 'red', linewidth = 5)\n",
    "plt.plot([0.0, comps[0,0]], [0.0,comps[0,1]], color = 'red', linewidth = 5)\n",
    "plt.plot([0.0, comps[1,0]], [0.0,comps[1,1]], color = 'red', linewidth = 5)\n",
    "\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the the fist principle component (the long red line) is along the direction of greatest variance of the data. This is as expected. The short red line is along the direction of the second principle component. The lengths of these lines is the variance in the directions of the projection. \n",
    "\n",
    "The ultimate goal of PCA is to transform data to a coordinate system with the highest variance directions along the axes. The code in the cell below uses the `transform` method on the PCA object to perform this operation and then plots the result. Execute this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pca_fit.transform(sample)#变换坐标系，使得坐标系中点信息量（方差最大化）\n",
    "plt.scatter(trans[:,0], trans[:,1])\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Sample data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the scale along these two coordinates are quite different. The first principle component is along the horizontal axis. The range of values on this direction is in the range of about $\\{ -2.5,2.5 \\}$. The range of values on the vertical axis or second principle component are only about $\\{ -0.2, 0.3 \\}$. It is clear that most of the variance is along the direction of the fist principle component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个矩阵代表样本\n",
    "import numpy as np \n",
    "x=np.array([2,2,4,8,4])\n",
    "y=np.array([2,6,6,8,8])\n",
    "data=np.vstack((x,y))\n",
    "print(data)\n",
    "print(data.shape)\n",
    "data_cov=np.cov(data)\n",
    "print(\"得到两个特征属性x,y 的协方差，是一个对称矩阵\")\n",
    "print(data_cov)\n",
    "print(\"实现特征属性的0均值化处理\")\n",
    "def toZero(x:np.array)->np.array:\n",
    "    m=np.mean(x)\n",
    "    return x-m\n",
    "x=toZero(x)\n",
    "y=toZero(y)\n",
    "data2=np.vstack((x,y))\n",
    "data_cov=np.cov(data2)\n",
    "print(\"零均值化处理后，协方差矩阵不变\")\n",
    "print(data_cov)\n",
    "print(\"实现协方差矩阵的对角化\")\n",
    "print(\"通过特征向量，特征值实现：\")\n",
    "eig,Q=np.linalg.eig(data_cov)\n",
    "print(eig)\n",
    "print(\"注意pyhton中特征向量构成矩阵转置后，才是新的空间矩阵\")\n",
    "print(Q.T)\n",
    "print(\"将特征值实现对角化再除(n-1)，就是协方差对应的对角化矩阵\")\n",
    "sigma=np.diag(eig)/(data.shape[1]-1)\n",
    "print(sigma)\n",
    "print(\"实现原样本到新空间的转换\")\n",
    "newV=Q.T@data2\n",
    "print(newV)\n",
    "print(data2)\n",
    "print(Q)\n",
    "\n",
    "pca_model = skde.PCA()\n",
    "pca_fit = pca_model.fit(data.T)\n",
    " \n",
    " \n",
    "\n",
    "comps = pca_fit.components_\n",
    "print(comps)\n",
    "print(pca_fit.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import linalg as la \n",
    "A=np.array([1,5,7,6,1,2,1,10,4,4, 3,6,7,5,2]).reshape(3,5)\n",
    "print(\"A与自身转置相乘，得到一个对称矩阵：\",A@A.T,\"得到对称矩阵形状：\",(A@A.T).shape)\n",
    "print(\"原始矩阵：\",A,\"原始矩阵形状:\",A.shape)\n",
    "print(A@A.T)\n",
    "U,s,Vt=la.svd(A)\n",
    "Sigma=np.zeros(A.shape)#首先得到一个m*n的空矩阵，作为奇异值矩阵\n",
    "print(\"左奇异矩阵\",U,\"矩阵形状：\",U.shape)\n",
    "print(\"右奇异矩阵\",Vt,\"矩阵形状：\",Vt.shape)\n",
    " \n",
    "for i in range(len(s)):#为奇异值矩阵对角元素赋值\n",
    "    Sigma[i,i]=s[i]\n",
    "print(\"奇异值矩阵：\",Sigma,\"奇异值矩阵形状：\",Sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=U@Sigma@Vt\n",
    "print(\"同还原得到矩阵是否与原始矩阵相同：\",np.allclose(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PCA 实例操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Features and Labels\n",
    "\n",
    "Keeping the foregoing simple example in mind, it is time to apply PCA to some real data. \n",
    "\n",
    "The code in the cell below loads the dataset which has had the the following preprocessing:\n",
    "1. Cleaning missing values.\n",
    "2. Aggregating categories of certain categorical variables. \n",
    "3. Encoding categorical variables as binary dummy variables.\n",
    "4. Standardizing numeric variables. \n",
    "\n",
    ">数据处理基本步骤\n",
    "> 1.清理缺失数据\n",
    "> 2.对部分数据根据类别进行聚合\n",
    "> 3.对类别数据进行哑元编码\n",
    "> 4. 数据归一化 \n",
    "\n",
    "Execute the code in the cell below to load the features and labels as numpy arrays for the example: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 数据读取，数据观察"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "def loadData():\n",
    "    Features = np.array(pd.read_csv('..\\..\\data\\Credit_Features.csv'))\n",
    "    Labels = np.array(pd.read_csv('..\\..\\data\\Credit_Labels.csv'))\n",
    "    return Features,Labels\n",
    "    print(Features.shape)\n",
    "    print(Labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 35 features in this data set. The numeric features have been Zscore scaled so they are zero centered (mean removed) and unit variance (divide by standard deviation). \n",
    "\n",
    "****\n",
    "**Note:** <font color=\"red\">Before performing PCA all features must be zero mean and unit variance. Failure to do so will result in biased computation of the components and scales. In this case, the data set has already been scaled, but ordinarily scaling is a key step. <font/>\n",
    "****\n",
    "\n",
    "\n",
    "Now, run the code in the cell below to split the data set into test and training subsets:\n",
    "训练集，测试集分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as nr\n",
    "def preData(Features,Labels):\n",
    "    nr.seed(1115)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Labels, test_size=0.3)\n",
    "    #将y有矩阵变为向量，通过np.ravel函数\n",
    "    y_train=np.ravel(y_train)\n",
    "    y_test=np.ravel(y_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute principle components 构建pca模型进行降维\n",
    "\n",
    "These numbers are a bit abstract. However, you can see that the variance ratios are in descending order and that the sum is 1.0. \n",
    "\n",
    "Execute the code in the cell below to create a plot of the explained variance vs. the component:  \n",
    "\n",
    "PCA中特征向量构成坐标系的方差贡献率（标准化后）的总和为1\n",
    "通过绘制碎石图得到方差率占比大的维度，作为新的维度\n",
    "\n",
    "The code in the cell below computes the principle components for the training feature subset. Execute this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition as skde\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy.random as nr\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    " \n",
    "def loadData():\n",
    "    Features = np.array(pd.read_csv('..\\..\\data\\Credit_Features.csv'))\n",
    "    Labels = np.array(pd.read_csv('..\\..\\data\\Credit_Labels.csv'))\n",
    "    \n",
    "    print(Features.shape)\n",
    "    print(Labels.shape)\n",
    "    return Features,Labels\n",
    "\n",
    "def preData(Features,Labels):\n",
    "    nr.seed(1115)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Features, Labels, test_size=0.3)\n",
    "    #将y有矩阵变为向量，通过np.ravel函数\n",
    "    y_train=np.ravel(y_train)\n",
    "    y_test=np.ravel(y_test)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "def plot_explained(mod):\n",
    "    comps = mod.explained_variance_ratio_\n",
    "    x = range(len(comps))\n",
    "    x = [y + 1 for y in x]          \n",
    "    plt.plot(x,comps)\n",
    "    \n",
    "def percentagePCA(model):\n",
    "    \n",
    "    comps = model.explained_variance_ratio_\n",
    "    x=0\n",
    "    for i in range(len(comps)):\n",
    "        x=x+comps[i]\n",
    "        print(\"第{}个特征向量对应的方差贡献率：{}\".format(i,x/1))\n",
    "        \n",
    "if __name__=='__main__':\n",
    "    Features,Labels=loadData()\n",
    "    X_train, X_test, y_train, y_test=preData(Features,Labels)\n",
    "    pca_model=skde.PCA()\n",
    "    pca_model.fit(X_train)\n",
    "    print(\"特征向量贡献率： \\n\",pca_model.explained_variance_ratio_)\n",
    "    print(\"特征向量贡献率和为1：\",sum(pca_model.explained_variance_ratio_))\n",
    "    plot_explained(pca_model)\n",
    "    percentagePCA(pca_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create a PCA model with a reduced number of components. The code in the cell below trains and fits a PCA model with 5 components, and then transforms the features using that model. Execute this code. \n",
    "\n",
    "sklearn 中pca模型构建首先要根据方差贡献率，确定了降维个数，然后降维。\n",
    "''' python \n",
    "1. pca_mod_5 = skde.PCA(n_components = x) # x 为降维个数\n",
    "2. pca_mod_5.fit(X_train)# 通过模型降维\n",
    " \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model_5=skde.PCA(n_components=5)\n",
    "pca_model_5.fit(X_train)\n",
    "comp_5=pca_model_5.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute and evaluate a logistic regression model 在压缩矩阵基础上进行逻辑回归计算\n",
    "\n",
    "Next, you will compute and evaluate a logistic regression model using the features transformed by the first 5 principle components. Execute the code in the cell below to define and fit a logistic regression model, and print the model coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define and fit the logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#######################################\n",
    "#Cfloat, default=1.0\n",
    "#Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "log_mod_5 =  LogisticRegression(C = 10.0, class_weight = {0:0.45, 1:0.55}) \n",
    "log_mod_5.fit(comp_5, y_train)\n",
    "print(log_mod_5.intercept_)\n",
    "print(log_mod_5.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对模型进行评估\n",
    "Notice that there are now 5 regression coefficients, one for each component. This number is in contrast to the 35 features in the raw data. \n",
    "\n",
    "Next, evaluate this model using the code below. Notice that the test features are transformed using the<font color='red'> same PCA transformation used for the training data</font>. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  sklearn.metrics as sklm \n",
    "def score_model(probs, threshold):\n",
    "    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n",
    "\n",
    "def print_metrics(labels, probs, threshold):\n",
    "    scores = score_model(probs, threshold)\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)#混淆矩阵\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n",
    "    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))/2.0))\n",
    "    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))/2.0))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "\n",
    "def plot_auc(labels, probs):\n",
    "    ## Compute the false positive rate, true positive rate\n",
    "    ## and threshold along with the AUC\n",
    "    fpr, tpr, threshold = sklm.roc_curve(labels, probs[:,1])\n",
    "    auc = sklm.auc(fpr, tpr)\n",
    "    \n",
    "    ## Plot the result\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, color = 'orange', label = 'AUC = %0.2f' % auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()    \n",
    "\n",
    "probabilities = log_mod_5.predict_proba(pca_model_5.transform(X_test))\n",
    "print_metrics(y_test, probabilities, 0.3)    \n",
    "plot_auc(y_test, probabilities)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 基于SVD的维度压缩\n",
    " 在进行压缩时，首先计算得到原始矩阵A（可以代表图片等），的对应$ U V_T\\Sigma$矩阵，从中抽取信息量载荷大的k行，再还原新的矩阵 $A_,$ 对比原有矩阵$ A $得到压缩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=2\n",
    "print(\"k值\",k)\n",
    "Sigma_k=Sigma[:k,:k]##奇异值矩阵选取k行ke列\n",
    "print(\"奇异值矩阵选取k行k列:\",Sigma_k)\n",
    "U_k=U[:,:k]\n",
    "print(\"右奇异矩阵选取k行\",U_k)\n",
    "Vt_k=Vt[:k,:]\n",
    "print(\"左奇异矩阵选取列\",Vt_k)\n",
    "\n",
    "A_k=U_k@Sigma_k@Vt_k\n",
    "print(\"通过计算得到压缩矩阵: \\n\",A_k)\n",
    "print(\"原始矩阵 \\n\",A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pca实例2 对半导体数据进行降维\n",
    "\n",
    "1. 实现数据读取\n",
    "2. 数据处理\n",
    "   * 缺失值处理\n",
    "   * 数据归一化\n",
    "3. 计算协方差矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据背景：\n",
    "Title: SECOM Data Set\n",
    "\n",
    "Abstract: Data from a semi-conductor manufacturing process\n",
    "\t\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "Data Set Characteristics: Multivariate\n",
    "Number of Instances: 1567\n",
    "Area: Computer\n",
    "Attribute Characteristics: Real\n",
    "Number of Attributes: 591\n",
    "Date Donated: 2008-11-19\n",
    "Associated Tasks: Classification, Causal-Discovery\n",
    "Missing Values? Yes\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "Source:\n",
    "\n",
    "Authors: Michael McCann, Adrian Johnston \n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "Data Set Information:\n",
    "\n",
    "A complex modern semi-conductor manufacturing process is normally under consistent \n",
    "surveillance via the monitoring of signals/variables collected from sensors and or \n",
    "process measurement points. However, not all of these signals are equally valuable \n",
    "in a specific monitoring system. The measured signals contain a combination of \n",
    "useful information, irrelevant information as well as noise. It is often the case \n",
    "that useful information is buried in the latter two. Engineers typically have a \n",
    "much larger number of signals than are actually required. If we consider each type \n",
    "of signal as a feature, then feature selection may be applied to identify the most \n",
    "relevant signals. The Process Engineers may then use these signals to determine key \n",
    "factors contributing to yield excursions downstream in the process. This will \n",
    "enable an increase in process throughput, decreased time to learning and reduce the \n",
    "per unit production costs.\n",
    "\n",
    "To enhance current business improvement techniques the application of feature \n",
    "selection as an intelligent systems technique is being investigated.\n",
    "\n",
    "The dataset presented in this case represents a selection of such features where \n",
    "each example represents a single production entity with associated measured \n",
    "features and the labels represent a simple pass/fail yield for in house line \n",
    "testing, figure 2, and associated date time stamp. Where .1 corresponds to a pass \n",
    "and 1 corresponds to a fail and the data time stamp is for that specific test \n",
    "point.\n",
    "\n",
    "\n",
    "Using feature selection techniques it is desired to rank features according to \n",
    "their impact on the overall yield for the product, causal relationships may also be \n",
    "considered with a view to identifying the key features.\n",
    "\n",
    "Results may be submitted in terms of feature relevance for predictability using \n",
    "error rates as our evaluation metrics. It is suggested that cross validation be \n",
    "applied to generate these results. Some baseline results are shown below for basic \n",
    "feature selection techniques using a simple kernel ridge classifier and 10 fold \n",
    "cross validation.\n",
    "\n",
    "Baseline Results: Pre-processing objects were applied to the dataset simply to \n",
    "standardize the data and remove the constant features and then a number of \n",
    "different feature selection objects selecting 40 highest ranked features were \n",
    "applied with a simple classifier to achieve some initial results. 10 fold cross \n",
    "validation was used and the balanced error rate (*BER) generated as our initial \n",
    "performance metric to help investigate this dataset.\n",
    "\n",
    "\n",
    "SECOM Dataset: 1567 examples 591 features, 104 fails\n",
    "\n",
    "FSmethod (40 features) BER % True + % True - %\n",
    "S2N (signal to noise) 34.5 +-2.6 57.8 +-5.3 73.1 +2.1\n",
    "Ttest 33.7 +-2.1 59.6 +-4.7 73.0 +-1.8\n",
    "Relief 40.1 +-2.8 48.3 +-5.9 71.6 +-3.2\n",
    "Pearson 34.1 +-2.0 57.4 +-4.3 74.4 +-4.9\n",
    "Ftest 33.5 +-2.2 59.1 +-4.8 73.8 +-1.8\n",
    "Gram Schmidt 35.6 +-2.4 51.2 +-11.8 77.5 +-2.3\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "Key facts: Data Structure: The data consists of 2 files the dataset file SECOM \n",
    "consisting of 1567 examples each with 591 features a 1567 x 591 matrix and a labels \n",
    "file containing the classifications and date time stamp for each example.\n",
    "\n",
    "As with any real life data situations this data contains null values varying in \n",
    "intensity depending on the individuals features. This needs to be taken into \n",
    "consideration when investigating the data either through pre-processing or within \n",
    "the technique applied.\n",
    "\n",
    "The data is represented in a raw text file each line representing an individual \n",
    "example and the features seperated by spaces. The null values are represented by \n",
    "the 'NaN' value as per MatLab.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  dtale\n",
    "!pip install sweetviz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sweetviz as sv\n",
    "\n",
    "#EDA using Autoviz\n",
    "sweet_report = sv.analyze(pd.read_csv(\"titanic.csv\"))\n",
    "\n",
    "#Saving results to HTML file\n",
    "sweet_report.show_html('sweet_report.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class PCA_explain:\n",
    "    def __init__(self,pca_model):\n",
    "        self.__pca_model=pca_model\n",
    "    \n",
    "    def plot_explained(self):\n",
    "        matplotlib.use('TkAgg')\n",
    "\n",
    "        comps = self.__pca_model.explained_variance_ratio_\n",
    "        x = range(len(comps))\n",
    "        x = [y + 1 for y in x]          \n",
    "        plt.plot(x,comps)\n",
    "        plt.show()\n",
    "        \n",
    "    def percentagePCA(self,percentage: float)->int:\n",
    "        \n",
    "        comps = self.__pca_model.explained_variance_ratio_\n",
    "        x=0\n",
    "        count=0\n",
    "        for i in range(len(comps)):\n",
    "            x=x+comps[i]\n",
    "            if float(x/1)>=percentage:\n",
    "              print(\"第{}个特征向量对应的方差贡献率：{}，超过预定贡献率:{}\".format(i,x/1,percentage)) \n",
    "              count=i\n",
    "              break \n",
    "        return count\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\..\\\\data\\\\Credit_Features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\math_class\\Linear Algebra\\05  Dimension Reduction PCA.ipynb Cell 46\u001b[0m in \u001b[0;36m<cell line: 68>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m fileName\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCredit_Features.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39m#df=loadDataSet(fileName,sep=\" \",fileType='data')\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m df\u001b[39m=\u001b[39mloadDataSet(fileName,sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m,fileType\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcsv\u001b[39;49m\u001b[39m'\u001b[39;49m,header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mprint\u001b[39m(df\u001b[39m.\u001b[39mdescribe())\n",
      "\u001b[1;32mc:\\Users\\tomis\\OneDrive\\LUCK\\luckLab\\irm_-class-master\\IRM_class\\math_class\\Linear Algebra\\05  Dimension Reduction PCA.ipynb Cell 46\u001b[0m in \u001b[0;36mloadDataSet\u001b[1;34m(dataFileName, sep, fileType, header)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#fr=open(dataFileName)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m#stringArr=[ line.strip().split(delim) for line in fr.readlines()]##strip()表示删除掉数据中的换行符，split（‘，’）则是数据中遇到‘,’ 就隔开。\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#df=pd.read_table(dataFileName,sep=\" \",header=None)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m#df=pd.DataFrame(stringArr)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m fileType\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcsv\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(dataFileName,sep\u001b[39m=\u001b[39;49msep,header\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melif\u001b[39;00m fileType\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomis/OneDrive/LUCK/luckLab/irm_-class-master/IRM_class/math_class/Linear%20Algebra/05%20%20Dimension%20Reduction%20PCA.ipynb#X63sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_table(dataFileName,sep\u001b[39m=\u001b[39msep,header\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\luck\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\luck\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\.conda\\envs\\luck\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\.conda\\envs\\luck\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\.conda\\envs\\luck\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m     f,\n\u001b[0;32m   1220\u001b[0m     mode,\n\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\.conda\\envs\\luck\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\..\\\\data\\\\Credit_Features.csv'"
     ]
    }
   ],
   "source": [
    "from fileinput import filename\n",
    "import string\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import dtale\n",
    "from pandas_profiling import ProfileReport\n",
    "import sweetviz as sv\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import sklearn.decomposition as skde\n",
    "\n",
    "import matplotlib\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "def loadDataSet(dataFileName,sep,fileType,header=None)->pd.DataFrame:\n",
    "    \"\"\"_读取数据，并转换为矩阵返回_\n",
    "\n",
    "    Args:\n",
    "        fileName (_str_): _文件路径_\n",
    "        delim (str, optional): _文件分隔符_. Defaults to '\\t'.\n",
    "\n",
    "    Returns:\n",
    "        _np.mat_: _返回矩阵_\n",
    "    \"\"\"\n",
    "    #fr=open(dataFileName)\n",
    "    #stringArr=[ line.strip().split(delim) for line in fr.readlines()]##strip()表示删除掉数据中的换行符，split（‘，’）则是数据中遇到‘,’ 就隔开。\n",
    "    #df=pd.read_table(dataFileName,sep=\" \",header=None)\n",
    "    #df=pd.DataFrame(stringArr)\n",
    "    if fileType=='csv':\n",
    "        df=pd.read_csv(dataFileName,sep=sep,header=None)\n",
    "    elif fileType=='data':\n",
    "        df=pd.read_table(dataFileName,sep=sep,header=None)\n",
    "    df=df.astype(float)\n",
    "    return df\n",
    "\n",
    "\n",
    "def preDataSet(dataSet:pd.DataFrame,method=\"del\")->pd.DataFrame:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        dataset (pd.DataFrame): _description_\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: _description_\n",
    "    \"\"\"\n",
    "    #print(dataSet.describe)\n",
    "    if method==\"del\":\n",
    "        dataSet.dropna(inplace=True)\n",
    "    elif method==\"zero\":\n",
    "        dataSet.fillna(0)\n",
    "    elif method==\"mean\":\n",
    "        dataSet.fillna(dataSet.mean())\n",
    "    \n",
    "    return dataSet    \n",
    "\n",
    "def percentage_null(data )->list:\n",
    "    sum=data.shape[0]\n",
    "    null_sum=data.isnull().sum()/sum\n",
    "\n",
    "def pca(data_cov):\n",
    "    b=data_cov@data_cov.T \n",
    "    # 计算特征值与特征向量矩阵\n",
    "    eg_value,eg_vec=np.linalg.eig(b)\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # readFile and EDA\n",
    "    fileName=\"..\\..\\data\\secom.data\"\n",
    "    fileName=\"..\\..\\data\\Credit_Features.csv\"\n",
    "    #df=loadDataSet(fileName,sep=\" \",fileType='data')\n",
    "    df=loadDataSet(fileName,sep=\",\",fileType='csv',header=True)\n",
    "    print(df.shape)\n",
    " \n",
    "    print(df.describe())\n",
    "    #df=preDataSet(df,method=\"mean\")\n",
    "    data=df.to_numpy()\n",
    "    # 数据标准化，基于sklearn包括\n",
    "    #1. 均值插补(meaneinputation,使用均值来代替null)\n",
    "    #2. 数据标准化standardization 实现属性的相同空间缩放\n",
    "    imp_mean=SimpleImputer(missing_values=np.nan,strategy=\"mean\")\n",
    "    imp_mean.fit(data)\n",
    "    data2=imp_mean.transform(data)\n",
    "    # 数据标准化\n",
    "    stdsc=StandardScaler()\n",
    "    data3=stdsc.fit_transform(data2)\n",
    "    print(data3)\n",
    "    #print(\"构建相关矩阵\")\n",
    "    #data_cov=np.cov(data3)\n",
    "    #print(data_cov)\n",
    "    pca_model=skde.PCA()\n",
    "    pca_model.fit(data3)\n",
    "    print(\"特征向量贡献率： \\n\",pca_model.explained_variance_ratio_)\n",
    "    print(\"特征向量贡献率和为1：\",sum(pca_model.explained_variance_ratio_))\n",
    "    pca_exp=PCA_explain(pca_model)\n",
    "    pca_exp.plot_explained()\n",
    "    count=pca_exp.percentagePCA(0.8)\n",
    "    print(count)\n",
    "    #对原有数据进行pca\n",
    "    pca_5=skde.PCA(n_components=count)\n",
    "    pca_5.fit(data3)\n",
    "    data5=pca_5.transform(data3)\n",
    "    print(data5)\n",
    " \n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现图片压缩\n",
    "https://blog.csdn.net/discoverer100/article/details/89356513"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图片处理背景\n",
    "通过对png图像进行数组化操作，得到一个shape为3度的数组，即为3D张量，注意其每个度上的维度不同，3个度的保存的信息分别为高度信息，宽度信息和颜色通道信息。\n",
    "\n",
    "例如下面代码得到图像的shape为(897, 631, 4)。其高度与长度构成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def image_svd(A,k):\n",
    "    U,s,Vt=np.linalg.svd(A)\n",
    "    Sigma=np.zeros(A.shape)\n",
    "    for i in range(len(s)):#为奇异值矩阵对角元素赋值\n",
    "      Sigma[i,i]=s[i]\n",
    "    Sigma_k=Sigma[:k,:k]\n",
    "    U_k=U[:,:k]\n",
    "    Vt_k=Vt[:k,:]\n",
    "    return U_k@Sigma_k@Vt_k\n",
    "   \n",
    "def imageR(img):\n",
    "      R=img[:,:,0]\n",
    "      G=img[:,:,1]\n",
    "      B=img[:,:,2]\n",
    "      #A=img[:,:,3]\n",
    "      return R,G,B#,A\n",
    "def imageS(R,G,B):\n",
    "      img=np.stack((R,G,B),2)\n",
    "      return img\n",
    "if __name__==\"__main__\":\n",
    "    image=Image.open(\"girl.jpg\",\"r\")\n",
    "    A=np.array(image)\n",
    "    print(\"\\n 图片形状：\",A.shape)\n",
    "  \n",
    "    fig,axes=plt.subplots(3,2)\n",
    "    fig.set_size_inches(15,15)\n",
    "    ax1=axes[0,0]\n",
    "    ax2=axes[0,1]\n",
    "    ax3=axes[1,0]\n",
    "    ax4=axes[1,1]\n",
    "    ax5=axes[2,0]\n",
    "    ax1.axis('off')\n",
    "    ax2.axis('off')\n",
    "    ax3.axis('off')\n",
    "    ax4.axis('off')#直接关闭坐标轴的可读性与表情\n",
    "    ax1.imshow(image)\n",
    "    R,G,B =imageR(A)\n",
    "    print(\"红色通道图片：\\n\" ,\"对应矩阵shape\",R.shape)\n",
    "    ax2.imshow(R)\n",
    "    print(\"绿色通道图片:\" )\n",
    "    ax3.imshow(G)\n",
    "    print(\"蓝色通道图片:\" )\n",
    "    ax4.imshow(B)\n",
    "    #print(\"透明通道图片:\" )\n",
    "    #ax5.imshow(A2)\n",
    "    k=20\n",
    "    R_k=image_svd(R,k)\n",
    "    G_k=image_svd(G,k)\n",
    "    B_k=image_svd(B,k)\n",
    "    #A_k=image_svd(A2,k)\n",
    "    \n",
    "    print(R,R.shape)\n",
    "    img2=imageS(R_k,G_k,B_k)\n",
    "    \n",
    "    ax6=axes[2,1]\n",
    "    ax6.imshow(G_k)\n",
    "  \n",
    "\n",
    " \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD在推荐系统中的使用\n",
    "> 推荐系统使用经典协同\n",
    "在推荐系统中，我们常常遇到的问题是这样的，我们有很多用户和物品，也有少部分用户对少部分物品的评分，我们希望预测目标用户对其他未评分物品的评分，进而将评分高的物品推荐给目标用户。比如下面的用户物品评分表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7f860de8d2f90ad8cbd5e9027150b9f1852816e89724c40b1a127c39f06d8e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
